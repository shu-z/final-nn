{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Union\n",
    "from numpy.typing import ArrayLike\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Union\n",
    "from numpy.typing import ArrayLike\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    This is a class that generates a fully-connected neural network.\n",
    "\n",
    "    Parameters:\n",
    "        nn_arch: List[Dict[str, float]]\n",
    "            A list of dictionaries describing the layers of the neural network.\n",
    "            e.g. [{'input_dim': 64, 'output_dim': 32, 'activation': 'relu'}, {'input_dim': 32, 'output_dim': 8, 'activation:': 'sigmoid'}]\n",
    "            will generate a two-layer deep network with an input dimension of 64, a 32 dimension hidden layer, and an 8 dimensional output.\n",
    "        lr: float\n",
    "            Learning rate (alpha).\n",
    "        seed: int\n",
    "            Random seed to ensure reproducibility.\n",
    "        batch_size: int\n",
    "            Size of mini-batches used for training.\n",
    "        epochs: int\n",
    "            Max number of epochs for training.\n",
    "        loss_function: str\n",
    "            Name of loss function.\n",
    "\n",
    "    Attributes:\n",
    "        arch: list of dicts\n",
    "            (see nn_arch above)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        #this throwing error fix later\n",
    "        #nn_arch: List[Dict[str, Union(int, str)]],\n",
    "        nn_arch: List,\n",
    "        lr: float,\n",
    "        seed: int,\n",
    "        batch_size: int,\n",
    "        epochs: int,\n",
    "        loss_function: str,\n",
    "        verbose: bool\n",
    "    ):\n",
    "\n",
    "        # Save architecture\n",
    "        self.arch = nn_arch\n",
    "\n",
    "        # Save hyperparameters\n",
    "        self._lr = lr\n",
    "        self._seed = seed\n",
    "        self._epochs = epochs\n",
    "        self._loss_func = loss_function\n",
    "        self._batch_size = batch_size\n",
    "        self._verbose=verbose\n",
    "\n",
    "        # Initialize the parameter dictionary for use in training\n",
    "        self._param_dict = self._init_params()\n",
    "\n",
    "    def _init_params(self) -> Dict[str, ArrayLike]:\n",
    "        \"\"\"\n",
    "        DO NOT MODIFY THIS METHOD! IT IS ALREADY COMPLETE!\n",
    "\n",
    "        This method generates the parameter matrices for all layers of\n",
    "        the neural network. This function returns the param_dict after\n",
    "        initialization.\n",
    "\n",
    "        Returns:\n",
    "            param_dict: Dict[str, ArrayLike]\n",
    "                Dictionary of parameters in neural network.\n",
    "        \"\"\"\n",
    "\n",
    "        # Seed NumPy\n",
    "        np.random.seed(self._seed)\n",
    "\n",
    "        # Define parameter dictionary\n",
    "        param_dict = {}\n",
    "\n",
    "        # Initialize each layer's weight matrices (W) and bias matrices (b)\n",
    "        for idx, layer in enumerate(self.arch):\n",
    "            layer_idx = idx + 1\n",
    "            input_dim = layer['input_dim']\n",
    "            output_dim = layer['output_dim']\n",
    "            param_dict['W' + str(layer_idx)] = np.random.randn(output_dim, input_dim) * 0.1\n",
    "            param_dict['b' + str(layer_idx)] = np.random.randn(output_dim, 1) * 0.1\n",
    "\n",
    "        return param_dict\n",
    "\n",
    "    def _single_forward(\n",
    "        self,\n",
    "        W_curr: ArrayLike,\n",
    "        b_curr: ArrayLike,\n",
    "        A_prev: ArrayLike,\n",
    "        activation: str\n",
    "    ) -> Tuple[ArrayLike, ArrayLike]:\n",
    "        \"\"\"\n",
    "        This method is used for a single forward pass on a single layer.\n",
    "\n",
    "        Args:\n",
    "            W_curr: ArrayLike\n",
    "                Current layer weight matrix.\n",
    "            b_curr: ArrayLike\n",
    "                Current layer bias matrix.\n",
    "            A_prev: ArrayLike\n",
    "                Previous layer activation matrix.\n",
    "            activation: str\n",
    "                Name of activation function for current layer.\n",
    "\n",
    "        Returns:\n",
    "            A_curr: ArrayLike\n",
    "                Current layer activation matrix.\n",
    "            Z_curr: ArrayLike\n",
    "                Current layer linear transformed matrix.\n",
    "        \"\"\"\n",
    "          \n",
    "        print('W_curr', W_curr.shape)\n",
    "        print('b_curr', b_curr.shape)\n",
    "        print('A_prev', A_prev.shape)\n",
    "        \n",
    "        #need to multiply every node in last layer with weights \n",
    "        #then apply activation function \n",
    "        \n",
    "        Z_curr=np.dot(A_prev, W_curr.T) + b_curr.T\n",
    "        \n",
    "        #apply activation function    \n",
    "        if activation == 'sigmoid':\n",
    "            A_curr=self._sigmoid(Z_curr)      \n",
    "        elif activation =='relu':\n",
    "            A_curr=self._relu(Z_curr)\n",
    "        else:\n",
    "            raise Exception('no suitable activation function chosen!!!!!!!1!!!!')\n",
    "        \n",
    "\n",
    "        return(A_curr, Z_curr)\n",
    "    \n",
    "    \n",
    "\n",
    "    def forward(self, X: ArrayLike) -> Tuple[ArrayLike, Dict[str, ArrayLike]]:\n",
    "        \"\"\"\n",
    "        This method is responsible for one forward pass of the entire neural network.\n",
    "\n",
    "        Args:\n",
    "            X: ArrayLike\n",
    "                Input matrix with shape [batch_size, features].\n",
    "\n",
    "        Returns:\n",
    "            output: ArrayLike\n",
    "                Output of forward pass.\n",
    "            cache: Dict[str, ArrayLike]:\n",
    "                Dictionary storing Z and A matrices from `_single_forward` for use in backprop.\n",
    "        \"\"\"\n",
    "              \n",
    "        \n",
    "        cache={}\n",
    "                \n",
    "        #for the first layer \n",
    "        A_prev=X\n",
    "        #add to cache\n",
    "        cache['A0']=X\n",
    "\n",
    "        \n",
    "        #loop through remaining number of layers in nn\n",
    "        for l in range(1,len(self.arch)+1):\n",
    "            #print('layer',  l)\n",
    "            \n",
    "\n",
    "            #weights \n",
    "            W_curr=self._param_dict['W'+str(l)]\n",
    "            b_curr=self._param_dict['b'+str(l)]\n",
    "            \n",
    "            activation=self.arch[l-1]['activation']\n",
    "            \n",
    "            #go through one step of feed forward \n",
    "            #remember that A is activation matrix, Z is linearly transformed matrix\n",
    "            A_curr, Z_curr=self._single_forward(W_curr, b_curr, A_prev, activation)\n",
    "\n",
    "            \n",
    "            cache['A'+str(l)]=A_curr\n",
    "            cache['Z'+str(l)]=Z_curr\n",
    "            \n",
    "            \n",
    "            A_prev=A_curr\n",
    "            \n",
    "        output=A_curr\n",
    "            \n",
    "        return(output, cache)\n",
    "\n",
    "\n",
    "\n",
    "    def _single_backprop(\n",
    "        self,\n",
    "        W_curr: ArrayLike,\n",
    "        b_curr: ArrayLike,\n",
    "        Z_curr: ArrayLike,\n",
    "        A_prev: ArrayLike,\n",
    "        dA_curr: ArrayLike,\n",
    "        activation_curr: str\n",
    "    ) -> Tuple[ArrayLike, ArrayLike, ArrayLike]:\n",
    "        \"\"\"\n",
    "        This method is used for a single backprop pass on a single layer.\n",
    "\n",
    "        Args:\n",
    "            W_curr: ArrayLike\n",
    "                Current layer weight matrix.\n",
    "            b_curr: ArrayLike\n",
    "                Current layer bias matrix.\n",
    "            Z_curr: ArrayLike\n",
    "                Current layer linear transform matrix.\n",
    "            A_prev: ArrayLike\n",
    "                Previous layer activation matrix.\n",
    "            dA_curr: ArrayLike\n",
    "                Partial derivative of loss function with respect to current layer activation matrix.\n",
    "            activation_curr: str\n",
    "                Name of activation function of layer.\n",
    "\n",
    "        Returns:\n",
    "            dA_prev: ArrayLike\n",
    "                Partial derivative of loss function with respect to previous layer activation matrix.\n",
    "            dW_curr: ArrayLike\n",
    "                Partial derivative of loss function with respect to current layer weight matrix.\n",
    "            db_curr: ArrayLike\n",
    "                Partial derivative of loss function with respect to current layer bias matrix.\n",
    "        \"\"\"\n",
    "        if activation_curr=='sigmoid':\n",
    "            dZ=self._sigmoid_backprop(dA_curr, Z_curr)\n",
    "        \n",
    "        elif activation_curr=='relu':\n",
    "            dZ=self._relu_backprop(dA_curr, Z_curr)\n",
    "        else:\n",
    "            raise Exception('no suitable activation function chosen!! check ur spelling')\n",
    "            \n",
    "        #check this   \n",
    "        m=A_prev.shape[1]\n",
    "        #dW_curr=np.dot(dZ.T, A_prev) / m \n",
    "        #db_curr=np.sum(dZ, axis=0) / m \n",
    "        dW_curr=np.dot(dZ.T, A_prev)  \n",
    "        db_curr=np.sum(dZ, axis=0) \n",
    "        #dA_prev=np.dot(W_curr.T, dZ.T)\n",
    "        dA_prev=np.dot(dZ, W_curr)\n",
    "        \n",
    "        \n",
    "        return(dA_prev, dW_curr, db_curr)\n",
    "    \n",
    "\n",
    "    def backprop(self, y: ArrayLike, y_hat: ArrayLike, cache: Dict[str, ArrayLike]):\n",
    "        \"\"\"\n",
    "        This method is responsible for the backprop of the whole fully connected neural network.\n",
    "\n",
    "        Args:\n",
    "            y (array-like):\n",
    "                Ground truth labels.\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output values.\n",
    "            cache: Dict[str, ArrayLike]\n",
    "                Dictionary containing the information about the\n",
    "                most recent forward pass, specifically A and Z matrices.\n",
    "\n",
    "        Returns:\n",
    "            grad_dict: Dict[str, ArrayLike]\n",
    "                Dictionary containing the gradient information from this pass of backprop.\n",
    "        \"\"\"\n",
    "        grad_dict={}\n",
    "\n",
    "        \n",
    "        #go b a c k w a r d s \n",
    "        for l in range(len(self.arch), 0, -1):\n",
    "           # print('backprop layer: ', l)\n",
    "            \n",
    "            \n",
    "            #need to get all the variables to run backpropasdfkl\n",
    "            #i think they come from cache, which comes from forward prop\n",
    "            W_curr=self._param_dict['W' + str(l)]\n",
    "            b_curr=self._param_dict['b' + str(l)]\n",
    "            Z_curr=cache['Z' + str(l)]\n",
    "            A_prev=cache['A' + str(l-1)]\n",
    "            activation_curr=self.arch[l-1]['activation']\n",
    "            \n",
    "            \n",
    "            #get dA i think\n",
    "            #calculate loss \n",
    "            if self._loss_func =='mse':\n",
    "                dA_curr=self._mean_squared_error_backprop(y=y, y_hat=y_hat)\n",
    "            elif self._loss_func =='bce':\n",
    "                dA_curr=self._binary_cross_entropy_backprop(y=y, y_hat=y_hat)\n",
    "            else:\n",
    "                raise Exception('No availalbe loss function chosen')\n",
    "                 \n",
    "            \n",
    "            \n",
    "            #idk man idk \n",
    "            dA_prev, dW_curr, db_curr=self._single_backprop(W_curr, b_curr, Z_curr,A_prev, dA_curr, activation_curr)\n",
    "            \n",
    "            #print(\"dW_curr shape\", dW_curr.shape)\n",
    "            #print(\"dA_prev shape\", dA_prev.shape)\n",
    "            #print(\"db_curr shape\", db_curr.shape)\n",
    "            \n",
    "            \n",
    "            #print(\"param W shape\", self._param_dict['W'+str(l)].shape)\n",
    "            #print(\"param b shape\", self._param_dict['b'+str(l)].shape)\n",
    "            \n",
    "            \n",
    "            #update grad_dict with gradients for W, b, A \n",
    "            grad_dict['W'+str(l)]=dW_curr\n",
    "            grad_dict['b'+str(l)]=db_curr\n",
    "            grad_dict['A'+str(l-1)]=dA_prev\n",
    "            \n",
    "\n",
    "            \n",
    "        return(grad_dict)\n",
    "\n",
    "    def _update_params(self, grad_dict: Dict[str, ArrayLike]):\n",
    "        \"\"\"\n",
    "        This function updates the parameters in the neural network after backprop. This function\n",
    "        only modifies internal attributes and does not return anything\n",
    "\n",
    "        Args:\n",
    "            grad_dict: Dict[str, ArrayLike]\n",
    "                Dictionary containing the gradient information from most recent round of backprop.\n",
    "        \"\"\"\n",
    "        #go through each layer and get corresponding gradient for each node\n",
    "        \n",
    "        for l in range(1,len(self.arch)+1):\n",
    "                        \n",
    "            #get relevant B and W grad \n",
    "            #w_grad=grad_dict['W'+str(l)]\n",
    "            #b_grad=grad_dict['b'+str(l)]\n",
    "            \n",
    "            #idk if the indices are right \n",
    "            #update based on learning rate and gradient for that node \n",
    "            self._param_dict['W'+str(l)] -= self._lr * grad_dict['W'+str(l)]\n",
    "            self._param_dict['b'+str(l)] -= self._lr * np.expand_dims(grad_dict['b'+str(l)], 1)\n",
    "            \n",
    "         \n",
    "\n",
    "           \n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X_train: ArrayLike,\n",
    "        y_train: ArrayLike,\n",
    "        X_val: ArrayLike,\n",
    "        y_val: ArrayLike\n",
    "    ) -> Tuple[List[float], List[float]]:\n",
    "        \"\"\"\n",
    "        This function trains the neural network by backpropagation for the number of epochs defined at\n",
    "        the initialization of this class instance.\n",
    "\n",
    "        Args:\n",
    "            X_train: ArrayLike\n",
    "                Input features of training set.\n",
    "            y_train: ArrayLike\n",
    "                Labels for training set.\n",
    "            X_val: ArrayLike\n",
    "                Input features of validation set.\n",
    "            y_val: ArrayLike\n",
    "                Labels for validation set.\n",
    "\n",
    "        Returns:\n",
    "            per_epoch_loss_train: List[float]\n",
    "                List of per epoch loss for training set.\n",
    "            per_epoch_loss_val: List[float]\n",
    "                List of per epoch loss for validation set.\n",
    "        \"\"\"\n",
    "        #init lists to store losses \n",
    "        per_epoch_loss_train=[]\n",
    "        per_epoch_loss_val=[]\n",
    "        \n",
    "            \n",
    "        for epoch in range(self._epochs):\n",
    "           \n",
    "            \n",
    "            if self._verbose==True:\n",
    "                if epoch % 100==0:\n",
    "                     print('epoch: ', epoch)\n",
    "            \n",
    "            \n",
    "            #this take nfrom hw7 \n",
    "            # Shuffling the training data for each epoch of training\n",
    "            #only need to expand dims if y_train has one dimension i think \n",
    "            #y_train=np.expand_dims(y_train, 1)\n",
    "            #would also need to reflatten y_train after\n",
    "            #.flatten()\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "            idx=np.arange(X_train.shape[0])\n",
    "            np.random.shuffle(idx)\n",
    "            X_train = X_train[idx, :]\n",
    "            y_train = y_train[idx, :]\n",
    "            \n",
    "            \n",
    "            #print('X_train shape', X_train.shape)\n",
    "            #print('y_train shape', y_train.shape)\n",
    "                      \n",
    "        \n",
    "            # Create batches\n",
    "            num_batches = int(X_train.shape[0] / self._batch_size) + 1\n",
    "            X_batch = np.array_split(X_train, num_batches)\n",
    "            y_batch = np.array_split(y_train, num_batches)\n",
    "                \n",
    "            \n",
    "            #store losses for each training batch \n",
    "            batch_loss_train=[]\n",
    "            \n",
    "            # Iterate through batches (one of these loops is one epoch of training)\n",
    "            for X_train_batch, y_train_batch in zip(X_batch, y_batch):\n",
    "                \n",
    "                #print('X_train shape', X_train.shape)\n",
    "                #print('y_train shape', y_train.shape)\n",
    "\n",
    "               \n",
    "                \n",
    "                \n",
    "                #forward pass\n",
    "                y_pred, cache = self.forward(X_train_batch)\n",
    "                #print('y_pred shape', y_pred.shape)\n",
    "\n",
    "                #calculate loss \n",
    "                if self._loss_func =='mse':\n",
    "                    train_loss=self._mean_squared_error(y=y_train_batch, y_hat=y_pred)\n",
    "                elif self._loss_func =='bce':\n",
    "                    train_loss=self._binary_cross_entropy(y=y_train_batch, y_hat=y_pred)\n",
    "                else:\n",
    "                    raise Exception('No availalbe loss function chosen')\n",
    "                  \n",
    "                #add to batch loss list \n",
    "                batch_loss_train.append(train_loss)\n",
    "                #print(train_loss)\n",
    "                print(train_loss)\n",
    "                \n",
    "                #then, backpropagate \n",
    "                grad_dict=self.backprop(y=y_train_batch, y_hat=y_pred, cache=cache)\n",
    "                print(grad_dict['W1'])\n",
    "                #print(grad_dict['W2'])\n",
    "                    \n",
    "                #update parameter weights \n",
    "                self._update_params(grad_dict)\n",
    "                \n",
    "                #is that it?????????/?sdflkajdslkfjalskdflaksdjf\n",
    "                \n",
    "          \n",
    "\n",
    "    \n",
    "            #after running all train batches\n",
    "        \n",
    "            #get mean of batch losses and add to epoch loss\n",
    "            per_epoch_loss_train.append(np.mean(batch_loss_train))\n",
    "            \n",
    "            \n",
    "            #Then, validation\n",
    "            #Make one prediction on val for each epoch \n",
    "            #idk if this is how it should be \n",
    "            \n",
    "            val_pred = self.predict(X_val)\n",
    "                \n",
    "            if self._loss_func =='mse':\n",
    "                val_loss=self._mean_squared_error(y=y_val, y_hat=val_pred)          \n",
    "            elif self._loss_func =='bce':\n",
    "                val_loss=self._binary_cross_entropy(y=y_val, y_hat=val_pred)\n",
    "            else:\n",
    "                raise Exception('No availalbe loss function chosen')\n",
    "                \n",
    "                \n",
    "            #add loss to per epoch loss val\n",
    "            per_epoch_loss_val.append(val_loss)\n",
    "\n",
    "            \n",
    "        #return training and validation losses\n",
    "        return(per_epoch_loss_train, per_epoch_loss_val)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, X: ArrayLike) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        This function returns the prediction of the neural network.\n",
    "\n",
    "        Args:\n",
    "            X: ArrayLike\n",
    "                Input data for prediction.\n",
    "\n",
    "        Returns:\n",
    "            y_hat: ArrayLike\n",
    "                Prediction from the model.\n",
    "        \"\"\"\n",
    "        y_hat,_ =self.forward(X)\n",
    "        return y_hat\n",
    "    \n",
    "\n",
    "    def _sigmoid(self, Z: ArrayLike) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        Sigmoid activation function.\n",
    "\n",
    "        Args:\n",
    "            Z: ArrayLike\n",
    "                Output of layer linear transform.\n",
    "\n",
    "        Returns:\n",
    "            nl_transform: ArrayLike\n",
    "                Activation function output.\n",
    "        \"\"\"\n",
    "        return((1/(1+np.exp(-Z))))\n",
    "\n",
    "    def _sigmoid_backprop(self, dA: ArrayLike, Z: ArrayLike):\n",
    "        \"\"\"\n",
    "        Sigmoid derivative for backprop.\n",
    "\n",
    "        Args:\n",
    "            dA: ArrayLike\n",
    "                Partial derivative of previous layer activation matrix.\n",
    "            Z: ArrayLike\n",
    "                Output of layer linear transform.\n",
    "\n",
    "        Returns:\n",
    "            dZ: ArrayLike\n",
    "                Partial derivative of current layer Z matrix.\n",
    "        \"\"\"\n",
    "        sig_Z=self._sigmoid(Z)\n",
    "        dZ=dA*sig_Z*(1-sig_Z)\n",
    "        \n",
    "        return(dZ)\n",
    "\n",
    "    def _relu(self, Z: ArrayLike) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        ReLU activation function.\n",
    "\n",
    "        Args:\n",
    "            Z: ArrayLike\n",
    "                Output of layer linear transform.\n",
    "\n",
    "        Returns:\n",
    "            nl_transform: ArrayLike\n",
    "                Activation function output.\n",
    "        \"\"\"\n",
    "        return(np.maximum(0, Z))\n",
    "\n",
    "    def _relu_backprop(self, dA: ArrayLike, Z: ArrayLike) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        ReLU derivative for backprop.\n",
    "\n",
    "        Args:\n",
    "            dA: ArrayLike\n",
    "                Partial derivative of previous layer activation matrix.\n",
    "            Z: ArrayLike\n",
    "                Output of layer linear transform.\n",
    "\n",
    "        Returns:\n",
    "            dZ: ArrayLike\n",
    "                Partial derivative of current layer Z matrix.\n",
    "        \"\"\"\n",
    "        relu_Z=self._relu(Z)\n",
    "        \n",
    "        dZ=np.multiply(dA, relu_Z)\n",
    "        \n",
    "        return(dZ)\n",
    "\n",
    "    def _binary_cross_entropy(self, y: ArrayLike, y_hat: ArrayLike, error=1e-5) -> float:\n",
    "        \"\"\"\n",
    "        Binary cross entropy loss function.\n",
    "\n",
    "        Args:\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output.\n",
    "            y: ArrayLike\n",
    "                Ground truth output.\n",
    "\n",
    "        Returns:\n",
    "            loss: float\n",
    "                Average loss over mini-batch.\n",
    "        \"\"\"\n",
    "        #need to convert to floats\n",
    "        y=np.float64(y)\n",
    "        y_hat=np.float64(y_hat)\n",
    "\n",
    "   \n",
    "        print(y_hat)\n",
    "\n",
    "         #add error so no divide by zero warning\n",
    "        bce_loss= -np.mean(y*(np.log(y_hat + error)) +  (1-y)*np.log(1-y_hat + error)) \n",
    "        return(bce_loss)\n",
    "\n",
    "    def _binary_cross_entropy_backprop(self, y: ArrayLike, y_hat: ArrayLike, error=1e-5) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        Binary cross entropy loss function derivative for backprop.\n",
    "\n",
    "        Args:\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output.\n",
    "            y: ArrayLike\n",
    "                Ground truth output.\n",
    "\n",
    "        Returns:\n",
    "            dA: ArrayLike\n",
    "                partial derivative of loss with respect to A matrix.\n",
    "        \"\"\"\n",
    "\n",
    "        #need to convert to floats\n",
    "        y=np.float64(y)\n",
    "        y_hat=np.float64(y_hat)\n",
    "\n",
    "\n",
    "        #first get bce \n",
    "        bce=self._binary_cross_entropy(y, y_hat)\n",
    "        \n",
    "        #i had to google this i'm sorry\n",
    "        #add error for divide by zero \n",
    "        #need to double check this i think it's wrong \n",
    "        dA=-np.mean((y/y_hat + error) + (1-y)/(1-y_hat+error))\n",
    "        #dA=np.mean((y/y_hat + error) + (1-y)/(1-y_hat+error))\n",
    "\n",
    "        \n",
    "        return(dA)\n",
    "    \n",
    "\n",
    "\n",
    "    def _mean_squared_error(self, y: ArrayLike, y_hat: ArrayLike) -> float:\n",
    "        \"\"\"\n",
    "        Mean squared error loss.\n",
    "\n",
    "        Args:\n",
    "            y: ArrayLike\n",
    "                Ground truth output.\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output.\n",
    "\n",
    "        Returns:\n",
    "            loss: float\n",
    "                Average loss of mini-batch.\n",
    "        \"\"\"\n",
    "    \n",
    "        mse = np.mean((y - y_hat) ** 2)\n",
    "        return(mse)\n",
    "\n",
    "    def _mean_squared_error_backprop(self, y: ArrayLike, y_hat: ArrayLike) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        Mean square error loss derivative for backprop.\n",
    "\n",
    "        Args:\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output.\n",
    "            y: ArrayLike\n",
    "                Ground truth output.\n",
    "\n",
    "        Returns:\n",
    "            dA: ArrayLike\n",
    "                partial derivative of loss with respect to A matrix.\n",
    "        \"\"\"\n",
    "        dA=np.mean(2*(y_hat-y))\n",
    "        return(dA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89, 64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load in digits dataset \n",
    "\n",
    "#digits.data is shape (1794, 64) --> 64 is flattened image pixels \n",
    "digits = load_digits()\n",
    "  \n",
    "#split into train and test sets \n",
    "X_train, X_test, y_train, y_test=train_test_split(digits.data, digits.target, train_size=0.05, random_state=3)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_NN = NeuralNetwork(nn_arch = [{'input_dim': 64, 'output_dim': 16, 'activation': 'relu'},\n",
    "                                      {'input_dim': 16, 'output_dim': 64, 'activation': 'relu'}],\n",
    "                                lr = 0.01, seed = 3, batch_size = 50, epochs = 1, \n",
    "                               loss_function='mse', verbose=True)\n",
    "\n",
    "\n",
    "#params are arrays of W1-Wx where x is feature number\n",
    "#and also an array for b1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "W_curr (16, 64)\n",
      "b_curr (16, 1)\n",
      "A_prev (45, 64)\n",
      "W_curr (64, 16)\n",
      "b_curr (64, 1)\n",
      "A_prev (45, 16)\n",
      "54.81623418605829\n",
      "[[ 0.00000000e+00  0.00000000e+00 -3.36624119e+02 ... -6.44034614e+02\n",
      "  -1.79920393e+02 -1.37554093e+01]\n",
      " [ 0.00000000e+00 -5.59734335e+02 -4.43120066e+03 ... -4.90351361e+03\n",
      "  -2.42052156e+03 -2.10352465e+02]\n",
      " [ 0.00000000e+00 -8.08389972e+02 -9.04026518e+03 ... -1.45056362e+04\n",
      "  -5.56435733e+03 -6.36045037e+02]\n",
      " ...\n",
      " [ 0.00000000e+00 -2.25546661e+02 -2.01985855e+03 ... -2.13902849e+03\n",
      "  -1.18072386e+03 -6.21485898e+01]\n",
      " [ 0.00000000e+00 -2.39011221e+03 -2.68285546e+04 ... -4.13911795e+04\n",
      "  -1.88763756e+04 -2.79469889e+03]\n",
      " [ 0.00000000e+00  0.00000000e+00 -2.70142920e+02 ... -7.92619407e+02\n",
      "  -4.59618510e+02 -1.75046956e+01]]\n",
      "W_curr (16, 64)\n",
      "b_curr (16, 1)\n",
      "A_prev (44, 64)\n",
      "W_curr (64, 16)\n",
      "b_curr (64, 1)\n",
      "A_prev (44, 16)\n",
      "260913795929761.47\n",
      "[[0.00000000e+00 9.70194312e+11 1.23922854e+13 ... 1.35442246e+13\n",
      "  3.68570950e+12 8.16718856e+11]\n",
      " [0.00000000e+00 6.06543113e+12 7.37776381e+13 ... 7.52124301e+13\n",
      "  2.15012356e+13 4.89414809e+12]\n",
      " [0.00000000e+00 1.40587475e+13 1.73094174e+14 ... 1.83546664e+14\n",
      "  5.18224671e+13 1.14096835e+13]\n",
      " ...\n",
      " [0.00000000e+00 2.12660958e+12 2.66267084e+13 ... 2.72596693e+13\n",
      "  8.02789941e+12 1.81715057e+12]\n",
      " [0.00000000e+00 3.70168819e+13 4.55651243e+14 ... 4.80945752e+14\n",
      "  1.37038351e+14 3.01799233e+13]\n",
      " [0.00000000e+00 4.05741121e+11 5.31333723e+12 ... 5.71246491e+12\n",
      "  1.68690966e+12 3.76035358e+11]]\n",
      "W_curr (16, 64)\n",
      "b_curr (16, 1)\n",
      "A_prev (1708, 64)\n",
      "W_curr (64, 16)\n",
      "b_curr (64, 1)\n",
      "A_prev (1708, 16)\n"
     ]
    }
   ],
   "source": [
    "#YOU FOOL\n",
    "#BC IT'S AN AUTOENCODER INPUT AND OUTPUT SHOULD BE THE SAME GLAKSDJF'LAKSDFJ'\n",
    "per_epoch_loss_train, per_epoch_loss_val=example_NN.fit(X_train, X_train, X_test, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#code from hw7\n",
    "#plot training and reconstruction loss for final model\n",
    "\n",
    "fig, axs = plt.subplots(2, figsize=(8, 8))\n",
    "axs[0].plot(np.arange(len(per_epoch_loss_train)), per_epoch_loss_train)\n",
    "axs[0].set_title('Training Loss History')\n",
    "axs[1].plot(np.arange(len(per_epoch_loss_val)), per_epoch_loss_val)\n",
    "axs[1].set_title('Validation Loss History')\n",
    "plt.xlabel('Steps')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
