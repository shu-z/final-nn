{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Union\n",
    "from numpy.typing import ArrayLike\n",
    "import sklearn \n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    This is a class that generates a fully-connected neural network.\n",
    "\n",
    "    Parameters:\n",
    "        nn_arch: List[Dict[str, float]]\n",
    "            A list of dictionaries describing the layers of the neural network.\n",
    "            e.g. [{'input_dim': 64, 'output_dim': 32, 'activation': 'relu'}, {'input_dim': 32, 'output_dim': 8, 'activation:': 'sigmoid'}]\n",
    "            will generate a two-layer deep network with an input dimension of 64, a 32 dimension hidden layer, and an 8 dimensional output.\n",
    "        lr: float\n",
    "            Learning rate (alpha).\n",
    "        seed: int\n",
    "            Random seed to ensure reproducibility.\n",
    "        batch_size: int\n",
    "            Size of mini-batches used for training.\n",
    "        epochs: int\n",
    "            Max number of epochs for training.\n",
    "        loss_function: str\n",
    "            Name of loss function.\n",
    "\n",
    "    Attributes:\n",
    "        arch: list of dicts\n",
    "            (see nn_arch above)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        #this throwing error fix later\n",
    "        #nn_arch: List[Dict[str, Union(int, str)]],\n",
    "        nn_arch: List, \n",
    "        lr: float,\n",
    "        seed: int,\n",
    "        batch_size: int,\n",
    "        epochs: int,\n",
    "        loss_function: str\n",
    "    ):\n",
    "\n",
    "        # Save architecture\n",
    "        self.arch = nn_arch\n",
    "\n",
    "        # Save hyperparameters\n",
    "        self._lr = lr\n",
    "        self._seed = seed\n",
    "        self._epochs = epochs\n",
    "        self._loss_func = loss_function\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "        # Initialize the parameter dictionary for use in training\n",
    "        self._param_dict = self._init_params()\n",
    "        #print(self._param_dict)\n",
    "\n",
    "    def _init_params(self) -> Dict[str, ArrayLike]:\n",
    "        \"\"\"\n",
    "        DO NOT MODIFY THIS METHOD! IT IS ALREADY COMPLETE!\n",
    "\n",
    "        This method generates the parameter matrices for all layers of\n",
    "        the neural network. This function returns the param_dict after\n",
    "        initialization.\n",
    "\n",
    "        Returns:\n",
    "            param_dict: Dict[str, ArrayLike]\n",
    "                Dictionary of parameters in neural network.\n",
    "        \"\"\"\n",
    "\n",
    "        # Seed NumPy\n",
    "        np.random.seed(self._seed)\n",
    "\n",
    "        # Define parameter dictionary\n",
    "        param_dict = {}\n",
    "\n",
    "        # Initialize each layer's weight matrices (W) and bias matrices (b)\n",
    "        for idx, layer in enumerate(self.arch):\n",
    "            layer_idx = idx + 1\n",
    "            input_dim = layer['input_dim']\n",
    "            output_dim = layer['output_dim']\n",
    "            param_dict['W' + str(layer_idx)] = np.random.randn(output_dim, input_dim) * 0.1\n",
    "            param_dict['b' + str(layer_idx)] = np.random.randn(output_dim, 1) * 0.1\n",
    "\n",
    "        return param_dict\n",
    "\n",
    "    def _single_forward(\n",
    "        self,\n",
    "        W_curr: ArrayLike,\n",
    "        b_curr: ArrayLike,\n",
    "        A_prev: ArrayLike,\n",
    "        activation: str\n",
    "    ) -> Tuple[ArrayLike, ArrayLike]:\n",
    "        \"\"\"\n",
    "        This method is used for a single forward pass on a single layer.\n",
    "\n",
    "        Args:\n",
    "            W_curr: ArrayLike\n",
    "                Current layer weight matrix.\n",
    "            b_curr: ArrayLike\n",
    "                Current layer bias matrix.\n",
    "            A_prev: ArrayLike\n",
    "                Previous layer activation matrix.\n",
    "            activation: str\n",
    "                Name of activation function for current layer.\n",
    "\n",
    "        Returns:\n",
    "            A_curr: ArrayLike\n",
    "                Current layer activation matrix.\n",
    "            Z_curr: ArrayLike\n",
    "                Current layer linear transformed matrix.\n",
    "        \"\"\"\n",
    "        \n",
    "        #need to multiply every node in last layer with weights \n",
    "        #then apply activation function \n",
    "        \n",
    "        \n",
    "        #Z is before A\n",
    "        Z_curr=np.dot(A_prev, W_curr.T) + b_curr.T\n",
    "        \n",
    "        #apply activation function \n",
    "        \n",
    "        if activation =='sigmoid':\n",
    "            A_curr=self._sigmoid(Z_curr)\n",
    "        \n",
    "        \n",
    "        elif activation =='relu':\n",
    "            A_curr=self._relu(Z_curr)\n",
    "                    \n",
    "        else:\n",
    "            raise Exception('no suitable activation function chosen!!!!!!!1!!!!')\n",
    "        \n",
    "        \n",
    "        \n",
    "        return(A_curr, Z_curr)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    def forward(self, X: ArrayLike) -> Tuple[ArrayLike, Dict[str, ArrayLike]]:\n",
    "        \"\"\"\n",
    "        This method is responsible for one forward pass of the entire neural network.\n",
    "\n",
    "        Args:\n",
    "            X: ArrayLike\n",
    "                Input matrix with shape [batch_size, features].\n",
    "\n",
    "        Returns:\n",
    "            output: ArrayLike\n",
    "                Output of forward pass.\n",
    "            cache: Dict[str, ArrayLike]:\n",
    "                Dictionary storing Z and A matrices from `_single_forward` for use in backprop.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        cache={}\n",
    "                \n",
    "        #for the first layer \n",
    "        A_prev=X\n",
    "        #add to cache\n",
    "        cache['A0']=X\n",
    "\n",
    "        \n",
    "        #loop through remaining number of layers in nn\n",
    "        for l in range(1,len(self.arch)+1):\n",
    "            print('layer',  l)\n",
    "            \n",
    "\n",
    "            #weights \n",
    "            W_curr=self._param_dict['W'+str(l)]\n",
    "            b_curr=self._param_dict['b'+str(l)]\n",
    "            \n",
    "            activation=self.arch[l-1]['activation']\n",
    "            print(activation)\n",
    "            \n",
    "            #go through one step of feed forward \n",
    "            #remember that A is activation matrix, Z is linearly transformed matrix\n",
    "            A_curr, Z_curr=self._single_forward(W_curr, b_curr, A_prev, activation)\n",
    "\n",
    "            \n",
    "            cache['A'+str(l)]=A_curr\n",
    "            cache['Z'+str(l)]=Z_curr\n",
    "            \n",
    "            \n",
    "            A_prev=A_curr\n",
    "            \n",
    "        output=A_curr\n",
    "            \n",
    "        return(output, cache)\n",
    "            \n",
    "\n",
    "    def _single_backprop(\n",
    "        self,\n",
    "        W_curr: ArrayLike,\n",
    "        b_curr: ArrayLike,\n",
    "        Z_curr: ArrayLike,\n",
    "        A_prev: ArrayLike,\n",
    "        dA_curr: ArrayLike,\n",
    "        activation_curr: str\n",
    "    ) -> Tuple[ArrayLike, ArrayLike, ArrayLike]:\n",
    "        \"\"\"\n",
    "        This method is used for a single backprop pass on a single layer.\n",
    "\n",
    "        Args:\n",
    "            W_curr: ArrayLike\n",
    "                Current layer weight matrix.\n",
    "            b_curr: ArrayLike\n",
    "                Current layer bias matrix.\n",
    "            Z_curr: ArrayLike\n",
    "                Current layer linear transform matrix.\n",
    "            A_prev: ArrayLike\n",
    "                Previous layer activation matrix.\n",
    "            dA_curr: ArrayLike\n",
    "                Partial derivative of loss function with respect to current layer activation matrix.\n",
    "            activation_curr: str\n",
    "                Name of activation function of layer.\n",
    "\n",
    "        Returns:\n",
    "            dA_prev: ArrayLike\n",
    "                Partial derivative of loss function with respect to previous layer activation matrix.\n",
    "            dW_curr: ArrayLike\n",
    "                Partial derivative of loss function with respect to current layer weight matrix.\n",
    "            db_curr: ArrayLike\n",
    "                Partial derivative of loss function with respect to current layer bias matrix.\n",
    "        \"\"\"\n",
    "        \n",
    "        if activation_curr=='sigmoid':\n",
    "            dZ=self._sigmoid_backprop(dA_curr, Z_curr)\n",
    "        \n",
    "        if activation_curr=='relu':\n",
    "            dZ=self._relu_backprop(dA_curr, Z_curr)\n",
    "        else:\n",
    "            raise Exception('no suitable activation function chosen!! check ur spelling')\n",
    "            \n",
    "        #check this   \n",
    "        m=A_prev.shape[1]\n",
    "        dW_curr=np.dot(A_prev.T, dZ) / m \n",
    "        db_curr=np.sum(dZ, axis=1) / m \n",
    "        dA_prev=np.dot(W_curr.T, dZ.T)\n",
    "        \n",
    "        \n",
    "        return(dA_prev, dW_curr, db_curr)\n",
    "        \n",
    "\n",
    "\n",
    "    def backprop(self, y: ArrayLike, y_hat: ArrayLike, cache: Dict[str, ArrayLike]):\n",
    "        \"\"\"\n",
    "        This method is responsible for the backprop of the whole fully connected neural network.\n",
    "\n",
    "        Args:\n",
    "            y (array-like):\n",
    "                Ground truth labels.\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output values.\n",
    "            cache: Dict[str, ArrayLike]\n",
    "                Dictionary containing the information about the\n",
    "                most recent forward pass, specifically A and Z matrices.\n",
    "\n",
    "        Returns:\n",
    "            grad_dict: Dict[str, ArrayLike]\n",
    "                Dictionary containing the gradient information from this pass of backprop.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        grad_dict={}\n",
    "        \n",
    "        #I THINK THIS NEEDS TO BE DIFFERENT FOR HTE FIRST NODE OH MY GOD \n",
    "        \n",
    "        #go b a c k w a r d s \n",
    "        for l in range(len(self.arch)-1, 0, -1):\n",
    "            print('layer: ', l)\n",
    "            \n",
    "            \n",
    "            #need to get all the variables to run backpropasdfkl\n",
    "            #i think they come from cache, which comes from forward prop\n",
    "            W_curr=self._param_dict['W' + str(l)]\n",
    "            b_curr=self._param_dict['b' + str(l)]\n",
    "            Z_curr=cache['Z' + str(l)]\n",
    "            A_prev=cache['A' + str(l-1)]\n",
    "            activation_curr=self.arch[l-1]['activation']\n",
    "            \n",
    "            \n",
    "            #get dA i think\n",
    "            #calculate loss \n",
    "            if self._loss_func =='mse':\n",
    "                dA_curr=self._mean_squared_error(y=y, y_hat=y_hat)\n",
    "            elif self._loss_func =='bce':\n",
    "                dA_curr=self._binary_cross_entropy(y=y, y_hat=y_hat)\n",
    "            else:\n",
    "                raise Exception('No availalbe loss function chosen')\n",
    "                 \n",
    "            \n",
    "            \n",
    "            #idk man idk \n",
    "            dA_prev, dW_curr, db_curr=self._single_backprop(W_curr, b_curr, Z_curr,A_prev, dA_curr, activation_curr)\n",
    "            \n",
    "            print(\"dW_curr shape\", dW_curr.shape)\n",
    "            print(\"dA_prev shape\", dA_prev.shape)\n",
    "            print(\"db_curr shape\", db_curr.shape)\n",
    "            \n",
    "            \n",
    "            print(\"param W shape\", self._param_dict['W'+str(l)].shape)\n",
    "            print(\"param b shape\", self._param_dict['b'+str(l)].shape)\n",
    "            \n",
    "            \n",
    "            #update grad_dict with gradients for W, b, A \n",
    "            grad_dict['W'+str(l)]=dW_curr\n",
    "            grad_dict['b'+str(l)]=db_curr\n",
    "            grad_dict['A'+str(l-1)]=dA_prev\n",
    "            \n",
    "\n",
    "            \n",
    "        return(grad_dict)\n",
    "            \n",
    "\n",
    "    def _update_params(self, grad_dict: Dict[str, ArrayLike]):\n",
    "        \"\"\"\n",
    "        This function updates the parameters in the neural network after backprop. This function\n",
    "        only modifies internal attributes and does not return anything\n",
    "\n",
    "        Args:\n",
    "            grad_dict: Dict[str, ArrayLike]\n",
    "                Dictionary containing the gradient information from most recent round of backprop.\n",
    "        \"\"\"\n",
    "                    \n",
    "            \n",
    "        #go through each layer and get corresponding gradient for each node\n",
    "        \n",
    "        for l in range(1,len(self.arch)+1):\n",
    "                        \n",
    "            #get relevant B and W grad \n",
    "            #w_grad=grad_dict['W'+str(l)]\n",
    "            #b_grad=grad_dict['b'+str(l)]\n",
    "            \n",
    "            #idk if the indices are right \n",
    "            #update based on learning rate and gradient for that node \n",
    "            self._param_dict['W'+str(l)] -= self._lr * grad_dict['W'+str(l)]\n",
    "            self._param_dict['b'+str(l)] -= self._lr * grad_dict['b'+str(l)]\n",
    "            \n",
    "            #i think just these two?\n",
    "\n",
    "            \n",
    "       \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X_train: ArrayLike,\n",
    "        y_train: ArrayLike,\n",
    "        X_val: ArrayLike,\n",
    "        y_val: ArrayLike\n",
    "    ) -> Tuple[List[float], List[float]]:\n",
    "        \"\"\"\n",
    "        This function trains the neural network by backpropagation for the number of epochs defined at\n",
    "        the initialization of this class instance.\n",
    "\n",
    "        Args:\n",
    "            X_train: ArrayLike\n",
    "                Input features of training set.\n",
    "            y_train: ArrayLike\n",
    "                Labels for training set.\n",
    "            X_val: ArrayLike\n",
    "                Input features of validation set.\n",
    "            y_val: ArrayLike\n",
    "                Labels for validation set.\n",
    "\n",
    "        Returns:\n",
    "            per_epoch_loss_train: List[float]\n",
    "                List of per epoch loss for training set.\n",
    "            per_epoch_loss_val: List[float]\n",
    "                List of per epoch loss for validation set.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        #init lists to store losses \n",
    "        per_epoch_loss_train=[]\n",
    "        per_epoch_loss_val=[]\n",
    "        \n",
    "            \n",
    "        for epoch in range(self._epochs):\n",
    "            \n",
    "            \n",
    "            #this take nfrom hw7 \n",
    "            # Shuffling the training data for each epoch of training\n",
    "            #only need to expand dims if y_train has one dimension i think \n",
    "            #y_train=np.expand_dims(y_train, 1)\n",
    "            #would also need to reflatten y_train after\n",
    "            #.flatten()\n",
    "            \n",
    "\n",
    "            idx=np.arange(X_train.shape[0])\n",
    "            np.random.shuffle(idx)\n",
    "            X_train = X_train[idx, :]\n",
    "            y_train = y_train[idx, :]\n",
    "            \n",
    "            \n",
    "            print('X_train shape', X_train.shape)\n",
    "            print('y_train shape', y_train.shape)\n",
    "                      \n",
    "        \n",
    "            # Create batches\n",
    "            num_batches = int(X_train.shape[0] / self._batch_size) + 1\n",
    "            X_batch = np.array_split(X_train, num_batches)\n",
    "            y_batch = np.array_split(y_train, num_batches)\n",
    "                \n",
    "            \n",
    "            #store losses for each training batch \n",
    "            batch_loss_train=[]\n",
    "            \n",
    "            # Iterate through batches (one of these loops is one epoch of training)\n",
    "            for X_train, y_train in zip(X_batch, y_batch):\n",
    "                \n",
    "                print('X_train shape', X_train.shape)\n",
    "                print('y_train shape', y_train.shape)\n",
    "                \n",
    "                \n",
    "                #forward pass\n",
    "                y_pred, cache = self.forward(X_train)\n",
    "                print('y_pred shape', y_pred.shape)\n",
    "\n",
    "                #calculate loss \n",
    "                if self._loss_func =='mse':\n",
    "                    train_loss=self._mean_squared_error(y=y_train, y_hat=y_pred)\n",
    "                elif self._loss_func =='bce':\n",
    "                    train_loss=self._binary_cross_entropy(y=y_train, y_hat=y_pred)\n",
    "                else:\n",
    "                    raise Exception('No availalbe loss function chosen')\n",
    "                  \n",
    "                #add to batch loss list \n",
    "                batch_loss_train.append(train_loss)\n",
    "                \n",
    "                \n",
    "                #then, backpropagate \n",
    "                grad_dict=self.backprop(y=y_train, y_hat=y_pred, cache=cache)\n",
    "                \n",
    "            \n",
    "                    \n",
    "                    \n",
    "                #update parameter weights \n",
    "                self._update_params(grad_dict)\n",
    "                \n",
    "                #is that it?????????/?sdflkajdslkfjalskdflaksdjf\n",
    "                \n",
    "          \n",
    "\n",
    "    \n",
    "            #after running all train batches\n",
    "        \n",
    "            #get mean of batch losses and add to epoch loss\n",
    "            per_epoch_loss_train=np.mean(batch_loss_train)\n",
    "            \n",
    "            \n",
    "            #Then, validation\n",
    "            #Make one prediction on val for each epoch \n",
    "            #idk if this is how it should be \n",
    "            \n",
    "            val_pred = self.predict(X_val)\n",
    "            \n",
    "            \n",
    "            if self._loss_func =='mse':\n",
    "                val_loss=self_mean_squared_error(y=y_val, y_hat=val_pred)          \n",
    "            elif self._loss_func =='bce':\n",
    "                val_loss=_binary_cross_entropy(y=y_val, y_hat=val_pred)\n",
    "            else:\n",
    "                raise Exception('No availalbe loss function chosen')\n",
    "                \n",
    "                \n",
    "            #add loss to per epoch loss val\n",
    "            per_epoch_loss_val.append(val_loss)\n",
    "\n",
    "            \n",
    "        #return training and validation losses\n",
    "        return(per_epoch_loss_train, per_epoch_loss_val)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, X: ArrayLike) -> ArrayLike: #DONE\n",
    "        \"\"\"\n",
    "        This function returns the prediction of the neural network.\n",
    "\n",
    "        Args:\n",
    "            X: ArrayLike\n",
    "                Input data for prediction.\n",
    "\n",
    "        Returns:\n",
    "            y_hat: ArrayLike\n",
    "                Prediction from the model.\n",
    "        \"\"\"\n",
    "        \n",
    "        y_hat,_ =self.forward(X)\n",
    "        return y_hat\n",
    "\n",
    "    def _sigmoid(self, Z: ArrayLike) -> ArrayLike: #DONE\n",
    "        \"\"\"\n",
    "        Sigmoid activation function.\n",
    "\n",
    "        Args:\n",
    "            Z: ArrayLike\n",
    "                Output of layer linear transform.\n",
    "\n",
    "        Returns:\n",
    "            nl_transform: ArrayLike\n",
    "                Activation function output.\n",
    "        \"\"\"\n",
    "        return((1/(1+np.exp(-Z))))\n",
    "\n",
    "    def _sigmoid_backprop(self, dA: ArrayLike, Z: ArrayLike): #DONE\n",
    "        \"\"\"\n",
    "        Sigmoid derivative for backprop.\n",
    "\n",
    "        Args:\n",
    "            dA: ArrayLike\n",
    "                Partial derivative of previous layer activation matrix.\n",
    "            Z: ArrayLike\n",
    "                Output of layer linear transform.\n",
    "\n",
    "        Returns:\n",
    "            dZ: ArrayLike\n",
    "                Partial derivative of current layer Z matrix.\n",
    "        \"\"\"\n",
    "        \n",
    "        sig_Z=self._sigmoid(Z)\n",
    "        dZ=dA*sig_Z*(1-sig_Z)\n",
    "        \n",
    "        return(dZ)\n",
    "    \n",
    "\n",
    "    def _relu(self, Z: ArrayLike) -> ArrayLike: #DONE\n",
    "        \"\"\"\n",
    "        ReLU activation function.\n",
    "\n",
    "        Args:\n",
    "            Z: ArrayLike\n",
    "                Output of layer linear transform.\n",
    "\n",
    "        Returns:\n",
    "            nl_transform: ArrayLike\n",
    "                Activation function output.\n",
    "        \"\"\"\n",
    "        return(np.maximum(0, Z))\n",
    "    \n",
    "\n",
    "    def _relu_backprop(self, dA: ArrayLike, Z: ArrayLike) -> ArrayLike: #DONE I THINK\n",
    "        \"\"\"\n",
    "        ReLU derivative for backprop.\n",
    "\n",
    "        Args:\n",
    "            dA: ArrayLike\n",
    "                Partial derivative of previous layer activation matrix.\n",
    "            Z: ArrayLike\n",
    "                Output of layer linear transform.\n",
    "\n",
    "        Returns:\n",
    "            dZ: ArrayLike\n",
    "                Partial derivative of current layer Z matrix.\n",
    "        \"\"\"\n",
    "        \n",
    "        relu_Z=self._relu(Z)\n",
    "        \n",
    "        dZ=np.multiply(dA, relu_Z)\n",
    "        \n",
    "        return(dZ)\n",
    "        \n",
    "        \n",
    "\n",
    "    def _binary_cross_entropy(self, y: ArrayLike, y_hat: ArrayLike, error=1e-5) -> float: #DONE\n",
    "        \"\"\"\n",
    "        Binary cross entropy loss function.\n",
    "\n",
    "        Args:\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output.\n",
    "            y: ArrayLike\n",
    "                Ground truth output.\n",
    "\n",
    "        Returns:\n",
    "            loss: float\n",
    "                Average loss over mini-batch.\n",
    "        \"\"\"\n",
    "        #add error so no divide by zero warning\n",
    "        bce_loss= -np.mean(y*(np.log(y_hat + error)) +  (1-y)*np.log(1-y_hat + error)) \n",
    "        return(bce_loss)\n",
    "    \n",
    "\n",
    "    def _binary_cross_entropy_backprop(self, y: ArrayLike, y_hat: ArrayLike, error=-1e5) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        Binary cross entropy loss function derivative for backprop.\n",
    "\n",
    "        Args:\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output.\n",
    "            y: ArrayLike\n",
    "                Ground truth output.\n",
    "\n",
    "        Returns:\n",
    "            dA: ArrayLike\n",
    "                partial derivative of loss with respect to A matrix.\n",
    "        \"\"\"\n",
    "        bce=_binary_cross_entropy(y, y_hat)\n",
    "        \n",
    "        #i had to google this i'm sorry\n",
    "        #add error for divide by zero \n",
    "        #need to double check this i think it's wrong \n",
    "        dA=np.mean(-(y/y_hat + error) + (1-y)/(1-y_hat+error))\n",
    "        \n",
    "        return(dA)\n",
    "     \n",
    "        \n",
    "    \n",
    "\n",
    "    def _mean_squared_error(self, y: ArrayLike, y_hat: ArrayLike) -> float:\n",
    "        \"\"\"\n",
    "        Mean squared error loss.\n",
    "\n",
    "        Args:\n",
    "            y: ArrayLike\n",
    "                Ground truth output.\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output.\n",
    "\n",
    "        Returns:\n",
    "            loss: float\n",
    "                Average loss of mini-batch.\n",
    "        \"\"\"\n",
    "\n",
    "        mse = np.mean((y - y_hat) ** 2)\n",
    "        return(mse)\n",
    "\n",
    "    def _mean_squared_error_backprop(self, y: ArrayLike, y_hat: ArrayLike) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        Mean square error loss derivative for backprop.\n",
    "\n",
    "        Args:\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output.\n",
    "            y: ArrayLike\n",
    "                Ground truth output.\n",
    "\n",
    "        Returns:\n",
    "            dA: ArrayLike\n",
    "                partial derivative of loss with respect to A matrix.\n",
    "        \"\"\"\n",
    "        \n",
    "        dA=np.mean(2*(y_hat-y))\n",
    "        return(dA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_NN = NeuralNetwork(nn_arch = [{'input_dim': 64, 'output_dim': 16, 'activation': 'relu'}, \n",
    "                                      {'input_dim': 16, 'output_dim': 64, 'activation': 'sigmoid'}],\n",
    "                            lr = 0.01, seed = 3, batch_size = 100, epochs = 10, loss_function='mse')\n",
    "\n",
    "\n",
    "#params are arrays of W1-Wx where x is feature number\n",
    "#and also an array for b1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (359, 64)\n",
      "y_train shape (359, 64)\n",
      "X_train shape (90, 64)\n",
      "y_train shape (90, 64)\n",
      "layer 1\n",
      "relu\n",
      "layer 2\n",
      "sigmoid\n",
      "y_pred shape (90, 64)\n",
      "layer:  1\n",
      "dW_curr shape (64, 16)\n",
      "dA_prev shape (64, 90)\n",
      "db_curr shape (90,)\n",
      "param W shape (16, 64)\n",
      "param b shape (16, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (16,64) (64,16) (16,64) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-233-6b4031ea1024>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mper_epoch_loss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_epoch_loss_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexample_NN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-231-63ac85d60b1a>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m                 \u001b[0;31m#update parameter weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                 \u001b[0;31m#is that it?????????/?sdflkajdslkfjalskdflaksdjf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-231-63ac85d60b1a>\u001b[0m in \u001b[0;36m_update_params\u001b[0;34m(self, grad_dict)\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0;31m#idk if the indices are right\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0;31m#update based on learning rate and gradient for that node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (16,64) (64,16) (16,64) "
     ]
    }
   ],
   "source": [
    "per_epoch_loss_train, per_epoch_loss_val=example_NN.fit(X_train, X_train, X_test, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_arch = [{'input_dim': 64, 'output_dim': 16, 'activation': 'relu'}, \n",
    "           {'input_dim': 16, 'output_dim': 64, 'activation': 'sigmoid'}]\n",
    "#nn_arch[0]['input_dim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(359, 64)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#digits.data is shape (1794, 64) --> 64 is flattened image pixels \n",
    "digits = load_digits()\n",
    "  \n",
    "#split into train and test sets \n",
    "X_train, X_test, y_train, y_test=train_test_split(digits.data, digits.target, train_size=0.2, random_state=3)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_dim': 16, 'output_dim': 64, 'activation:': 'sigmoid'}"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 3 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-930d8ebf3108>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#YOU FOOL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#BC IT'S AN AUTOENCODER INPUT AND OUTPUT SHOULD BE THE SAME GLAKSDJF'LAKSDFJ'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mper_epoch_loss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_epoch_loss_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexample_NN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#ah yes nothign works\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-93-d6fa61179add>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;31m#this take nfrom hw7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m             \u001b[0;31m# Shuffling the training data for each epoch of training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m             \u001b[0mshuffle_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshuffle_arr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 3 dimension(s)"
     ]
    }
   ],
   "source": [
    "#YOU FOOL\n",
    "#BC IT'S AN AUTOENCODER INPUT AND OUTPUT SHOULD BE THE SAME GLAKSDJF'LAKSDFJ'\n",
    "per_epoch_loss_train, per_epoch_loss_val=example_NN.fit(X_train, X_train, X_test, X_test)\n",
    "#ah yes nothign works "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 5 1 4 9 0 2 6 7 8]\n"
     ]
    }
   ],
   "source": [
    "idx=np.arange(10)\n",
    "np.random.shuffle(idx)\n",
    "print(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taken from hw7\n",
    "fig, axs = plt.subplots(2, figsize=(8, 8))\n",
    "    fig.suptitle('Loss History')\n",
    "    axs[0].plot(np.arange(len(self.loss_hist_train)), self.loss_hist_train)\n",
    "    axs[0].set_title('Training')\n",
    "    axs[1].plot(np.arange(len(self.loss_hist_val)), self.loss_hist_val)\n",
    "    axs[1].set_title('Validation')\n",
    "    plt.xlabel('Steps')\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "3\n",
      "2\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
