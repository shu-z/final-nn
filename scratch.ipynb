{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "from numpy.typing import ArrayLike\n",
    "import sklearn \n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Union\n",
    "from numpy.typing import ArrayLike\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    This is a class that generates a fully-connected neural network.\n",
    "\n",
    "    Parameters:\n",
    "        nn_arch: List[Dict[str, float]]\n",
    "            A list of dictionaries describing the layers of the neural network.\n",
    "            e.g. [{'input_dim': 64, 'output_dim': 32, 'activation': 'relu'}, {'input_dim': 32, 'output_dim': 8, 'activation:': 'sigmoid'}]\n",
    "            will generate a two-layer deep network with an input dimension of 64, a 32 dimension hidden layer, and an 8 dimensional output.\n",
    "        lr: float\n",
    "            Learning rate (alpha).\n",
    "        seed: int\n",
    "            Random seed to ensure reproducibility.\n",
    "        batch_size: int\n",
    "            Size of mini-batches used for training.\n",
    "        epochs: int\n",
    "            Max number of epochs for training.\n",
    "        loss_function: str\n",
    "            Name of loss function.\n",
    "\n",
    "    Attributes:\n",
    "        arch: list of dicts\n",
    "            (see nn_arch above)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        #this throwing error fix later\n",
    "        #nn_arch: List[Dict[str, Union(int, str)]],\n",
    "        nn_arch: List, \n",
    "        lr: float,\n",
    "        seed: int,\n",
    "        batch_size: int,\n",
    "        epochs: int,\n",
    "        loss_function: str\n",
    "    ):\n",
    "\n",
    "        # Save architecture\n",
    "        self.arch = nn_arch\n",
    "\n",
    "        # Save hyperparameters\n",
    "        self._lr = lr\n",
    "        self._seed = seed\n",
    "        self._epochs = epochs\n",
    "        self._loss_func = loss_function\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "        # Initialize the parameter dictionary for use in training\n",
    "        self._param_dict = self._init_params()\n",
    "        #print(self._param_dict)\n",
    "\n",
    "    def _init_params(self) -> Dict[str, ArrayLike]:\n",
    "        \"\"\"\n",
    "        DO NOT MODIFY THIS METHOD! IT IS ALREADY COMPLETE!\n",
    "\n",
    "        This method generates the parameter matrices for all layers of\n",
    "        the neural network. This function returns the param_dict after\n",
    "        initialization.\n",
    "\n",
    "        Returns:\n",
    "            param_dict: Dict[str, ArrayLike]\n",
    "                Dictionary of parameters in neural network.\n",
    "        \"\"\"\n",
    "\n",
    "        # Seed NumPy\n",
    "        np.random.seed(self._seed)\n",
    "\n",
    "        # Define parameter dictionary\n",
    "        param_dict = {}\n",
    "\n",
    "        # Initialize each layer's weight matrices (W) and bias matrices (b)\n",
    "        for idx, layer in enumerate(self.arch):\n",
    "            layer_idx = idx + 1\n",
    "            input_dim = layer['input_dim']\n",
    "            output_dim = layer['output_dim']\n",
    "            param_dict['W' + str(layer_idx)] = np.random.randn(output_dim, input_dim) * 0.1\n",
    "            param_dict['b' + str(layer_idx)] = np.random.randn(output_dim, 1) * 0.1\n",
    "\n",
    "        return param_dict\n",
    "\n",
    "    def _single_forward(\n",
    "        self,\n",
    "        W_curr: ArrayLike,\n",
    "        b_curr: ArrayLike,\n",
    "        A_prev: ArrayLike,\n",
    "        activation: str\n",
    "    ) -> Tuple[ArrayLike, ArrayLike]:\n",
    "        \"\"\"\n",
    "        This method is used for a single forward pass on a single layer.\n",
    "\n",
    "        Args:\n",
    "            W_curr: ArrayLike\n",
    "                Current layer weight matrix.\n",
    "            b_curr: ArrayLike\n",
    "                Current layer bias matrix.\n",
    "            A_prev: ArrayLike\n",
    "                Previous layer activation matrix.\n",
    "            activation: str\n",
    "                Name of activation function for current layer.\n",
    "\n",
    "        Returns:\n",
    "            A_curr: ArrayLike\n",
    "                Current layer activation matrix.\n",
    "            Z_curr: ArrayLike\n",
    "                Current layer linear transformed matrix.\n",
    "        \"\"\"\n",
    "        \n",
    "        #need to multiply every node in last layer with weights \n",
    "        #then apply activation function \n",
    "        \n",
    "        \n",
    "        #Z is before A\n",
    "        Z_curr=np.dot(A_prev, W_curr.T) + b_curr.T\n",
    "        \n",
    "        #apply activation function \n",
    "        \n",
    "        if activation == 'sigmoid':\n",
    "            A_curr=self._sigmoid(Z_curr)\n",
    "        \n",
    "        \n",
    "        if activation =='relu':\n",
    "            A_curr=self._relu(Z_curr)\n",
    "                    \n",
    "        else:\n",
    "            raise Exception('no suitable activation function chosen!!!!!!!1!!!!')\n",
    "        \n",
    "        \n",
    "        \n",
    "        return(A_curr, Z_curr)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    def forward(self, X: ArrayLike) -> Tuple[ArrayLike, Dict[str, ArrayLike]]:\n",
    "        \"\"\"\n",
    "        This method is responsible for one forward pass of the entire neural network.\n",
    "\n",
    "        Args:\n",
    "            X: ArrayLike\n",
    "                Input matrix with shape [batch_size, features].\n",
    "\n",
    "        Returns:\n",
    "            output: ArrayLike\n",
    "                Output of forward pass.\n",
    "            cache: Dict[str, ArrayLike]:\n",
    "                Dictionary storing Z and A matrices from `_single_forward` for use in backprop.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        cache={}\n",
    "        \n",
    "        \n",
    "        #for the first layer \n",
    "        A_prev=X\n",
    "        #add to cache\n",
    "        cache['A0']=X\n",
    "\n",
    "        \n",
    "        #figure out what the range should be\n",
    "        #loop through remaining number of layers in nn? \n",
    "        for l in range(1,len(self.arch)):\n",
    "            \n",
    "            #need to get W, b!!!!!!!!!!!\n",
    "            #from param dict \n",
    "            \n",
    "            W_curr=self._param_dict['W'+str(l)]\n",
    "            b_curr=self._param_dict['b'+str(l)]\n",
    "            #check that this is right layer?? i think its wrong \n",
    "            activation=self.arch[l-1]['activation']\n",
    "            \n",
    "            #go through one step of feed forward \n",
    "            A_curr, Z_curr=self._single_forward(W_curr, b_curr, A_prev, activation)\n",
    "            output=0\n",
    "            \n",
    "            cache['A'+str(l)]=A_curr\n",
    "            cache['Z'+str(l)]=Z_curr\n",
    "            \n",
    "            output=A_curr\n",
    "            A_prev=A_curr\n",
    "            \n",
    "            return(output, cache)\n",
    "            \n",
    "\n",
    "    def _single_backprop(\n",
    "        self,\n",
    "        W_curr: ArrayLike,\n",
    "        b_curr: ArrayLike,\n",
    "        Z_curr: ArrayLike,\n",
    "        A_prev: ArrayLike,\n",
    "        dA_curr: ArrayLike,\n",
    "        activation_curr: str\n",
    "    ) -> Tuple[ArrayLike, ArrayLike, ArrayLike]:\n",
    "        \"\"\"\n",
    "        This method is used for a single backprop pass on a single layer.\n",
    "\n",
    "        Args:\n",
    "            W_curr: ArrayLike\n",
    "                Current layer weight matrix.\n",
    "            b_curr: ArrayLike\n",
    "                Current layer bias matrix.\n",
    "            Z_curr: ArrayLike\n",
    "                Current layer linear transform matrix.\n",
    "            A_prev: ArrayLike\n",
    "                Previous layer activation matrix.\n",
    "            dA_curr: ArrayLike\n",
    "                Partial derivative of loss function with respect to current layer activation matrix.\n",
    "            activation_curr: str\n",
    "                Name of activation function of layer.\n",
    "\n",
    "        Returns:\n",
    "            dA_prev: ArrayLike\n",
    "                Partial derivative of loss function with respect to previous layer activation matrix.\n",
    "            dW_curr: ArrayLike\n",
    "                Partial derivative of loss function with respect to current layer weight matrix.\n",
    "            db_curr: ArrayLike\n",
    "                Partial derivative of loss function with respect to current layer bias matrix.\n",
    "        \"\"\"\n",
    "        \n",
    "        if activation_curr=='sigmoid':\n",
    "            dZ=_sigmoid_backprop(dA_curr, Z_curr)\n",
    "        \n",
    "        if activation_curr=='relu':\n",
    "            dZ=_relu_backprop(dA_curr, Z_curr)\n",
    "        else:\n",
    "            raise Exception ('no suitable activation function chosen!! check ur spelling')\n",
    "            \n",
    "            \n",
    "        dW_curr=np.dot(dZ, A_prev.T)\n",
    "        db_curr=np.mean(dZ)\n",
    "        dA_prev=np.dot(W_curr.T, dZ)\n",
    "        \n",
    "        \n",
    "        return(dA_prev, dW_curr, db_curr)\n",
    "        \n",
    "\n",
    "\n",
    "    def backprop(self, y: ArrayLike, y_hat: ArrayLike, cache: Dict[str, ArrayLike]):\n",
    "        \"\"\"\n",
    "        This method is responsible for the backprop of the whole fully connected neural network.\n",
    "\n",
    "        Args:\n",
    "            y (array-like):\n",
    "                Ground truth labels.\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output values.\n",
    "            cache: Dict[str, ArrayLike]\n",
    "                Dictionary containing the information about the\n",
    "                most recent forward pass, specifically A and Z matrices.\n",
    "\n",
    "        Returns:\n",
    "            grad_dict: Dict[str, ArrayLike]\n",
    "                Dictionary containing the gradient information from this pass of backprop.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        test1=self._single_backprop( W_curr, b_curr, Z_curr,A_prev, dA_curr, activation_curr)\n",
    "        \n",
    "        \n",
    "        #go b a c k w a r d s \n",
    "        for l in range(len(self.arch), 0, -1):\n",
    "            \n",
    "            #need to get W, b!!!!!!!!!!!\n",
    "            #from param dict \n",
    "            \n",
    "            W_curr=self._param_dict['W'+str(l)]\n",
    "            b_curr=self._param_dict['b'+str(l)]\n",
    "            #check that this is right layer?? i think its wrong \n",
    "            activation=self.arch[l-1]['activation']\n",
    "            \n",
    "            #go through one step of feed forward \n",
    "            A_curr, Z_curr=self._single_forward(W_curr, b_curr, A_prev, activation)\n",
    "            output=0\n",
    "            \n",
    "            cache['A'+str(l)]=A_curr\n",
    "            cache['Z'+str(l)]=Z_curr\n",
    "            \n",
    "            output=A_curr\n",
    "            A_prev=A_curr\n",
    "            \n",
    "            return(output, cache)\n",
    "            \n",
    "\n",
    "    def _update_params(self, grad_dict: Dict[str, ArrayLike]):\n",
    "        \"\"\"\n",
    "        This function updates the parameters in the neural network after backprop. This function\n",
    "        only modifies internal attributes and does not return anything\n",
    "\n",
    "        Args:\n",
    "            grad_dict: Dict[str, ArrayLike]\n",
    "                Dictionary containing the gradient information from most recent round of backprop.\n",
    "        \"\"\"\n",
    "        \n",
    "        #go through each layer and get corresponding gradient for each node\n",
    "        #update based on learning rate and gradient\n",
    "        for idx, layer in enumerate(self.arch):\n",
    "            layer_idx = idx + 1\n",
    "            input_dim = layer['input_dim']\n",
    "            output_dim = layer['output_dim']\n",
    "            \n",
    "            #this part below is wrong\n",
    "            self._param_dict['W' + str(layer_idx)] = np.random.randn(output_dim, input_dim) * 0.1\n",
    "            self._param_dict['b' + str(layer_idx)] = np.random.randn(output_dim, 1) * 0.1\n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X_train: ArrayLike,\n",
    "        y_train: ArrayLike,\n",
    "        X_val: ArrayLike,\n",
    "        y_val: ArrayLike\n",
    "    ) -> Tuple[List[float], List[float]]:\n",
    "        \"\"\"\n",
    "        This function trains the neural network by backpropagation for the number of epochs defined at\n",
    "        the initialization of this class instance.\n",
    "\n",
    "        Args:\n",
    "            X_train: ArrayLike\n",
    "                Input features of training set.\n",
    "            y_train: ArrayLike\n",
    "                Labels for training set.\n",
    "            X_val: ArrayLike\n",
    "                Input features of validation set.\n",
    "            y_val: ArrayLike\n",
    "                Labels for validation set.\n",
    "\n",
    "        Returns:\n",
    "            per_epoch_loss_train: List[float]\n",
    "                List of per epoch loss for training set.\n",
    "            per_epoch_loss_val: List[float]\n",
    "                List of per epoch loss for validation set.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        #init lists to store losses \n",
    "        per_epoch_loss_train=[]\n",
    "        per_epoch_loss_val=[]\n",
    "        \n",
    "        \n",
    "        #everything below is adapated from homework 7\n",
    "        \n",
    "        \n",
    "        # Padding data with vector of ones for bias term\n",
    "        X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "        X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "         \n",
    "            \n",
    "        for epoch in range(self._epochs):\n",
    "             \n",
    "            # Shuffling the training data for each epoch of training\n",
    "            shuffle_arr = np.concatenate([X_train, np.expand_dims(y_train, 1)], axis=1)\n",
    "            np.random.shuffle(shuffle_arr)\n",
    "            X_train = shuffle_arr[:, :-1]\n",
    "            y_train = shuffle_arr[:, -1].flatten()\n",
    "                      \n",
    "            # Create batches\n",
    "            num_batches = int(X_train.shape[0] / self._batch_size) + 1\n",
    "            X_batch = np.array_split(X_train, num_batches)\n",
    "            y_batch = np.array_split(y_train, num_batches)\n",
    "            \n",
    "            \n",
    "            #store losses for each training batch \n",
    "            batch_loss_train=[]\n",
    "            \n",
    "            # Iterate through batches (one of these loops is one epoch of training)\n",
    "            for X_train, y_train in zip(X_batch, y_batch):\n",
    "\n",
    "                # Make prediction and calculate loss\n",
    "                \n",
    "                #predict is essentially one forward pass \n",
    "                y_pred, cache = self.forward(X_train)\n",
    "                \n",
    "                \n",
    "                \n",
    "                #calculate loss \n",
    "                if self._loss_func =='mse':\n",
    "                    train_loss=self_mean_squared_error(y=y_batch, y_hat=y_pred)\n",
    "                    \n",
    "                elif self._loss_func =='bce':\n",
    "                    train_loss=_binary_cross_entropy(y=y_batch, y_hat=y_pred)\n",
    "                    \n",
    "                else:\n",
    "                    raise Exception('No availalbe loss function chosen')\n",
    "                  \n",
    "                #add to batch loss list \n",
    "                batch_loss_train.append(train_loss)\n",
    "                \n",
    "                \n",
    "                #then, backpropagate \n",
    "                #idk where cache comes from rn\n",
    "                backprop=self.backprop(y=y_batch, y_hat=y_pred, cache)\n",
    "                    \n",
    "                #THIS IS ALL WRONG \n",
    "                # Update weights\n",
    "                prev_W = self.W\n",
    "                grad = self.calculate_gradient(y_train, X_train)\n",
    "                new_W = prev_W - self.lr * grad \n",
    "                self.W = new_W\n",
    "\n",
    "                # Save parameter update size\n",
    "                update_sizes.append(np.abs(new_W - prev_W))\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "    \n",
    "            #after running all train batches\n",
    "        \n",
    "            #get mean of batch losses and add to epoch loss\n",
    "            per_epoch_loss_train=np.mean(batch_loss_train)\n",
    "            \n",
    "            \n",
    "            #Then, validation\n",
    "            #Make one prediction on val for each epoch \n",
    "            #idk if this is how it should be \n",
    "            \n",
    "            val_pred = self.predict(X_val)\n",
    "            \n",
    "            \n",
    "            if self._loss_func =='mse':\n",
    "                val_loss=self_mean_squared_error(y=y_val, y_hat=val_pred)          \n",
    "            elif self._loss_func =='bce':\n",
    "                val_loss=_binary_cross_entropy(y=y_val, y_hat=val_pred)\n",
    "            else:\n",
    "                raise Exception('No availalbe loss function chosen')\n",
    "                \n",
    "                \n",
    "            #add loss to per epoch loss val\n",
    "            per_epoch_loss_val.append(val_loss)\n",
    "\n",
    "            \n",
    "        #return training and validation losses\n",
    "        return(per_epoch_loss_train, per_epoch_loss_val)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, X: ArrayLike) -> ArrayLike: #DONE\n",
    "        \"\"\"\n",
    "        This function returns the prediction of the neural network.\n",
    "\n",
    "        Args:\n",
    "            X: ArrayLike\n",
    "                Input data for prediction.\n",
    "\n",
    "        Returns:\n",
    "            y_hat: ArrayLike\n",
    "                Prediction from the model.\n",
    "        \"\"\"\n",
    "        \n",
    "        y_hat,_ =self.forward(X)\n",
    "        return y_hat\n",
    "\n",
    "    def _sigmoid(self, Z: ArrayLike) -> ArrayLike: #DONE\n",
    "        \"\"\"\n",
    "        Sigmoid activation function.\n",
    "\n",
    "        Args:\n",
    "            Z: ArrayLike\n",
    "                Output of layer linear transform.\n",
    "\n",
    "        Returns:\n",
    "            nl_transform: ArrayLike\n",
    "                Activation function output.\n",
    "        \"\"\"\n",
    "        return((1/(1+np.exp(-Z))))\n",
    "\n",
    "    def _sigmoid_backprop(self, dA: ArrayLike, Z: ArrayLike): #DONE\n",
    "        \"\"\"\n",
    "        Sigmoid derivative for backprop.\n",
    "\n",
    "        Args:\n",
    "            dA: ArrayLike\n",
    "                Partial derivative of previous layer activation matrix.\n",
    "            Z: ArrayLike\n",
    "                Output of layer linear transform.\n",
    "\n",
    "        Returns:\n",
    "            dZ: ArrayLike\n",
    "                Partial derivative of current layer Z matrix.\n",
    "        \"\"\"\n",
    "        \n",
    "        sig_Z=self._sigmoid(Z)\n",
    "        dZ=dA*sig_Z*(1-sig_Z)\n",
    "        \n",
    "        return(dZ)\n",
    "    \n",
    "\n",
    "    def _relu(self, Z: ArrayLike) -> ArrayLike: #DONE\n",
    "        \"\"\"\n",
    "        ReLU activation function.\n",
    "\n",
    "        Args:\n",
    "            Z: ArrayLike\n",
    "                Output of layer linear transform.\n",
    "\n",
    "        Returns:\n",
    "            nl_transform: ArrayLike\n",
    "                Activation function output.\n",
    "        \"\"\"\n",
    "        return(np.maximum(0, Z))\n",
    "    \n",
    "\n",
    "    def _relu_backprop(self, dA: ArrayLike, Z: ArrayLike) -> ArrayLike: #DONE I THINK\n",
    "        \"\"\"\n",
    "        ReLU derivative for backprop.\n",
    "\n",
    "        Args:\n",
    "            dA: ArrayLike\n",
    "                Partial derivative of previous layer activation matrix.\n",
    "            Z: ArrayLike\n",
    "                Output of layer linear transform.\n",
    "\n",
    "        Returns:\n",
    "            dZ: ArrayLike\n",
    "                Partial derivative of current layer Z matrix.\n",
    "        \"\"\"\n",
    "        \n",
    "        relu_Z=self._relu(Z)\n",
    "        \n",
    "        dZ=np.multiply(dA, relu_Z)\n",
    "        \n",
    "        return(dZ)\n",
    "        \n",
    "        \n",
    "\n",
    "    def _binary_cross_entropy(self, y: ArrayLike, y_hat: ArrayLike, error=1e-5) -> float: #DONE\n",
    "        \"\"\"\n",
    "        Binary cross entropy loss function.\n",
    "\n",
    "        Args:\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output.\n",
    "            y: ArrayLike\n",
    "                Ground truth output.\n",
    "\n",
    "        Returns:\n",
    "            loss: float\n",
    "                Average loss over mini-batch.\n",
    "        \"\"\"\n",
    "        #add error so no divide by zero warning\n",
    "        bce_loss= -np.mean(y*(np.log(y_hat + error)) +  (1-y)*np.log(1-y_hat + error)) \n",
    "        return(bce_loss)\n",
    "    \n",
    "\n",
    "    def _binary_cross_entropy_backprop(self, y: ArrayLike, y_hat: ArrayLike, error=-1e5) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        Binary cross entropy loss function derivative for backprop.\n",
    "\n",
    "        Args:\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output.\n",
    "            y: ArrayLike\n",
    "                Ground truth output.\n",
    "\n",
    "        Returns:\n",
    "            dA: ArrayLike\n",
    "                partial derivative of loss with respect to A matrix.\n",
    "        \"\"\"\n",
    "        bce=_binary_cross_entropy(y, y_hat)\n",
    "        \n",
    "        #i had to google this i'm sorry\n",
    "        #add error for divide by zero \n",
    "        dA=np.mean(-(y/y_hat + error) + (1-y)/(1-y_hat+error))\n",
    "        \n",
    "        return()\n",
    "        #idk man\n",
    "        pass\n",
    "        \n",
    "    \n",
    "\n",
    "    def _mean_squared_error(self, y: ArrayLike, y_hat: ArrayLike) -> float:\n",
    "        \"\"\"\n",
    "        Mean squared error loss.\n",
    "\n",
    "        Args:\n",
    "            y: ArrayLike\n",
    "                Ground truth output.\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output.\n",
    "\n",
    "        Returns:\n",
    "            loss: float\n",
    "                Average loss of mini-batch.\n",
    "        \"\"\"\n",
    "\n",
    "        mse = np.mean((y - y_hat) ** 2)\n",
    "        return(mse)\n",
    "\n",
    "    def _mean_squared_error_backprop(self, y: ArrayLike, y_hat: ArrayLike) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        Mean square error loss derivative for backprop.\n",
    "\n",
    "        Args:\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output.\n",
    "            y: ArrayLike\n",
    "                Ground truth output.\n",
    "\n",
    "        Returns:\n",
    "            dA: ArrayLike\n",
    "                partial derivative of loss with respect to A matrix.\n",
    "        \"\"\"\n",
    "        \n",
    "        dA=np.mean(2*(y_hat-y))\n",
    "        return(dA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_NN = NeuralNetwork(nn_arch = [{'input_dim': 64, 'output_dim': 16, 'activation': 'relu'}, \n",
    "                                      {'input_dim': 16, 'output_dim': 64, 'activation:': 'sigmoid'}],\n",
    "                            lr = 0.01, seed = 3, batch_size = 100, epochs = 10, loss_function='log loss')\n",
    "\n",
    "\n",
    "#params are arrays of W1-Wx where x is feature number\n",
    "#and also an array for b1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_arch = [{'input_dim': 64, 'output_dim': 16, 'activation': 'relu'}, \n",
    "            {'input_dim': 16, 'output_dim': 64, 'activation:': 'sigmoid'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{'input_dim': 64, 'output_dim': 16, 'activation': 'relu'}\n",
      "1\n",
      "{'input_dim': 16, 'output_dim': 64, 'activation:': 'sigmoid'}\n"
     ]
    }
   ],
   "source": [
    "nn_arch[0]['input_dim']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#digits.data is shape (1794, 64) --> 64 is flattened image pixels \n",
    "digits = load_digits()\n",
    "  \n",
    "#split into train and test sets \n",
    "X_train, X_test, y_train, y_test=train_test_split(digits.data, digits.target, train_size=0.2, random_state=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NeuralNetwork' object has no attribute 'make_prediction'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-94a5e4017faf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexample_NN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-8176e1b57d3a>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;31m# Make prediction and calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_hist_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NeuralNetwork' object has no attribute 'make_prediction'"
     ]
    }
   ],
   "source": [
    "per_epoch_loss_train, per_epoch_loss_val=example_NN.fit(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taken from hw7\n",
    "fig, axs = plt.subplots(2, figsize=(8, 8))\n",
    "    fig.suptitle('Loss History')\n",
    "    axs[0].plot(np.arange(len(self.loss_hist_train)), self.loss_hist_train)\n",
    "    axs[0].set_title('Training')\n",
    "    axs[1].plot(np.arange(len(self.loss_hist_val)), self.loss_hist_val)\n",
    "    axs[1].set_title('Validation')\n",
    "    plt.xlabel('Steps')\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "3\n",
      "2\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
