{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib.plt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-294-38a868d748a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_digits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib.plt'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Union\n",
    "from numpy.typing import ArrayLike\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    This is a class that generates a fully-connected neural network.\n",
    "\n",
    "    Parameters:\n",
    "        nn_arch: List[Dict[str, float]]\n",
    "            A list of dictionaries describing the layers of the neural network.\n",
    "            e.g. [{'input_dim': 64, 'output_dim': 32, 'activation': 'relu'}, {'input_dim': 32, 'output_dim': 8, 'activation:': 'sigmoid'}]\n",
    "            will generate a two-layer deep network with an input dimension of 64, a 32 dimension hidden layer, and an 8 dimensional output.\n",
    "        lr: float\n",
    "            Learning rate (alpha).\n",
    "        seed: int\n",
    "            Random seed to ensure reproducibility.\n",
    "        batch_size: int\n",
    "            Size of mini-batches used for training.\n",
    "        epochs: int\n",
    "            Max number of epochs for training.\n",
    "        loss_function: str\n",
    "            Name of loss function.\n",
    "\n",
    "    Attributes:\n",
    "        arch: list of dicts\n",
    "            (see nn_arch above)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        #this throwing error fix later\n",
    "        #nn_arch: List[Dict[str, Union(int, str)]],\n",
    "        nn_arch: List, \n",
    "        lr: float,\n",
    "        seed: int,\n",
    "        batch_size: int,\n",
    "        epochs: int,\n",
    "        loss_function: str\n",
    "    ):\n",
    "\n",
    "        # Save architecture\n",
    "        self.arch = nn_arch\n",
    "\n",
    "        # Save hyperparameters\n",
    "        self._lr = lr\n",
    "        self._seed = seed\n",
    "        self._epochs = epochs\n",
    "        self._loss_func = loss_function\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "        # Initialize the parameter dictionary for use in training\n",
    "        self._param_dict = self._init_params()\n",
    "        #print(self._param_dict)\n",
    "\n",
    "    def _init_params(self) -> Dict[str, ArrayLike]:\n",
    "        \"\"\"\n",
    "        DO NOT MODIFY THIS METHOD! IT IS ALREADY COMPLETE!\n",
    "\n",
    "        This method generates the parameter matrices for all layers of\n",
    "        the neural network. This function returns the param_dict after\n",
    "        initialization.\n",
    "\n",
    "        Returns:\n",
    "            param_dict: Dict[str, ArrayLike]\n",
    "                Dictionary of parameters in neural network.\n",
    "        \"\"\"\n",
    "\n",
    "        # Seed NumPy\n",
    "        np.random.seed(self._seed)\n",
    "\n",
    "        # Define parameter dictionary\n",
    "        param_dict = {}\n",
    "\n",
    "        # Initialize each layer's weight matrices (W) and bias matrices (b)\n",
    "        for idx, layer in enumerate(self.arch):\n",
    "            layer_idx = idx + 1\n",
    "            input_dim = layer['input_dim']\n",
    "            output_dim = layer['output_dim']\n",
    "            param_dict['W' + str(layer_idx)] = np.random.randn(output_dim, input_dim) * 0.1\n",
    "            param_dict['b' + str(layer_idx)] = np.random.randn(output_dim, 1) * 0.1\n",
    "\n",
    "        return param_dict\n",
    "\n",
    "    def _single_forward(\n",
    "        self,\n",
    "        W_curr: ArrayLike,\n",
    "        b_curr: ArrayLike,\n",
    "        A_prev: ArrayLike,\n",
    "        activation: str\n",
    "    ) -> Tuple[ArrayLike, ArrayLike]:\n",
    "        \"\"\"\n",
    "        This method is used for a single forward pass on a single layer.\n",
    "\n",
    "        Args:\n",
    "            W_curr: ArrayLike\n",
    "                Current layer weight matrix.\n",
    "            b_curr: ArrayLike\n",
    "                Current layer bias matrix.\n",
    "            A_prev: ArrayLike\n",
    "                Previous layer activation matrix.\n",
    "            activation: str\n",
    "                Name of activation function for current layer.\n",
    "\n",
    "        Returns:\n",
    "            A_curr: ArrayLike\n",
    "                Current layer activation matrix.\n",
    "            Z_curr: ArrayLike\n",
    "                Current layer linear transformed matrix.\n",
    "        \"\"\"\n",
    "        \n",
    "        #need to multiply every node in last layer with weights \n",
    "        #then apply activation function \n",
    "        \n",
    "        \n",
    "        #Z is before A\n",
    "        Z_curr=np.dot(A_prev, W_curr.T) + b_curr.T\n",
    "        \n",
    "        #apply activation function \n",
    "        \n",
    "        if activation =='sigmoid':\n",
    "            A_curr=self._sigmoid(Z_curr)\n",
    "        \n",
    "        \n",
    "        elif activation =='relu':\n",
    "            A_curr=self._relu(Z_curr)\n",
    "                    \n",
    "        else:\n",
    "            raise Exception('no suitable activation function chosen!!!!!!!1!!!!')\n",
    "        \n",
    "        \n",
    "        \n",
    "        return(A_curr, Z_curr)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    def forward(self, X: ArrayLike) -> Tuple[ArrayLike, Dict[str, ArrayLike]]:\n",
    "        \"\"\"\n",
    "        This method is responsible for one forward pass of the entire neural network.\n",
    "\n",
    "        Args:\n",
    "            X: ArrayLike\n",
    "                Input matrix with shape [batch_size, features].\n",
    "\n",
    "        Returns:\n",
    "            output: ArrayLike\n",
    "                Output of forward pass.\n",
    "            cache: Dict[str, ArrayLike]:\n",
    "                Dictionary storing Z and A matrices from `_single_forward` for use in backprop.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        cache={}\n",
    "                \n",
    "        #for the first layer \n",
    "        A_prev=X\n",
    "        #add to cache\n",
    "        cache['A0']=X\n",
    "\n",
    "        \n",
    "        #loop through remaining number of layers in nn\n",
    "        for l in range(1,len(self.arch)+1):\n",
    "            #print('layer',  l)\n",
    "            \n",
    "\n",
    "            #weights \n",
    "            W_curr=self._param_dict['W'+str(l)]\n",
    "            b_curr=self._param_dict['b'+str(l)]\n",
    "            \n",
    "            activation=self.arch[l-1]['activation']\n",
    "            \n",
    "            #go through one step of feed forward \n",
    "            #remember that A is activation matrix, Z is linearly transformed matrix\n",
    "            A_curr, Z_curr=self._single_forward(W_curr, b_curr, A_prev, activation)\n",
    "\n",
    "            \n",
    "            cache['A'+str(l)]=A_curr\n",
    "            cache['Z'+str(l)]=Z_curr\n",
    "            \n",
    "            \n",
    "            A_prev=A_curr\n",
    "            \n",
    "        output=A_curr\n",
    "            \n",
    "        return(output, cache)\n",
    "            \n",
    "\n",
    "    def _single_backprop(\n",
    "        self,\n",
    "        W_curr: ArrayLike,\n",
    "        b_curr: ArrayLike,\n",
    "        Z_curr: ArrayLike,\n",
    "        A_prev: ArrayLike,\n",
    "        dA_curr: ArrayLike,\n",
    "        activation_curr: str\n",
    "    ) -> Tuple[ArrayLike, ArrayLike, ArrayLike]:\n",
    "        \"\"\"\n",
    "        This method is used for a single backprop pass on a single layer.\n",
    "\n",
    "        Args:\n",
    "            W_curr: ArrayLike\n",
    "                Current layer weight matrix.\n",
    "            b_curr: ArrayLike\n",
    "                Current layer bias matrix.\n",
    "            Z_curr: ArrayLike\n",
    "                Current layer linear transform matrix.\n",
    "            A_prev: ArrayLike\n",
    "                Previous layer activation matrix.\n",
    "            dA_curr: ArrayLike\n",
    "                Partial derivative of loss function with respect to current layer activation matrix.\n",
    "            activation_curr: str\n",
    "                Name of activation function of layer.\n",
    "\n",
    "        Returns:\n",
    "            dA_prev: ArrayLike\n",
    "                Partial derivative of loss function with respect to previous layer activation matrix.\n",
    "            dW_curr: ArrayLike\n",
    "                Partial derivative of loss function with respect to current layer weight matrix.\n",
    "            db_curr: ArrayLike\n",
    "                Partial derivative of loss function with respect to current layer bias matrix.\n",
    "        \"\"\"\n",
    "        \n",
    "        if activation_curr=='sigmoid':\n",
    "            dZ=self._sigmoid_backprop(dA_curr, Z_curr)\n",
    "        \n",
    "        elif activation_curr=='relu':\n",
    "            dZ=self._relu_backprop(dA_curr, Z_curr)\n",
    "        else:\n",
    "            raise Exception('no suitable activation function chosen!! check ur spelling')\n",
    "            \n",
    "        #check this   \n",
    "        m=A_prev.shape[1]\n",
    "        dW_curr=np.dot(dZ.T, A_prev) / m \n",
    "        db_curr=np.sum(dZ, axis=0) / m \n",
    "        #dA_prev=np.dot(W_curr.T, dZ.T)\n",
    "        dA_prev=(dZ, W_curr)\n",
    "        \n",
    "        \n",
    "        return(dA_prev, dW_curr, db_curr)\n",
    "        \n",
    "\n",
    "\n",
    "    def backprop(self, y: ArrayLike, y_hat: ArrayLike, cache: Dict[str, ArrayLike]):\n",
    "        \"\"\"\n",
    "        This method is responsible for the backprop of the whole fully connected neural network.\n",
    "\n",
    "        Args:\n",
    "            y (array-like):\n",
    "                Ground truth labels.\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output values.\n",
    "            cache: Dict[str, ArrayLike]\n",
    "                Dictionary containing the information about the\n",
    "                most recent forward pass, specifically A and Z matrices.\n",
    "\n",
    "        Returns:\n",
    "            grad_dict: Dict[str, ArrayLike]\n",
    "                Dictionary containing the gradient information from this pass of backprop.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        grad_dict={}\n",
    "        \n",
    "        #go b a c k w a r d s \n",
    "        for l in range(len(self.arch), 0, -1):\n",
    "           # print('backprop layer: ', l)\n",
    "            \n",
    "            \n",
    "            #need to get all the variables to run backpropasdfkl\n",
    "            #i think they come from cache, which comes from forward prop\n",
    "            W_curr=self._param_dict['W' + str(l)]\n",
    "            b_curr=self._param_dict['b' + str(l)]\n",
    "            Z_curr=cache['Z' + str(l)]\n",
    "            A_prev=cache['A' + str(l-1)]\n",
    "            activation_curr=self.arch[l-1]['activation']\n",
    "            \n",
    "            \n",
    "            #get dA i think\n",
    "            #calculate loss \n",
    "            if self._loss_func =='mse':\n",
    "                dA_curr=self._mean_squared_error_backprop(y=y, y_hat=y_hat)\n",
    "            elif self._loss_func =='bce':\n",
    "                dA_curr=self._binary_cross_entropy_backprop(y=y, y_hat=y_hat)\n",
    "            else:\n",
    "                raise Exception('No availalbe loss function chosen')\n",
    "                 \n",
    "            \n",
    "            \n",
    "            #idk man idk \n",
    "            dA_prev, dW_curr, db_curr=self._single_backprop(W_curr, b_curr, Z_curr,A_prev, dA_curr, activation_curr)\n",
    "            \n",
    "            #print(\"dW_curr shape\", dW_curr.shape)\n",
    "            #print(\"dA_prev shape\", dA_prev.shape)\n",
    "            #print(\"db_curr shape\", db_curr.shape)\n",
    "            \n",
    "            \n",
    "            #print(\"param W shape\", self._param_dict['W'+str(l)].shape)\n",
    "            #print(\"param b shape\", self._param_dict['b'+str(l)].shape)\n",
    "            \n",
    "            \n",
    "            #update grad_dict with gradients for W, b, A \n",
    "            grad_dict['W'+str(l)]=dW_curr\n",
    "            grad_dict['b'+str(l)]=db_curr\n",
    "            grad_dict['A'+str(l-1)]=dA_prev\n",
    "            \n",
    "\n",
    "            \n",
    "        return(grad_dict)\n",
    "            \n",
    "\n",
    "    def _update_params(self, grad_dict: Dict[str, ArrayLike]):\n",
    "        \"\"\"\n",
    "        This function updates the parameters in the neural network after backprop. This function\n",
    "        only modifies internal attributes and does not return anything\n",
    "\n",
    "        Args:\n",
    "            grad_dict: Dict[str, ArrayLike]\n",
    "                Dictionary containing the gradient information from most recent round of backprop.\n",
    "        \"\"\"\n",
    "                    \n",
    "            \n",
    "        #go through each layer and get corresponding gradient for each node\n",
    "        \n",
    "        for l in range(1,len(self.arch)+1):\n",
    "                        \n",
    "            #get relevant B and W grad \n",
    "            #w_grad=grad_dict['W'+str(l)]\n",
    "            #b_grad=grad_dict['b'+str(l)]\n",
    "            \n",
    "            #idk if the indices are right \n",
    "            #update based on learning rate and gradient for that node \n",
    "            self._param_dict['W'+str(l)] -= self._lr * grad_dict['W'+str(l)]\n",
    "            self._param_dict['b'+str(l)] -= self._lr * np.expand_dims(grad_dict['b'+str(l)], 1)\n",
    "            \n",
    "            \n",
    "            \n",
    "            #i think just these two?\n",
    "\n",
    "            \n",
    "       \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X_train: ArrayLike,\n",
    "        y_train: ArrayLike,\n",
    "        X_val: ArrayLike,\n",
    "        y_val: ArrayLike\n",
    "    ) -> Tuple[List[float], List[float]]:\n",
    "        \"\"\"\n",
    "        This function trains the neural network by backpropagation for the number of epochs defined at\n",
    "        the initialization of this class instance.\n",
    "\n",
    "        Args:\n",
    "            X_train: ArrayLike\n",
    "                Input features of training set.\n",
    "            y_train: ArrayLike\n",
    "                Labels for training set.\n",
    "            X_val: ArrayLike\n",
    "                Input features of validation set.\n",
    "            y_val: ArrayLike\n",
    "                Labels for validation set.\n",
    "\n",
    "        Returns:\n",
    "            per_epoch_loss_train: List[float]\n",
    "                List of per epoch loss for training set.\n",
    "            per_epoch_loss_val: List[float]\n",
    "                List of per epoch loss for validation set.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        #init lists to store losses \n",
    "        per_epoch_loss_train=[]\n",
    "        per_epoch_loss_val=[]\n",
    "        \n",
    "            \n",
    "        for epoch in range(self._epochs):\n",
    "           \n",
    "            \n",
    "            if epoch % 50==0:\n",
    "                 print('epoch: ', epoch)\n",
    "            \n",
    "            \n",
    "            #this take nfrom hw7 \n",
    "            # Shuffling the training data for each epoch of training\n",
    "            #only need to expand dims if y_train has one dimension i think \n",
    "            #y_train=np.expand_dims(y_train, 1)\n",
    "            #would also need to reflatten y_train after\n",
    "            #.flatten()\n",
    "            \n",
    "\n",
    "            idx=np.arange(X_train.shape[0])\n",
    "            np.random.shuffle(idx)\n",
    "            X_train = X_train[idx, :]\n",
    "            y_train = y_train[idx, :]\n",
    "            \n",
    "            \n",
    "            #print('X_train shape', X_train.shape)\n",
    "            #print('y_train shape', y_train.shape)\n",
    "                      \n",
    "        \n",
    "            # Create batches\n",
    "            num_batches = int(X_train.shape[0] / self._batch_size) + 1\n",
    "            X_batch = np.array_split(X_train, num_batches)\n",
    "            y_batch = np.array_split(y_train, num_batches)\n",
    "                \n",
    "            \n",
    "            #store losses for each training batch \n",
    "            batch_loss_train=[]\n",
    "            \n",
    "            # Iterate through batches (one of these loops is one epoch of training)\n",
    "            for X_train, y_train in zip(X_batch, y_batch):\n",
    "                \n",
    "                #print('X_train shape', X_train.shape)\n",
    "                #print('y_train shape', y_train.shape)\n",
    "                \n",
    "                \n",
    "                #forward pass\n",
    "                y_pred, cache = self.forward(X_train)\n",
    "                #print('y_pred shape', y_pred.shape)\n",
    "\n",
    "                #calculate loss \n",
    "                if self._loss_func =='mse':\n",
    "                    train_loss=self._mean_squared_error(y=y_train, y_hat=y_pred)\n",
    "                elif self._loss_func =='bce':\n",
    "                    train_loss=self._binary_cross_entropy(y=y_train, y_hat=y_pred)\n",
    "                else:\n",
    "                    raise Exception('No availalbe loss function chosen')\n",
    "                  \n",
    "                #add to batch loss list \n",
    "                batch_loss_train.append(train_loss)\n",
    "                \n",
    "                \n",
    "                #then, backpropagate \n",
    "                grad_dict=self.backprop(y=y_train, y_hat=y_pred, cache=cache)\n",
    "                \n",
    "\n",
    "                    \n",
    "                    \n",
    "                #update parameter weights \n",
    "                self._update_params(grad_dict)\n",
    "                \n",
    "                #is that it?????????/?sdflkajdslkfjalskdflaksdjf\n",
    "                \n",
    "          \n",
    "\n",
    "    \n",
    "            #after running all train batches\n",
    "        \n",
    "            #get mean of batch losses and add to epoch loss\n",
    "            per_epoch_loss_train.append(np.mean(batch_loss_train))\n",
    "            \n",
    "            \n",
    "            #Then, validation\n",
    "            #Make one prediction on val for each epoch \n",
    "            #idk if this is how it should be \n",
    "            \n",
    "            val_pred = self.predict(X_val)\n",
    "            \n",
    "            \n",
    "            if self._loss_func =='mse':\n",
    "                val_loss=self._mean_squared_error(y=y_val, y_hat=val_pred)          \n",
    "            elif self._loss_func =='bce':\n",
    "                val_loss=self._binary_cross_entropy(y=y_val, y_hat=val_pred)\n",
    "            else:\n",
    "                raise Exception('No availalbe loss function chosen')\n",
    "                \n",
    "                \n",
    "            #add loss to per epoch loss val\n",
    "            per_epoch_loss_val.append(val_loss)\n",
    "\n",
    "            \n",
    "        #return training and validation losses\n",
    "        return(per_epoch_loss_train, per_epoch_loss_val)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, X: ArrayLike) -> ArrayLike: #DONE\n",
    "        \"\"\"\n",
    "        This function returns the prediction of the neural network.\n",
    "\n",
    "        Args:\n",
    "            X: ArrayLike\n",
    "                Input data for prediction.\n",
    "\n",
    "        Returns:\n",
    "            y_hat: ArrayLike\n",
    "                Prediction from the model.\n",
    "        \"\"\"\n",
    "        \n",
    "        y_hat,_ =self.forward(X)\n",
    "        return y_hat\n",
    "\n",
    "    def _sigmoid(self, Z: ArrayLike) -> ArrayLike: #DONE\n",
    "        \"\"\"\n",
    "        Sigmoid activation function.\n",
    "\n",
    "        Args:\n",
    "            Z: ArrayLike\n",
    "                Output of layer linear transform.\n",
    "\n",
    "        Returns:\n",
    "            nl_transform: ArrayLike\n",
    "                Activation function output.\n",
    "        \"\"\"\n",
    "        return((1/(1+np.exp(-Z))))\n",
    "\n",
    "    def _sigmoid_backprop(self, dA: ArrayLike, Z: ArrayLike): #DONE\n",
    "        \"\"\"\n",
    "        Sigmoid derivative for backprop.\n",
    "\n",
    "        Args:\n",
    "            dA: ArrayLike\n",
    "                Partial derivative of previous layer activation matrix.\n",
    "            Z: ArrayLike\n",
    "                Output of layer linear transform.\n",
    "\n",
    "        Returns:\n",
    "            dZ: ArrayLike\n",
    "                Partial derivative of current layer Z matrix.\n",
    "        \"\"\"\n",
    "        \n",
    "        sig_Z=self._sigmoid(Z)\n",
    "        dZ=dA*sig_Z*(1-sig_Z)\n",
    "        \n",
    "        return(dZ)\n",
    "    \n",
    "\n",
    "    def _relu(self, Z: ArrayLike) -> ArrayLike: #DONE\n",
    "        \"\"\"\n",
    "        ReLU activation function.\n",
    "\n",
    "        Args:\n",
    "            Z: ArrayLike\n",
    "                Output of layer linear transform.\n",
    "\n",
    "        Returns:\n",
    "            nl_transform: ArrayLike\n",
    "                Activation function output.\n",
    "        \"\"\"\n",
    "        return(np.maximum(0, Z))\n",
    "    \n",
    "\n",
    "    def _relu_backprop(self, dA: ArrayLike, Z: ArrayLike) -> ArrayLike: #DONE I THINK\n",
    "        \"\"\"\n",
    "        ReLU derivative for backprop.\n",
    "\n",
    "        Args:\n",
    "            dA: ArrayLike\n",
    "                Partial derivative of previous layer activation matrix.\n",
    "            Z: ArrayLike\n",
    "                Output of layer linear transform.\n",
    "\n",
    "        Returns:\n",
    "            dZ: ArrayLike\n",
    "                Partial derivative of current layer Z matrix.\n",
    "        \"\"\"\n",
    "        \n",
    "        relu_Z=self._relu(Z)\n",
    "        \n",
    "        dZ=np.multiply(dA, relu_Z)\n",
    "        \n",
    "        return(dZ)\n",
    "        \n",
    "        \n",
    "\n",
    "    def _binary_cross_entropy(self, y: ArrayLike, y_hat: ArrayLike, error=1e-5) -> float: #DONE\n",
    "        \"\"\"\n",
    "        Binary cross entropy loss function.\n",
    "\n",
    "        Args:\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output.\n",
    "            y: ArrayLike\n",
    "                Ground truth output.\n",
    "\n",
    "        Returns:\n",
    "            loss: float\n",
    "                Average loss over mini-batch.\n",
    "        \"\"\"\n",
    "        #add error so no divide by zero warning\n",
    "        bce_loss= -np.mean(y*(np.log(y_hat + error)) +  (1-y)*np.log(1-y_hat + error)) \n",
    "        return(bce_loss)\n",
    "    \n",
    "\n",
    "    def _binary_cross_entropy_backprop(self, y: ArrayLike, y_hat: ArrayLike, error=-1e5) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        Binary cross entropy loss function derivative for backprop.\n",
    "\n",
    "        Args:\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output.\n",
    "            y: ArrayLike\n",
    "                Ground truth output.\n",
    "\n",
    "        Returns:\n",
    "            dA: ArrayLike\n",
    "                partial derivative of loss with respect to A matrix.\n",
    "        \"\"\"\n",
    "        bce=self._binary_cross_entropy(y, y_hat)\n",
    "        \n",
    "        #i had to google this i'm sorry\n",
    "        #add error for divide by zero \n",
    "        #need to double check this i think it's wrong \n",
    "        dA=np.mean(-(y/y_hat + error) + (1-y)/(1-y_hat+error))\n",
    "        #dA=np.mean((y/y_hat + error) + (1-y)/(1-y_hat+error))\n",
    "\n",
    "        \n",
    "        return(dA)\n",
    "     \n",
    "        \n",
    "    \n",
    "\n",
    "    def _mean_squared_error(self, y: ArrayLike, y_hat: ArrayLike) -> float:\n",
    "        \"\"\"\n",
    "        Mean squared error loss.\n",
    "\n",
    "        Args:\n",
    "            y: ArrayLike\n",
    "                Ground truth output.\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output.\n",
    "\n",
    "        Returns:\n",
    "            loss: float\n",
    "                Average loss of mini-batch.\n",
    "        \"\"\"\n",
    "\n",
    "        mse = np.mean((y - y_hat) ** 2)\n",
    "        return(mse)\n",
    "\n",
    "    def _mean_squared_error_backprop(self, y: ArrayLike, y_hat: ArrayLike) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        Mean square error loss derivative for backprop.\n",
    "\n",
    "        Args:\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output.\n",
    "            y: ArrayLike\n",
    "                Ground truth output.\n",
    "\n",
    "        Returns:\n",
    "            dA: ArrayLike\n",
    "                partial derivative of loss with respect to A matrix.\n",
    "        \"\"\"\n",
    "        \n",
    "        dA=np.mean(2*(y_hat-y))\n",
    "        return(dA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_NN = NeuralNetwork(nn_arch = [{'input_dim': 64, 'output_dim': 16, 'activation': 'relu'}, \n",
    "                                      {'input_dim': 16, 'output_dim': 64, 'activation': 'relu'}],\n",
    "                            lr = 0.01, seed = 3, batch_size = 50, epochs = 200, loss_function='mse')\n",
    "\n",
    "\n",
    "#params are arrays of W1-Wx where x is feature number\n",
    "#and also an array for b1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "epoch:  50\n",
      "epoch:  100\n",
      "epoch:  150\n"
     ]
    }
   ],
   "source": [
    "#YOU FOOL\n",
    "#BC IT'S AN AUTOENCODER INPUT AND OUTPUT SHOULD BE THE SAME GLAKSDJF'LAKSDFJ'\n",
    "per_epoch_loss_train, per_epoch_loss_val=example_NN.fit(X_train, X_train, X_test, X_test)\n",
    "#ah yes nothign works "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(359, 64)"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#digits.data is shape (1794, 64) --> 64 is flattened image pixels \n",
    "digits = load_digits()\n",
    "  \n",
    "#split into train and test sets \n",
    "X_train, X_test, y_train, y_test=train_test_split(digits.data, digits.target, train_size=0.2, random_state=3)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#taken from hw7\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "fig, axs = plt.subplots(2, figsize=(8, 8))\n",
    "axs[0].plot(np.arange(len(per_epoch_loss_train)), per_epoch_loss_train)\n",
    "axs[0].set_title('Training Loss History')\n",
    "axs[1].plot(np.arange(len(per_epoch_loss_val)), per_epoch_loss_val)\n",
    "axs[1].set_title('Validation Loss History')\n",
    "plt.xlabel('Steps')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "3\n",
      "2\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
