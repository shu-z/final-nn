{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib.plt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-294-38a868d748a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_digits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib.plt'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Union\n",
    "from numpy.typing import ArrayLike\n",
    "import sklearn \n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.plt as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    This is a class that generates a fully-connected neural network.\n",
    "\n",
    "    Parameters:\n",
    "        nn_arch: List[Dict[str, float]]\n",
    "            A list of dictionaries describing the layers of the neural network.\n",
    "            e.g. [{'input_dim': 64, 'output_dim': 32, 'activation': 'relu'}, {'input_dim': 32, 'output_dim': 8, 'activation:': 'sigmoid'}]\n",
    "            will generate a two-layer deep network with an input dimension of 64, a 32 dimension hidden layer, and an 8 dimensional output.\n",
    "        lr: float\n",
    "            Learning rate (alpha).\n",
    "        seed: int\n",
    "            Random seed to ensure reproducibility.\n",
    "        batch_size: int\n",
    "            Size of mini-batches used for training.\n",
    "        epochs: int\n",
    "            Max number of epochs for training.\n",
    "        loss_function: str\n",
    "            Name of loss function.\n",
    "\n",
    "    Attributes:\n",
    "        arch: list of dicts\n",
    "            (see nn_arch above)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        #this throwing error fix later\n",
    "        #nn_arch: List[Dict[str, Union(int, str)]],\n",
    "        nn_arch: List, \n",
    "        lr: float,\n",
    "        seed: int,\n",
    "        batch_size: int,\n",
    "        epochs: int,\n",
    "        loss_function: str\n",
    "    ):\n",
    "\n",
    "        # Save architecture\n",
    "        self.arch = nn_arch\n",
    "\n",
    "        # Save hyperparameters\n",
    "        self._lr = lr\n",
    "        self._seed = seed\n",
    "        self._epochs = epochs\n",
    "        self._loss_func = loss_function\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "        # Initialize the parameter dictionary for use in training\n",
    "        self._param_dict = self._init_params()\n",
    "        #print(self._param_dict)\n",
    "\n",
    "    def _init_params(self) -> Dict[str, ArrayLike]:\n",
    "        \"\"\"\n",
    "        DO NOT MODIFY THIS METHOD! IT IS ALREADY COMPLETE!\n",
    "\n",
    "        This method generates the parameter matrices for all layers of\n",
    "        the neural network. This function returns the param_dict after\n",
    "        initialization.\n",
    "\n",
    "        Returns:\n",
    "            param_dict: Dict[str, ArrayLike]\n",
    "                Dictionary of parameters in neural network.\n",
    "        \"\"\"\n",
    "\n",
    "        # Seed NumPy\n",
    "        np.random.seed(self._seed)\n",
    "\n",
    "        # Define parameter dictionary\n",
    "        param_dict = {}\n",
    "\n",
    "        # Initialize each layer's weight matrices (W) and bias matrices (b)\n",
    "        for idx, layer in enumerate(self.arch):\n",
    "            layer_idx = idx + 1\n",
    "            input_dim = layer['input_dim']\n",
    "            output_dim = layer['output_dim']\n",
    "            param_dict['W' + str(layer_idx)] = np.random.randn(output_dim, input_dim) * 0.1\n",
    "            param_dict['b' + str(layer_idx)] = np.random.randn(output_dim, 1) * 0.1\n",
    "\n",
    "        return param_dict\n",
    "\n",
    "    def _single_forward(\n",
    "        self,\n",
    "        W_curr: ArrayLike,\n",
    "        b_curr: ArrayLike,\n",
    "        A_prev: ArrayLike,\n",
    "        activation: str\n",
    "    ) -> Tuple[ArrayLike, ArrayLike]:\n",
    "        \"\"\"\n",
    "        This method is used for a single forward pass on a single layer.\n",
    "\n",
    "        Args:\n",
    "            W_curr: ArrayLike\n",
    "                Current layer weight matrix.\n",
    "            b_curr: ArrayLike\n",
    "                Current layer bias matrix.\n",
    "            A_prev: ArrayLike\n",
    "                Previous layer activation matrix.\n",
    "            activation: str\n",
    "                Name of activation function for current layer.\n",
    "\n",
    "        Returns:\n",
    "            A_curr: ArrayLike\n",
    "                Current layer activation matrix.\n",
    "            Z_curr: ArrayLike\n",
    "                Current layer linear transformed matrix.\n",
    "        \"\"\"\n",
    "        \n",
    "        #need to multiply every node in last layer with weights \n",
    "        #then apply activation function \n",
    "        \n",
    "        \n",
    "        #Z is before A\n",
    "        Z_curr=np.dot(A_prev, W_curr.T) + b_curr.T\n",
    "        \n",
    "        #apply activation function \n",
    "        \n",
    "        if activation =='sigmoid':\n",
    "            A_curr=self._sigmoid(Z_curr)\n",
    "        \n",
    "        \n",
    "        elif activation =='relu':\n",
    "            A_curr=self._relu(Z_curr)\n",
    "                    \n",
    "        else:\n",
    "            raise Exception('no suitable activation function chosen!!!!!!!1!!!!')\n",
    "        \n",
    "        \n",
    "        \n",
    "        return(A_curr, Z_curr)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    def forward(self, X: ArrayLike) -> Tuple[ArrayLike, Dict[str, ArrayLike]]:\n",
    "        \"\"\"\n",
    "        This method is responsible for one forward pass of the entire neural network.\n",
    "\n",
    "        Args:\n",
    "            X: ArrayLike\n",
    "                Input matrix with shape [batch_size, features].\n",
    "\n",
    "        Returns:\n",
    "            output: ArrayLike\n",
    "                Output of forward pass.\n",
    "            cache: Dict[str, ArrayLike]:\n",
    "                Dictionary storing Z and A matrices from `_single_forward` for use in backprop.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        cache={}\n",
    "                \n",
    "        #for the first layer \n",
    "        A_prev=X\n",
    "        #add to cache\n",
    "        cache['A0']=X\n",
    "\n",
    "        \n",
    "        #loop through remaining number of layers in nn\n",
    "        for l in range(1,len(self.arch)+1):\n",
    "            #print('layer',  l)\n",
    "            \n",
    "\n",
    "            #weights \n",
    "            W_curr=self._param_dict['W'+str(l)]\n",
    "            b_curr=self._param_dict['b'+str(l)]\n",
    "            \n",
    "            activation=self.arch[l-1]['activation']\n",
    "            \n",
    "            #go through one step of feed forward \n",
    "            #remember that A is activation matrix, Z is linearly transformed matrix\n",
    "            A_curr, Z_curr=self._single_forward(W_curr, b_curr, A_prev, activation)\n",
    "\n",
    "            \n",
    "            cache['A'+str(l)]=A_curr\n",
    "            cache['Z'+str(l)]=Z_curr\n",
    "            \n",
    "            \n",
    "            A_prev=A_curr\n",
    "            \n",
    "        output=A_curr\n",
    "            \n",
    "        return(output, cache)\n",
    "            \n",
    "\n",
    "    def _single_backprop(\n",
    "        self,\n",
    "        W_curr: ArrayLike,\n",
    "        b_curr: ArrayLike,\n",
    "        Z_curr: ArrayLike,\n",
    "        A_prev: ArrayLike,\n",
    "        dA_curr: ArrayLike,\n",
    "        activation_curr: str\n",
    "    ) -> Tuple[ArrayLike, ArrayLike, ArrayLike]:\n",
    "        \"\"\"\n",
    "        This method is used for a single backprop pass on a single layer.\n",
    "\n",
    "        Args:\n",
    "            W_curr: ArrayLike\n",
    "                Current layer weight matrix.\n",
    "            b_curr: ArrayLike\n",
    "                Current layer bias matrix.\n",
    "            Z_curr: ArrayLike\n",
    "                Current layer linear transform matrix.\n",
    "            A_prev: ArrayLike\n",
    "                Previous layer activation matrix.\n",
    "            dA_curr: ArrayLike\n",
    "                Partial derivative of loss function with respect to current layer activation matrix.\n",
    "            activation_curr: str\n",
    "                Name of activation function of layer.\n",
    "\n",
    "        Returns:\n",
    "            dA_prev: ArrayLike\n",
    "                Partial derivative of loss function with respect to previous layer activation matrix.\n",
    "            dW_curr: ArrayLike\n",
    "                Partial derivative of loss function with respect to current layer weight matrix.\n",
    "            db_curr: ArrayLike\n",
    "                Partial derivative of loss function with respect to current layer bias matrix.\n",
    "        \"\"\"\n",
    "        \n",
    "        if activation_curr=='sigmoid':\n",
    "            dZ=self._sigmoid_backprop(dA_curr, Z_curr)\n",
    "        \n",
    "        elif activation_curr=='relu':\n",
    "            dZ=self._relu_backprop(dA_curr, Z_curr)\n",
    "        else:\n",
    "            raise Exception('no suitable activation function chosen!! check ur spelling')\n",
    "            \n",
    "        #check this   \n",
    "        m=A_prev.shape[1]\n",
    "        dW_curr=np.dot(dZ.T, A_prev) / m \n",
    "        db_curr=np.sum(dZ, axis=0) / m \n",
    "        dA_prev=np.dot(W_curr.T, dZ.T)\n",
    "        \n",
    "        \n",
    "        return(dA_prev, dW_curr, db_curr)\n",
    "        \n",
    "\n",
    "\n",
    "    def backprop(self, y: ArrayLike, y_hat: ArrayLike, cache: Dict[str, ArrayLike]):\n",
    "        \"\"\"\n",
    "        This method is responsible for the backprop of the whole fully connected neural network.\n",
    "\n",
    "        Args:\n",
    "            y (array-like):\n",
    "                Ground truth labels.\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output values.\n",
    "            cache: Dict[str, ArrayLike]\n",
    "                Dictionary containing the information about the\n",
    "                most recent forward pass, specifically A and Z matrices.\n",
    "\n",
    "        Returns:\n",
    "            grad_dict: Dict[str, ArrayLike]\n",
    "                Dictionary containing the gradient information from this pass of backprop.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        grad_dict={}\n",
    "        \n",
    "        #I THINK THIS NEEDS TO BE DIFFERENT FOR HTE FIRST NODE OH MY GOD \n",
    "        \n",
    "        #go b a c k w a r d s \n",
    "        for l in range(len(self.arch), 0, -1):\n",
    "           # print('backprop layer: ', l)\n",
    "            \n",
    "            \n",
    "            #need to get all the variables to run backpropasdfkl\n",
    "            #i think they come from cache, which comes from forward prop\n",
    "            W_curr=self._param_dict['W' + str(l)]\n",
    "            b_curr=self._param_dict['b' + str(l)]\n",
    "            Z_curr=cache['Z' + str(l)]\n",
    "            A_prev=cache['A' + str(l-1)]\n",
    "            activation_curr=self.arch[l-1]['activation']\n",
    "            \n",
    "            \n",
    "            #get dA i think\n",
    "            #calculate loss \n",
    "            if self._loss_func =='mse':\n",
    "                dA_curr=self._mean_squared_error(y=y, y_hat=y_hat)\n",
    "            elif self._loss_func =='bce':\n",
    "                dA_curr=self._binary_cross_entropy(y=y, y_hat=y_hat)\n",
    "            else:\n",
    "                raise Exception('No availalbe loss function chosen')\n",
    "                 \n",
    "            \n",
    "            \n",
    "            #idk man idk \n",
    "            dA_prev, dW_curr, db_curr=self._single_backprop(W_curr, b_curr, Z_curr,A_prev, dA_curr, activation_curr)\n",
    "            \n",
    "            #print(\"dW_curr shape\", dW_curr.shape)\n",
    "            #print(\"dA_prev shape\", dA_prev.shape)\n",
    "            #print(\"db_curr shape\", db_curr.shape)\n",
    "            \n",
    "            \n",
    "            #print(\"param W shape\", self._param_dict['W'+str(l)].shape)\n",
    "            #print(\"param b shape\", self._param_dict['b'+str(l)].shape)\n",
    "            \n",
    "            \n",
    "            #update grad_dict with gradients for W, b, A \n",
    "            grad_dict['W'+str(l)]=dW_curr\n",
    "            grad_dict['b'+str(l)]=db_curr\n",
    "            grad_dict['A'+str(l-1)]=dA_prev\n",
    "            \n",
    "\n",
    "            \n",
    "        return(grad_dict)\n",
    "            \n",
    "\n",
    "    def _update_params(self, grad_dict: Dict[str, ArrayLike]):\n",
    "        \"\"\"\n",
    "        This function updates the parameters in the neural network after backprop. This function\n",
    "        only modifies internal attributes and does not return anything\n",
    "\n",
    "        Args:\n",
    "            grad_dict: Dict[str, ArrayLike]\n",
    "                Dictionary containing the gradient information from most recent round of backprop.\n",
    "        \"\"\"\n",
    "                    \n",
    "            \n",
    "        #go through each layer and get corresponding gradient for each node\n",
    "        \n",
    "        for l in range(1,len(self.arch)+1):\n",
    "                        \n",
    "            #get relevant B and W grad \n",
    "            #w_grad=grad_dict['W'+str(l)]\n",
    "            #b_grad=grad_dict['b'+str(l)]\n",
    "            \n",
    "            #idk if the indices are right \n",
    "            #update based on learning rate and gradient for that node \n",
    "            self._param_dict['W'+str(l)] -= self._lr * grad_dict['W'+str(l)]\n",
    "            self._param_dict['b'+str(l)] -= self._lr * np.expand_dims(grad_dict['b'+str(l)], 1)\n",
    "            \n",
    "            \n",
    "            \n",
    "            #i think just these two?\n",
    "\n",
    "            \n",
    "       \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X_train: ArrayLike,\n",
    "        y_train: ArrayLike,\n",
    "        X_val: ArrayLike,\n",
    "        y_val: ArrayLike\n",
    "    ) -> Tuple[List[float], List[float]]:\n",
    "        \"\"\"\n",
    "        This function trains the neural network by backpropagation for the number of epochs defined at\n",
    "        the initialization of this class instance.\n",
    "\n",
    "        Args:\n",
    "            X_train: ArrayLike\n",
    "                Input features of training set.\n",
    "            y_train: ArrayLike\n",
    "                Labels for training set.\n",
    "            X_val: ArrayLike\n",
    "                Input features of validation set.\n",
    "            y_val: ArrayLike\n",
    "                Labels for validation set.\n",
    "\n",
    "        Returns:\n",
    "            per_epoch_loss_train: List[float]\n",
    "                List of per epoch loss for training set.\n",
    "            per_epoch_loss_val: List[float]\n",
    "                List of per epoch loss for validation set.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        #init lists to store losses \n",
    "        per_epoch_loss_train=[]\n",
    "        per_epoch_loss_val=[]\n",
    "        \n",
    "            \n",
    "        for epoch in range(self._epochs):\n",
    "            print('epoch: ', epoch)\n",
    "            \n",
    "            \n",
    "            #this take nfrom hw7 \n",
    "            # Shuffling the training data for each epoch of training\n",
    "            #only need to expand dims if y_train has one dimension i think \n",
    "            #y_train=np.expand_dims(y_train, 1)\n",
    "            #would also need to reflatten y_train after\n",
    "            #.flatten()\n",
    "            \n",
    "\n",
    "            idx=np.arange(X_train.shape[0])\n",
    "            np.random.shuffle(idx)\n",
    "            X_train = X_train[idx, :]\n",
    "            y_train = y_train[idx, :]\n",
    "            \n",
    "            \n",
    "            #print('X_train shape', X_train.shape)\n",
    "            #print('y_train shape', y_train.shape)\n",
    "                      \n",
    "        \n",
    "            # Create batches\n",
    "            num_batches = int(X_train.shape[0] / self._batch_size) + 1\n",
    "            X_batch = np.array_split(X_train, num_batches)\n",
    "            y_batch = np.array_split(y_train, num_batches)\n",
    "                \n",
    "            \n",
    "            #store losses for each training batch \n",
    "            batch_loss_train=[]\n",
    "            \n",
    "            # Iterate through batches (one of these loops is one epoch of training)\n",
    "            for X_train, y_train in zip(X_batch, y_batch):\n",
    "                \n",
    "                #print('X_train shape', X_train.shape)\n",
    "                #print('y_train shape', y_train.shape)\n",
    "                \n",
    "                \n",
    "                #forward pass\n",
    "                y_pred, cache = self.forward(X_train)\n",
    "                #print('y_pred shape', y_pred.shape)\n",
    "\n",
    "                #calculate loss \n",
    "                if self._loss_func =='mse':\n",
    "                    train_loss=self._mean_squared_error(y=y_train, y_hat=y_pred)\n",
    "                elif self._loss_func =='bce':\n",
    "                    train_loss=self._binary_cross_entropy(y=y_train, y_hat=y_pred)\n",
    "                else:\n",
    "                    raise Exception('No availalbe loss function chosen')\n",
    "                  \n",
    "                #add to batch loss list \n",
    "                batch_loss_train.append(train_loss)\n",
    "                \n",
    "                \n",
    "                #then, backpropagate \n",
    "                grad_dict=self.backprop(y=y_train, y_hat=y_pred, cache=cache)\n",
    "                \n",
    "\n",
    "                    \n",
    "                    \n",
    "                #update parameter weights \n",
    "                self._update_params(grad_dict)\n",
    "                \n",
    "                #is that it?????????/?sdflkajdslkfjalskdflaksdjf\n",
    "                \n",
    "          \n",
    "\n",
    "    \n",
    "            #after running all train batches\n",
    "        \n",
    "            #get mean of batch losses and add to epoch loss\n",
    "            per_epoch_loss_train.append(np.mean(batch_loss_train))\n",
    "            \n",
    "            \n",
    "            #Then, validation\n",
    "            #Make one prediction on val for each epoch \n",
    "            #idk if this is how it should be \n",
    "            \n",
    "            val_pred = self.predict(X_val)\n",
    "            \n",
    "            \n",
    "            if self._loss_func =='mse':\n",
    "                val_loss=self._mean_squared_error(y=y_val, y_hat=val_pred)          \n",
    "            elif self._loss_func =='bce':\n",
    "                val_loss=self._binary_cross_entropy(y=y_val, y_hat=val_pred)\n",
    "            else:\n",
    "                raise Exception('No availalbe loss function chosen')\n",
    "                \n",
    "                \n",
    "            #add loss to per epoch loss val\n",
    "            per_epoch_loss_val.append(val_loss)\n",
    "\n",
    "            \n",
    "        #return training and validation losses\n",
    "        return(per_epoch_loss_train, per_epoch_loss_val)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, X: ArrayLike) -> ArrayLike: #DONE\n",
    "        \"\"\"\n",
    "        This function returns the prediction of the neural network.\n",
    "\n",
    "        Args:\n",
    "            X: ArrayLike\n",
    "                Input data for prediction.\n",
    "\n",
    "        Returns:\n",
    "            y_hat: ArrayLike\n",
    "                Prediction from the model.\n",
    "        \"\"\"\n",
    "        \n",
    "        y_hat,_ =self.forward(X)\n",
    "        return y_hat\n",
    "\n",
    "    def _sigmoid(self, Z: ArrayLike) -> ArrayLike: #DONE\n",
    "        \"\"\"\n",
    "        Sigmoid activation function.\n",
    "\n",
    "        Args:\n",
    "            Z: ArrayLike\n",
    "                Output of layer linear transform.\n",
    "\n",
    "        Returns:\n",
    "            nl_transform: ArrayLike\n",
    "                Activation function output.\n",
    "        \"\"\"\n",
    "        return((1/(1+np.exp(-Z))))\n",
    "\n",
    "    def _sigmoid_backprop(self, dA: ArrayLike, Z: ArrayLike): #DONE\n",
    "        \"\"\"\n",
    "        Sigmoid derivative for backprop.\n",
    "\n",
    "        Args:\n",
    "            dA: ArrayLike\n",
    "                Partial derivative of previous layer activation matrix.\n",
    "            Z: ArrayLike\n",
    "                Output of layer linear transform.\n",
    "\n",
    "        Returns:\n",
    "            dZ: ArrayLike\n",
    "                Partial derivative of current layer Z matrix.\n",
    "        \"\"\"\n",
    "        \n",
    "        sig_Z=self._sigmoid(Z)\n",
    "        dZ=dA*sig_Z*(1-sig_Z)\n",
    "        \n",
    "        return(dZ)\n",
    "    \n",
    "\n",
    "    def _relu(self, Z: ArrayLike) -> ArrayLike: #DONE\n",
    "        \"\"\"\n",
    "        ReLU activation function.\n",
    "\n",
    "        Args:\n",
    "            Z: ArrayLike\n",
    "                Output of layer linear transform.\n",
    "\n",
    "        Returns:\n",
    "            nl_transform: ArrayLike\n",
    "                Activation function output.\n",
    "        \"\"\"\n",
    "        return(np.maximum(0, Z))\n",
    "    \n",
    "\n",
    "    def _relu_backprop(self, dA: ArrayLike, Z: ArrayLike) -> ArrayLike: #DONE I THINK\n",
    "        \"\"\"\n",
    "        ReLU derivative for backprop.\n",
    "\n",
    "        Args:\n",
    "            dA: ArrayLike\n",
    "                Partial derivative of previous layer activation matrix.\n",
    "            Z: ArrayLike\n",
    "                Output of layer linear transform.\n",
    "\n",
    "        Returns:\n",
    "            dZ: ArrayLike\n",
    "                Partial derivative of current layer Z matrix.\n",
    "        \"\"\"\n",
    "        \n",
    "        relu_Z=self._relu(Z)\n",
    "        \n",
    "        dZ=np.multiply(dA, relu_Z)\n",
    "        \n",
    "        return(dZ)\n",
    "        \n",
    "        \n",
    "\n",
    "    def _binary_cross_entropy(self, y: ArrayLike, y_hat: ArrayLike, error=1e-5) -> float: #DONE\n",
    "        \"\"\"\n",
    "        Binary cross entropy loss function.\n",
    "\n",
    "        Args:\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output.\n",
    "            y: ArrayLike\n",
    "                Ground truth output.\n",
    "\n",
    "        Returns:\n",
    "            loss: float\n",
    "                Average loss over mini-batch.\n",
    "        \"\"\"\n",
    "        #add error so no divide by zero warning\n",
    "        bce_loss= -np.mean(y*(np.log(y_hat + error)) +  (1-y)*np.log(1-y_hat + error)) \n",
    "        return(bce_loss)\n",
    "    \n",
    "\n",
    "    def _binary_cross_entropy_backprop(self, y: ArrayLike, y_hat: ArrayLike, error=-1e5) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        Binary cross entropy loss function derivative for backprop.\n",
    "\n",
    "        Args:\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output.\n",
    "            y: ArrayLike\n",
    "                Ground truth output.\n",
    "\n",
    "        Returns:\n",
    "            dA: ArrayLike\n",
    "                partial derivative of loss with respect to A matrix.\n",
    "        \"\"\"\n",
    "        bce=_binary_cross_entropy(y, y_hat)\n",
    "        \n",
    "        #i had to google this i'm sorry\n",
    "        #add error for divide by zero \n",
    "        #need to double check this i think it's wrong \n",
    "        dA=np.mean(-(y/y_hat + error) + (1-y)/(1-y_hat+error))\n",
    "        \n",
    "        return(dA)\n",
    "     \n",
    "        \n",
    "    \n",
    "\n",
    "    def _mean_squared_error(self, y: ArrayLike, y_hat: ArrayLike) -> float:\n",
    "        \"\"\"\n",
    "        Mean squared error loss.\n",
    "\n",
    "        Args:\n",
    "            y: ArrayLike\n",
    "                Ground truth output.\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output.\n",
    "\n",
    "        Returns:\n",
    "            loss: float\n",
    "                Average loss of mini-batch.\n",
    "        \"\"\"\n",
    "\n",
    "        mse = np.mean((y - y_hat) ** 2)\n",
    "        return(mse)\n",
    "\n",
    "    def _mean_squared_error_backprop(self, y: ArrayLike, y_hat: ArrayLike) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        Mean square error loss derivative for backprop.\n",
    "\n",
    "        Args:\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output.\n",
    "            y: ArrayLike\n",
    "                Ground truth output.\n",
    "\n",
    "        Returns:\n",
    "            dA: ArrayLike\n",
    "                partial derivative of loss with respect to A matrix.\n",
    "        \"\"\"\n",
    "        \n",
    "        dA=np.mean(2*(y_hat-y))\n",
    "        return(dA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_NN = NeuralNetwork(nn_arch = [{'input_dim': 64, 'output_dim': 16, 'activation': 'relu'}, \n",
    "                                      {'input_dim': 16, 'output_dim': 64, 'activation': 'sigmoid'}],\n",
    "                            lr = 0.01, seed = 3, batch_size = 50, epochs = 100, loss_function='mse')\n",
    "\n",
    "\n",
    "#params are arrays of W1-Wx where x is feature number\n",
    "#and also an array for b1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "epoch:  1\n",
      "epoch:  2\n",
      "epoch:  3\n",
      "epoch:  4\n",
      "epoch:  5\n",
      "epoch:  6\n",
      "epoch:  7\n",
      "epoch:  8\n",
      "epoch:  9\n",
      "epoch:  10\n",
      "epoch:  11\n",
      "epoch:  12\n",
      "epoch:  13\n",
      "epoch:  14\n",
      "epoch:  15\n",
      "epoch:  16\n",
      "epoch:  17\n",
      "epoch:  18\n",
      "epoch:  19\n",
      "epoch:  20\n",
      "epoch:  21\n",
      "epoch:  22\n",
      "epoch:  23\n",
      "epoch:  24\n",
      "epoch:  25\n",
      "epoch:  26\n",
      "epoch:  27\n",
      "epoch:  28\n",
      "epoch:  29\n",
      "epoch:  30\n",
      "epoch:  31\n",
      "epoch:  32\n",
      "epoch:  33\n",
      "epoch:  34\n",
      "epoch:  35\n",
      "epoch:  36\n",
      "epoch:  37\n",
      "epoch:  38\n",
      "epoch:  39\n",
      "epoch:  40\n",
      "epoch:  41\n",
      "epoch:  42\n",
      "epoch:  43\n",
      "epoch:  44\n",
      "epoch:  45\n",
      "epoch:  46\n",
      "epoch:  47\n",
      "epoch:  48\n",
      "epoch:  49\n",
      "epoch:  50\n",
      "epoch:  51\n",
      "epoch:  52\n",
      "epoch:  53\n",
      "epoch:  54\n",
      "epoch:  55\n",
      "epoch:  56\n",
      "epoch:  57\n",
      "epoch:  58\n",
      "epoch:  59\n",
      "epoch:  60\n",
      "epoch:  61\n",
      "epoch:  62\n",
      "epoch:  63\n",
      "epoch:  64\n",
      "epoch:  65\n",
      "epoch:  66\n",
      "epoch:  67\n",
      "epoch:  68\n",
      "epoch:  69\n",
      "epoch:  70\n",
      "epoch:  71\n",
      "epoch:  72\n",
      "epoch:  73\n",
      "epoch:  74\n",
      "epoch:  75\n",
      "epoch:  76\n",
      "epoch:  77\n",
      "epoch:  78\n",
      "epoch:  79\n",
      "epoch:  80\n",
      "epoch:  81\n",
      "epoch:  82\n",
      "epoch:  83\n",
      "epoch:  84\n",
      "epoch:  85\n",
      "epoch:  86\n",
      "epoch:  87\n",
      "epoch:  88\n",
      "epoch:  89\n",
      "epoch:  90\n",
      "epoch:  91\n",
      "epoch:  92\n",
      "epoch:  93\n",
      "epoch:  94\n",
      "epoch:  95\n",
      "epoch:  96\n",
      "epoch:  97\n",
      "epoch:  98\n",
      "epoch:  99\n"
     ]
    }
   ],
   "source": [
    "#YOU FOOL\n",
    "#BC IT'S AN AUTOENCODER INPUT AND OUTPUT SHOULD BE THE SAME GLAKSDJF'LAKSDFJ'\n",
    "per_epoch_loss_train, per_epoch_loss_val=example_NN.fit(X_train, X_train, X_test, X_test)\n",
    "#ah yes nothign works "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(359, 64)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#digits.data is shape (1794, 64) --> 64 is flattened image pixels \n",
    "digits = load_digits()\n",
    "  \n",
    "#split into train and test sets \n",
    "X_train, X_test, y_train, y_test=train_test_split(digits.data, digits.target, train_size=0.2, random_state=3)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[59.29244867395143,\n",
       " 59.41121549234063,\n",
       " 59.506590175345046,\n",
       " 59.58451078483185,\n",
       " 59.6491571706953,\n",
       " 59.703525259078546,\n",
       " 59.74980244584723,\n",
       " 59.78961450281116,\n",
       " 59.824190124510174,\n",
       " 59.854472357670645,\n",
       " 59.8811953437222,\n",
       " 59.90493806368299,\n",
       " 59.9261625881039,\n",
       " 59.945241717673234,\n",
       " 59.96247924520674,\n",
       " 59.97812500875451,\n",
       " 59.99238621521947,\n",
       " 60.00543605797238,\n",
       " 60.017420346420046,\n",
       " 60.02846265781256,\n",
       " 60.0386683784972,\n",
       " 60.04812790196621,\n",
       " 60.05691918049656,\n",
       " 60.06510977675068,\n",
       " 60.07275852527015,\n",
       " 60.07991688718659,\n",
       " 60.08663006185787,\n",
       " 60.09293790453816,\n",
       " 60.09887568823092,\n",
       " 60.10447473957615,\n",
       " 60.109762972294476,\n",
       " 60.11476533684395,\n",
       " 60.119504201178245,\n",
       " 60.12399967456003,\n",
       " 60.12826988407965,\n",
       " 60.132331211711865,\n",
       " 60.136198498301304,\n",
       " 60.13988521971535,\n",
       " 60.14340363947899,\n",
       " 60.14676494146175,\n",
       " 60.14997934558174,\n",
       " 60.153056209001186,\n",
       " 60.15600411488473,\n",
       " 60.15883095046242,\n",
       " 60.16154397586618,\n",
       " 60.164149884983345,\n",
       " 60.16665485938369,\n",
       " 60.169064616219536,\n",
       " 60.17138445086841,\n",
       " 60.173619274976915,\n",
       " 60.17577365047323,\n",
       " 60.17785182003596,\n",
       " 60.1798577344419,\n",
       " 60.181795077158384,\n",
       " 60.18366728649789,\n",
       " 60.185477575611515,\n",
       " 60.18722895056267,\n",
       " 60.188924226692286,\n",
       " 60.190566043460365,\n",
       " 60.19215687792665,\n",
       " 60.19369905701336,\n",
       " 60.19519476867615,\n",
       " 60.19664607209485,\n",
       " 60.19805490698266,\n",
       " 60.19942310210125,\n",
       " 60.20075238305951,\n",
       " 60.20204437946533,\n",
       " 60.20330063149172,\n",
       " 60.204522595912636,\n",
       " 60.205711651657545,\n",
       " 60.206869104928955,\n",
       " 60.20799619392232,\n",
       " 60.209094093183865,\n",
       " 60.21016391763821,\n",
       " 60.211206726314856,\n",
       " 60.21222352579893,\n",
       " 60.21321527343009,\n",
       " 60.21418288027057,\n",
       " 60.215127213861436,\n",
       " 60.2160491007848,\n",
       " 60.216949329047196,\n",
       " 60.21782865029891,\n",
       " 60.21868778190217,\n",
       " 60.21952740885988,\n",
       " 60.22034818561596,\n",
       " 60.221150737736984,\n",
       " 60.22193566348415,\n",
       " 60.222703535283905,\n",
       " 60.22345490110472,\n",
       " 60.224190285746836,\n",
       " 60.224910192051425,\n",
       " 60.225615102034915,\n",
       " 60.226305477953915,\n",
       " 60.22698176330544,\n",
       " 60.227644383767164,\n",
       " 60.22829374808176,\n",
       " 60.228930248889185,\n",
       " 60.22955426351042,\n",
       " 60.23016615468597,\n",
       " 60.23076627127207]"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_epoch_loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjgAAAI4CAYAAABndZP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeZxedX33/9fn2mZfM9kXspCE1QRM2EEWBQy22NbttlDUWqxVa71dqv7uX/Vua29//rTVu/W2IuDGooigVIXihhaUQAIBEhIgZF8msySzL9f2uf8410wmw4TMZK6Za+Y67+fjcT3OOd9zznV9rzNk5s13OcfcHREREZFiEil0BURERETyTQFHREREio4CjoiIiBQdBRwREREpOgo4IiIiUnQUcERERKToKOCIyJRjZg+a2U35PlZEwsN0HxwRyQcz6xqyWQ70A5nc9vvc/c7Jr5WIhJUCjojknZntAt7r7r8YYV/M3dOTXysRCRN1UYnIhDKzy81sn5n9rZk1At80szoz+4mZNZvZkdz6giHnPGJm782tv8vMHjWzL+aO3WlmbzzJY5eY2W/NrNPMfmFmXzWzOybxcojIJFHAEZHJMAeoB04Bbib43fPN3PYioBf4t1c5/3zgBaAB+AJwm5nZSRx7F/AEMAP4LHDjSX8jEZnSFHBEZDJkgc+4e7+797p7q7v/0N173L0T+Bzwulc5f7e7f8PdM8C3gbnA7LEca2aLgLXA37l70t0fBR7I1xcUkalFAUdEJkOzu/cNbJhZuZl93cx2m1kH8Fug1syixzm/cWDF3Xtyq5VjPHYecHhIGcDeMX4PEZkmFHBEZDIMn83wUWAlcL67VwOX5cqP1+2UDweBejMrH1K2cAI/T0QKSAFHRAqhimDcTZuZ1QOfmegPdPfdwAbgs2aWMLMLgT+Y6M8VkcJQwBGRQvgyUAa0AI8DD03S5/4pcCHQCvwj8H2C+/WISJHRfXBEJLTM7PvANnef8BYkEZlcasERkdAws7VmtszMImZ2LXA98KNC10tE8i9W6AqIiEyiOcB9BPfB2Qe8392fLmyVRGQiqItKREREio66qERERKToTMkuqoaGBl+8eHGhqyEiIiJT3MaNG1vcfebw8ikZcBYvXsyGDRsKXQ0RERGZ4sxs90jl6qISERGRojOqgGNmu8zsOTPbZGYbhpR/yMxeMLMtZvaF45xba2b3mtk2M9uau3uoiIiIyIQZSxfVFe7eMrBhZlcQ3EPiNe7eb2azjnPeV4CH3P0tZpYAyo9znIiIiEhejGcMzvuBz7t7P4C7Nw0/wMwGHqL3rtwxSSA5js8UEREROaHRjsFx4GEz22hmN+fKVgCXmtl6M/uNma0d4bylQDPwTTN72sxuNbOKkT7AzG42sw1mtqG5uXnMX0RERERkwGgDzsXufi7wRuADZnYZQetPHXAB8HHgHjOzYefFgHOBr7n7OUA38MmRPsDdb3H3Ne6+ZubMV8z2EhERERm1UQUcdz+QWzYB9wPnEdzm/D4PPAFkgYZhp+4D9rn7+tz2vQSBR0RERGTCnDDgmFmFmVUNrANXA5sJHlB3Za58BZAAWoae6+6NwF4zW5krugp4Pm+1FxERERnBaFpwZgOPmtkzwBPAT939IeB2YKmZbQa+B9zk7m5m88zsZ0PO/xBwp5k9C6wG/im/X2H0fre9hWu//Ftebu4qVBVERERkEpxwFpW77wBWjVCeBG4YofwAsG7I9iZgzfiqmR89yQzbGjvp6c8UuioiIiIygUJ1J+N4LPi6yUy2wDURERGRiRSugBMNJnmlFHBERESKWqgCTiIafF0FHBERkeIWqoATU8AREREJhVAFnIEuqmTaC1wTERERmUihCjjqohIREQmHUAWceC7gpLMKOCIiIsUsXAEnN008pS4qERGRohaugDMwBkddVCIiIkUtVAFHY3BERETCIVQBR9PERUREwiFUAefonYw1BkdERKSYhSvgRHLPokqrBUdERKSYhSrgRCJGLGKaJi4iIlLkQhVwILgXjrqoREREilsIA46pi0pERKTIhS7gJGIRzaISEREpcqELOLGIAo6IiEixC13AicdMY3BERESKXPgCTjSiRzWIiIgUudAFnEQ0QloBR0REpKiFLuBomriIiEjxC2HAMQ0yFhERKXIhDDgR3QdHRESkyIUy4KgFR0REpLiFMOBomriIiEixC2HAUQuOiIhIsQtfwNGjGkRERIpe6AJOQtPERUREil6s0BWYbJomLiIiYebupLNOMp0NXplg2Z/bTmWOlg0sU8OWyYyPUBasp9LBvv5MllQ6y/Wr53Pda+ZO+vcMYcBRF5WIiEy+bNZJZrL0pTL0p7P0p7IkMxn6UkfDRX86M2Q92B4aPoZvHz3u6L7BcDJ8PZ0LHZksnseODLOgdyQRjZCIRYhHI8RjRjxX1t6byt+HjcGoAo6Z7QI6gQyQdvc1ufIPAR8E0sBP3f0Txzk/CmwA9rv7m/JQ75Om++CIiISbuw8GjN5Uhr5Uhr50EDT6BrZTubCRyub2HV0/dpmlP5V5xXJoQOkfCDB5+J/riEFJLEpJPAgPA8tELEoiFqEkGqEiEaOufPj+o6+SaBBChpYNHhM9tiw+bN/gdi7EJKIRYtGpOdplLC04V7h7y8CGmV0BXA+8xt37zWzWq5z7YWArUH1y1cwfTRMXEZmaBoJHbzJDbypDTzIIFr2pzGBZ35D13lSGviHrvcmjAWXw2FQQOHqHBJe+dOakWzAiBqXxKKXxKCWxCCWxSLCe264pi5OoLKE0HhlyTBA+SuPBekkuQAzsT+TeJxE7ur80HiERjQ7uGwgqUzVMTEXj6aJ6P/B5d+8HcPemkQ4yswXAdcDngP8+js/LC3VRiYicPPegm6WnP0N3Mk1vMkN3MkNPf5qe5NGynmSGnmQ6t8zkjhuyL5WhN5k+Gl5yQSU7xuBhBmW5wFEai1CWCNbL4lHKEzHqKyKD26XxaLA/FqEkHh1SHsmFlgiludaRklj0aFk8Ohhk4goY08ZoA44DD5uZA19391uAFcClZvY5oA/4mLs/OcK5XwY+AVS92geY2c3AzQCLFi0aZbXGLh6NkM467o6ZTdjniIhMBe5OXypLZ3+K7v4M3f1pOvvSdPen6U6m6erPref2dSeDZU9uX8/g9tFlegwpJBoxyhPR3CuWCx5RqktjzKkuCcoS0cHyo+EkOlheljh230D5QPDQ73IZyWgDzsXufiDXDfVzM9uWO7cOuABYC9xjZkvdjzb8mdmbgCZ332hml7/aB+RC0y0Aa9asmbA+pEQsSN+pjJOI6R+FiExN7k53MkNnX4rOvvTgsisXULr60nT2B8uu/tTR8lxZd39uvT896laRsniUipIYFSVRKhLBsq48wcK6csoTwb6yRJTKktgxoWVgWVFybFlZIkoiqgAihTGqgOPuB3LLJjO7HzgP2Afclws0T5hZFmgAmoecejHwh2a2DigFqs3sDne/IZ9fYizi0eAfWiqTHQw7IiL5lsk6nX0pOnrTdPSlaO9N0dGbomNIWWdf+mhZX3pwu7MvNapgYgYViRhVpTEqS2JU5pZzqkupKMmVlcSC9dIYlbngMlBWMbg/CCXRiIKIFI8TBhwzqwAi7t6ZW78a+HugC7gSeMTMVgAJoGXoue7+KeBTufe5nKAbq2DhBhjsP9U4HBE5EXenqz9NW08QUAaXvcnB9fYhZe29ucDSm6KzP/2q720G1aVxqkpjVJXGqS6NMb+2jNPnVFFdFqeyJDa4L1ge3R7YV5GIEVEoERnRaFpwZgP355oYY8Bd7v6QmSWA281sM5AEbnJ3N7N5wK3uvm7Caj0OAwEnH9P1RGT6yGSdtp4kR3qSHO5OcaQnyZHuJId7grBypDvJkZ4UbT1J2npzy57Uq443GZg1M/CaV1M6GFCGlleXBQGmuiwIKzVlcYUTkQl2woDj7juAVSOUJ4FXtMbkurNeEW7c/RHgkZOpZD4d7aLSVHGR6SyZznK4O0lLVz+t3UkOd/fT2pUM1ruC4HK4+2iIae9NHXdqcEksQl15gtryOHXlCVbMrqSmLEFdeZza8ji1ZcG+2vIENWVBWU1ZnNJ4dHK/tIiMWijvZAyQ0s3+RKacvlSG5s5+mrv6aRlcBiFm4NXaFWx39I3cBRSLGHUVCWZUJKivSHD6vGrqy4P1uvI49ZUl1JcnqKsIwkx9RUJBRaQIhTbgpLMKOCKTwd3p6EvT1NFHU2c/h3LLpo4gwDR19NHc1U9zR/9xx63UlsdpqCxhRkWC0+dW01CZYEZlCTMqE8yoGFgG69VlMc3aEZHwBpxkWl1UIuOVymRp6uynsb0veHX00djey6GOfho7+mjqCMr6Uq/8H4ryRJRZVSXMqirl9DnVXLa8hJlVuVdlsGyoLKG+IqEZjyIyZqELOAP3vtEsKpFXl806Ld39HGjr40Bbb+7Vx8H2Xg60B0GmqbP/FeNaSmIR5tSUMruqlLMX1PL6qhJmV5cyqzq3rCphVnUplSWh+/UjIpModL9hNE1cJJDJOoc6+th7uId9R3rZ39bLviM97G/rZf+RIMQMfzBtWTzK3NpS5tWUsWL5TObWlDK3tow51aXMqSllbk0pNWVxdRGJSMGFNuBomriEQXtvir2He9jd2sOew8Fr7+Ee9h7p4UBb7ytmE86sKmFBXRlnza/hmjPnML+ujHk1ZcyrLWNercKLiEwfIQw4miYuxaWtJ8mOlm52tXSzq7WH3a1Hl209qWOOra9IsLCujLPn17Du7LksqCtjYV05C+qCEKPZRCJSLEIYcDRNXKaf/nSG3a09vNzUxY6Wbl5u7mJnSzc7W44NMRGDebVlLJ5Rwbqz53JKfTmnzChnUX0FC+vLqCqNF/BbiIhMntAGHE0Tl6moqz/N9qYutjd18VJTJy83dfFSUxd7D/cc81yiOdWlLJ0ZhJilDRUsnlHB4oYgxJTE1AojIhLagJNUF5UUUH86w/amLl5o7OSFQ528dChY39/WO3hMPGosaajgrHk1XL9qHstmVbK0oZIlMys0A0lE5ARC91syoS4qmUTuTnNnP1sOdvD8gQ62NXay7WAHO1q6yeSaZBLRCEtnVvDaU+r4b+ctZPnsKpbPqmRRfTmxqO7/IiJyMkIXcOK6D45MEHdn7+FentvfznP723n+YAfPH2inpSs5eMyCujJOm1PNtWfNYeWcKk6bU8XiGRUKMiIieRa+gKP74EgeuDsH2/t4Zm8bm/a1sXl/O5v3d9DeGwz4jUeN5bOquGLlLM6YV82Z82o4bW4V1RrkKyIyKcIXcCIagyNj192f5pm9bTy15wib9raxaW87LV39QBBmTptTzbqz53L2/BrOnl/DijmVGuwrIlJA4Qs46qKSUdh3pIcNu47w5K7DPLWnjRcaOwZnMS2dWcFlKxpYtaCWVQtrOX1ulcKMiMgUE76AMzBNXAFHctyd7U1dPL6jlSd3HWHDrsMcaO8DoLIkxuqFtXzwilM555Q6zllYS215osA1FhGREwldwIlFghYcdVGF19BA8/iOwzy+o5XW7mAg8OzqEtYurud9i+tZu7ielXOqiEb0aAIRkekmdAHHzEhEI+qiCplDHX08tr2FR7e38Nj2Fg51BONn5taU8roVM7lg6QzOX1rPovpyPWtJRKQIhC7gQDAoVPfBKW796Qwbdh3hkRea+M2Lzbx4qAsInsV00bIZXHxqAxctm6FAIyJSpMIZcGJqwSlGhzr6+OXWJn61rYnfvdxCTzJDIhph7ZI6/uTcBVyyvIHT51QTUZeTiEjRC2XAiUUiGoNTBNydrQc7+cXWQ/xi6yGe3dcOwPzaMv7onPlcvnIWFy2bQYUeayAiEjqh/M2fiJpacKapbNbZtK+NB587yIObG9l3pBczWL2wlo9fs5I3nDGb5bMq1e0kIhJyoQw48VhE08SnEXfnqT1H+I9nDvLQ5kYaO/qIR41LTm3gQ1eeypWnzWZmVUmhqykiIlNIOANONEJKXVRTmruzrbGTB545wAObDrC/rZdELMLrVszkb89eyZWnzaamTI89EBGRkYU24CTVgjMlNXX0cf/T+/nhU/t48VAX0UjQUvPRq1fwhjNmU6VnOYmIyCiEMuBoDM7U0p/O8Ivnm7h3415+82IzWYdzF9XyD9efybqz5zKjUt1PIiIyNqEMOHHd6G9KeLm5i7vX7+Hep/bR1pNibk0p7798GX9y7gKWzqwsdPVERGQaC2XAiUWNVFpjcAohmc7y8PON3Pn4Hn6/o5VYxLjmzDm847yFXLSsQY9FEBGRvAhlwIlHI3Sm0oWuRqi0dPVz5+N7+O7ju2np6mdBXRkfv2Ylb1uzUDOgREQk70IZcBLRCOmsuqgmw7bGDm5/dCc/2nSAZDrL5StnctOFi7lsxUy11oiIyIQJZcCJRyPqoppA7s76nYf56q+3818vtVAaj/DW1y7g3Rcv5tRZVYWunoiIhMCoAo6Z7QI6gQyQdvc1ufIPAR8E0sBP3f0Tw85bCHwHmANkgVvc/St5q/1J0rOoJoa786ttTXz119t5ak8bDZUJPn7NSt553iLqKhKFrp6IiITIWFpwrnD3loENM7sCuB54jbv3m9msEc5JAx9196fMrArYaGY/d/fnx1ft8YlHTffByaNs1vnPLY185Zcvsa2xk/m1ZfzD9Wfy1jULKY1HC109EREJofF0Ub0f+Ly79wO4e9PwA9z9IHAwt95pZluB+UBBA05C08Tzwt155IVmvvjwC2w50MHSmRV86a2r+MPV84hHI4WunoiIhNhoA44DD5uZA19391uAFcClZvY5oA/4mLs/ebw3MLPFwDnA+uPsvxm4GWDRokWjrf9JiUVNj2oYp9+93MKXHn6RjbuPsLC+jC+9dRVvPme+Bg6LiMiUMNqAc7G7H8h1Q/3czLblzq0DLgDWAveY2VJ3f0VyMLNK4IfA37h7x0gfkAtNtwCsWbNmQtNHMMhYLTgnY3tTJ5/76VZ+/UIzc6pL+dwfncXb1ixUi42IiEwpowo47n4gt2wys/uB84B9wH25QPOEmWWBBqB56LlmFicIN3e6+335rPzJSkQjpDRNfEyOdCf58i9e5I71eyiPR/nUG0/jposWa4yNiIhMSScMOGZWAURyY2gqgKuBvwe6gCuBR8xsBZAAWoada8BtwFZ3/+d8V/5k6Wnio5fKZPnO73fzlV+8SFd/mneev4iPvH6Fng8lIiJT2mhacGYD9wdZhRhwl7s/ZGYJ4HYz2wwkgZvc3c1sHnCru68DLgZuBJ4zs0259/u0u/8s799kDOLRCJmsk8m6xoy8iqf2HOHT9z3HtsZOLl3ewP+47gxWztF9bEREZOo7YcBx9x3AqhHKk8ANI5QfANbl1h8FplyCiMeCKqUyWaIRdbEM196b4gsPbeOuJ/Ywp7qUr9/4Wq4+Yza5kCsiIjLlhfJOxoncgNhUJqsxJEO4Oz959iD/8z+e53B3P++5eAkfecMKKktC+Z+JiIhMY6H8yxWLDLTgaBzOgNaufv7Hjzbz4OZGXrOghm+9ey1nza8pdLVEREROSigDTjx2tAVH4OfPH+JT9z1LR2+av732NG6+bKnGJomIyLQWzoATVcAB6OxL8ff/8Tw/2LiP0+dWc8d7V3HanOpCV0tERGTcQhlwjo7BCW8X1bP72vjAXU+x/0gvH7hiGR++agWJmG7WJyIixSGUASfMLTjuzncf380//mQrDZUJfvCXF/LaU+oLXS0REZG8CmnACcaXJEP2uIau/jSf/OGz/OTZg1yxcib//LbV1FUkCl0tERGRvAtnwAnhIONtjR381R1Psau1m09cu5K/vGwZEQ0kFhGRIhXOgBMJ1xicX249xF/f/TTlJTHu+osLuGDpjEJXSUREZEKFM+BEj97JuJi5O7c9upPP/WwrZ82r4Rt/toY5NaWFrpaIiMiEC2fACUEXVSqT5e9+vIW7n9jDtWfO4V/evpqyhO7aLCIi4RDKgFPs08Tbe1P81Z0beWx7K391+TI+dvVKjbcREZFQCWXAKeZp4s2d/dx423pebu7ii29dxVteu6DQVRIREZl0IQ04xTkGZ39bLzfeup6D7X3c/q61XLp8ZqGrJCIiUhAhDThBC04x3QdnZ0s3N9y6no6+FN/98/NYs1g37xMRkfAKdcApljE4Ww92cONtT5B15+6/uEBPARcRkdALacApni6qzfvb+dNb11MWj3LHey/g1FmVha6SiIhIwYUz4BTJNPHtTZ382e1PUFkS43s3X8DC+vJCV0lERGRKCOXjo4thmvjewz3ccOsTRMy4473nK9yIiIgMEcqAM92niTd19HHDbevpTWW4473nsaShotBVEhERmVJCGXCiESNi0zPgHOlOcsNt62nu7Odb717LaXOqC10lERGRKSeUY3AgaMVJTrOA05NM865vPsGu1h6+9e61nLOortBVEhERmZJC2YIDQcBJpafPGJxs1vnv33+G5/a389V3nstFyxoKXSUREZEpK8QBx6ZVF9W//OJFHtrSyKfXnc4bzphd6OqIiIhMaSEOOBHS2ekRcH68aT//+qvtvH3NQv78kiWFro6IiMiUF+qAk5wGXVSb9rbx8Xuf5bzF9fzDm8/CTE8FFxEROZHQBpxELDLlu6gOtvfyF9/ZwKyqEr52w7kkYqH9cYmIiIxJaP9iTvUxOP3pDO/77kZ6+tPcdtNaZlSWFLpKIiIi00aop4lP5YDzhYde4Nl97Xz9xteyck5VoasjIiIyrYS2BScWjZCcoo9q+PW2Jm57dCc3XXgK15w5p9DVERERmXZCG3ASUSOVnnotOE0dfXzsB89w2pwqPrXu9EJXR0REZFoaVcAxs11m9pyZbTKzDUPKP2RmL5jZFjP7wnHOvTZ3zHYz+2S+Kj5eU3GaeDbrfOSeTXQn0/zbO8+hNB4tdJVERESmpbGMwbnC3VsGNszsCuB64DXu3m9ms4afYGZR4KvAG4B9wJNm9oC7Pz/Oeo9bPBqhO5kpdDWO8e+/fZnHtrfy+T8+m1NnadyNiIjIyRpPF9X7gc+7ez+AuzeNcMx5wHZ33+HuSeB7BKGo4IJHNUydFpyn9hzhSw+/yHVnz+XtaxcWujoiIiLT2mgDjgMPm9lGM7s5V7YCuNTM1pvZb8xs7QjnzQf2Dtnelyt7BTO72cw2mNmG5ubm0db/pCViU2eaeF8qw0fveYY51aX80x+frZv5iYiIjNNou6gudvcDuW6on5vZtty5dcAFwFrgHjNb6u5DpyaN9Jd6xKlL7n4LcAvAmjVrJnx601SaJv5vv9rOzpZu7vjz86kpixe6OiIiItPeqFpw3P1AbtkE3E/Q9bQPuM8DTwBZYPgjrvcBQ/tbFgAHxlvpfAgCTuGnib/Q2Mm//+Zl/vjc+VyyXE8IFxERyYcTBhwzqzCzqoF14GpgM/Aj4Mpc+QogAbQMO/1JYLmZLTGzBPAO4IH8Vf/kxaNGssAtONms88n7nqWqNMb/uO6MgtZFRESkmIymi2o2cH9uXEgMuMvdH8oFltvNbDOQBG5ydzezecCt7r7O3dNm9kHgP4EocLu7b5mYrzI28WiEdIEDzp3rd/P0nja+9NZV1FckCloXERGRYnLCgOPuO4BVI5QngRtGKD8ArBuy/TPgZ+OrZv4Vuouqsb2PLzz0AhefOoM/PnfEcdciIiJykkJ7J+N4NFLQLqrPPrCFZCbL596sWVMiIiL5FtqAk8g9TfzYSV+T4+EtjTy0pZEPv345ixsqJv3zRUREil1oA048GsEdMtnJDTjJdJZ/+tlWls+q5C8uXTqpny0iIhIW4Q04seCrT/Y4nLuf2MOu1h4+te404tHQXn4REZEJFdq/sLFIMO5lMsfhdPal+N+/fInzl9RzxcpXPLpLRERE8mQsD9ssKolcC85kThX/xm930Nqd5LZ1p2tgsYiIyAQKbQvOQPfQZHVRNXX08Y3/2sl1Z89l9cLaSflMERGRsFLAmaQWnH/5xUukMlk+fs3KSfk8ERGRMAtxwJm8MTjbm7q4Z8Ne/vT8RZoWLiIiMglCG3ASk9iC84WHtlEai/Chq5ZP+GeJiIhIiAPOYBdVemLH4GzcfYSHnz/E+163jIbKkgn9LBEREQmENuDEJqmL6qu/3k59RYL3XrpkQj9HREREjgptwBnooprIaeIvNHbyq21N3HThYsoToZ2RLyIiMulCG3Am407GX//ty5TFo/zZhadM2GeIiIjIK4U34EzwIOMDbb08sOkAb1+7kLqKxIR8hoiIiIwsxAFnYsfg3PboThw09kZERKQAQhtwJnKaeHtPiruf2MMfvGYuC+rK8/7+IiIi8upCG3Amsovqu4/voieZ4X2vW5b39xYREZETC23AGZgmnu/74PSlMnzrd7u4fOVMTp9bndf3FhERkdEJbcAZ7KLK5rcF596N+2jpSvK+y9R6IyIiUiihDThH72Scv4CTyTrf+K8drFpYywVL6/P2viIiIjI24Q04E3AfnF9va2J3aw/vu2wpZpa39xUREZGxCW/AmYBp4t97ci8zq0p4wxmz8/aeIiIiMnbhDTiR/M6iauro49cvNPGW1y4Y7P4SERGRwgjtX+JIxIhFLG8B596n9pHJOm9bszAv7yciIiInL7QBB4Kp4vkYg+PufP/JvZy/pJ4lDRV5qJmIiIiMR6gDTjwayUsLzuM7DrO7tYd3nKfWGxERkakg1AEnkaeA8/0n91BVGuONZ83NQ61ERERkvEIdcOLRyLjvZNzek+LBzY28efV8SuPRPNVMRERExiPcASc2/kHGP35mP/3pLG9fq+4pERGRqSLcAScaGfd9cL73xF7Oml/NWfNr8lQrERERGa9RBRwz22Vmz5nZJjPbkCv7rJntz5VtMrN1xzn3I2a2xcw2m9ndZlaazy8wHuMdg7N5fzvPH+zg7WsX5bFWIiIiMl6xMRx7hbu3DCv7F3f/4vFOMLP5wF8DZ7h7r5ndA7wD+NaYazoBxjtN/HtP7qEkFuEPV83LY61ERERkvCajiyoGlJlZDCgHDkzCZ47KeKaJ96Uy/PjpA1x39lxqyuJ5rpmIiIiMx2gDjgMPm9lGM0JJD+sAACAASURBVLt5SPkHzexZM7vdzOpecZL7fuCLwB7gINDu7g+P9AFmdrOZbTCzDc3NzWP8GidnPAHn0Zda6OxPc/058/NcKxERERmv0Qaci939XOCNwAfM7DLga8AyYDVBePnS8JNyoed6YAkwD6gwsxtG+gB3v8Xd17j7mpkzZ479m5yEYAzOyXVRPbi5kerSGBcunZHnWomIiMh4jSrguPuB3LIJuB84z90PuXvG3bPAN4DzRjj19cBOd2929xRwH3BRfqo+fvHoyU0TT2Wy/GLrIV5/xmwSsVBPRBMREZmSTvjX2cwqzKxqYB24GthsZkNv2/tHwOYRTt8DXGBm5WZmwFXA1vFXOz/i0QjJ9NgDzuM7WmnvTenOxSIiIlPUaGZRzQbuD/IJMeAud3/IzL5rZqsJxufsAt4HYGbzgFvdfZ27rzeze4GngDTwNHBL/r/GyYnHTm4MzoObGylPRLl0ecME1EpERETG64QBx913AKtGKL/xOMcfANYN2f4M8Jlx1HHCxCNjnyaeyToPb2nkitNm6dEMIiIiU1SoB5DEoxHSY2zB2bDrMC1dSd541pwJqpWIiIiMV7gDTixCcowtOA9taSQRi3D5ylkTVCsREREZr1AHnLE+qsHd+c/NjVy2fCaVJWO5CbSIiIhMplAHnLFOE39mXzsH2vvUPSUiIjLFhTzgjK0F58HNB4lFjNefPnsCayUiIiLjpYCTcdxPPA7H3XlocyMXLptBTbmePSUiIjKVhTzgGMCopopva+xkd2uPbu4nIiIyDYQ84ARfP509cTfVg5sbMYOrz1T3lIiIyFSngAOk0iduwXl4SyNrF9fTUFky0dUSERGRcQp3wMk9KDN5goHGTZ19bGvs5Ard+0ZERGRaCHXASQyOwXn1gPO77a0AXHKqnj0lIiIyHYQ64Ax2UZ0g4Dy2vYWasjhnzKuejGqJiIjIOCng8OoBx915bHsLFy2bQTRik1U1ERERGYeQB5wgsCRfZZDxrtYeDrT3cbG6p0RERKaNkAecE08Tf3R7C4ACjoiIyDSigMOrd1E99lIL82vLWDyjfLKqJSIiIuOkgMPxu6gyWef3O1q5aNkMzDT+RkREZLoIdcBJxF59mviWA+2096a4ZLm6p0RERKaTUAecE3VRDYy/uWiZAo6IiMh0ooDD8QPO77a3snJ2FTOr9HgGERGR6STkASc3TXyEp4n3pTI8seuwZk+JiIhMQyEPOLlp4iO04GzcfYRkOssly2dMdrVERERknBRwGLmL6rHtLcQixnlLFHBERESmGwUcRu6iemx7C6sX1lJZEpvsaomIiMg4hTrgJAZacNLHtuC096R4bn+7xt+IiIhMU6EOOPHj3Afn9ztayboezyAiIjJdhTvgHGcMzu9ebqE8EWX1wtpCVEtERETGKdQBJxYZeZr4hl1HOHdRHYlYqC+PiIjItBXqv+BmRjxqx0wT701meOFQp1pvREREprFQBxwIuqmGdlFtOdBOJuusUsARERGZthRwohFSQ7qoNu1tA2DVwppCVUlERETGaVQBx8x2mdlzZrbJzDbkyj5rZvtzZZvMbN1xzq01s3vNbJuZbTWzC/P5BcYrHo2QHNKCs2lvG/Nry5hVVVrAWomIiMh4jOUudle4e8uwsn9x9y+e4LyvAA+5+1vMLAGUj6mGEywRtWPug/PMvja13oiIiExzE9pFZWbVwGXAbQDunnT3ton8zLGKx46OwWnt6mfv4V5WLdD4GxERkelstAHHgYfNbKOZ3Tyk/INm9qyZ3W5mdSOctxRoBr5pZk+b2a1mVjHSB5jZzWa2wcw2NDc3j+1bjEMsYoNjcJ7ZNzD+RgFHRERkOhttwLnY3c8F3gh8wMwuA74GLANWAweBL41wXgw4F/iau58DdAOfHOkD3P0Wd1/j7mtmzpw5xq9x8obOotq0t52Iwdnz1UUlIiIynY0q4Lj7gdyyCbgfOM/dD7l7xt2zwDeA80Y4dR+wz93X57bvJQg8U0ZiSBfVM3vbWDG7igo9YFNERGRaO2HAMbMKM6saWAeuBjab2dwhh/0RsHn4ue7eCOw1s5W5oquA58dd6zwamCbu7sEAY42/ERERmfZG01QxG7jfzAaOv8vdHzKz75rZaoLxObuA9wGY2TzgVncfmDb+IeDO3AyqHcC78/sVxiceNZKZLLtbe2jrSbF6kQKOiIjIdHfCgOPuO4BVI5TfeJzjDwDrhmxvAtaMo44TKh6N0NWfPjrAWC04IiIi017o72ScyA0y3rS3jbJ4lBWzKwtdJRERERmn0AeceDRCKu1s2tvG2fNriEVDf0lERESmvdD/NY9FjZ5Umi0HOnQHYxERkSIR+vnQiWiEvYd7Ad3gT0REpFiEvgUnPqRLSgOMRUREioMCTswAaKhMsKCurMC1ERERkXxQwMm14KxaUEvuXj8iIiIyzYU+4CQGAo7G34iIiBSN0AecgRac1Qo4IiIiRSP0Aae8JErE4DULNEVcRESkWIR+mvg7z1vE2sX11JYnCl0VERERyZPQt+DUlidYu7i+0NUQERGRPAp9wBEREZHio4AjIiIiRUcBR0RERIqOAo6IiIgUHQUcERERKToKOCIiIlJ0FHBERESk6CjgiIiISNExdy90HV7BzJqB3RP09g1AywS9t4yOfgaFpetfePoZFJauf+Hl82dwirvPHF44JQPORDKzDe6+ptD1CDP9DApL17/w9DMoLF3/wpuMn4G6qERERKToKOCIiIhI0QljwLml0BUQ/QwKTNe/8PQzKCxd/8Kb8J9B6MbgiIiISPELYwuOiIiIFDkFHBERESk6oQo4Znatmb1gZtvN7JOFrk+xM7OFZvZrM9tqZlvM7MO58noz+7mZvZRb1hW6rsXMzKJm9rSZ/SS3res/icys1szuNbNtuX8LF+pnMHnM7CO53z+bzexuMyvV9Z9YZna7mTWZ2eYhZce95mb2qdzf5RfM7Jp81SM0AcfMosBXgTcCZwD/zczOKGytil4a+Ki7nw5cAHwgd80/CfzS3ZcDv8xty8T5MLB1yLau/+T6CvCQu58GrCL4WehnMAnMbD7w18Aadz8LiALvQNd/on0LuHZY2YjXPPc34R3Amblz/k/u7/W4hSbgAOcB2919h7snge8B1xe4TkXN3Q+6+1O59U6CX+zzCa77t3OHfRt4c2FqWPzMbAFwHXDrkGJd/0liZtXAZcBtAO6edPc29DOYTDGgzMxiQDlwAF3/CeXuvwUODys+3jW/Hvieu/e7+05gO8Hf63ELU8CZD+wdsr0vVyaTwMwWA+cA64HZ7n4QghAEzCpczYrel4FPANkhZbr+k2cp0Ax8M9dNeKuZVaCfwaRw9/3AF4E9wEGg3d0fRte/EI53zSfsb3OYAo6NUKY58pPAzCqBHwJ/4+4dha5PWJjZm4Amd99Y6LqEWAw4F/iau58DdKPukEmTG+dxPbAEmAdUmNkNha2VDDNhf5vDFHD2AQuHbC8gaKqUCWRmcYJwc6e735crPmRmc3P75wJNhapfkbsY+EMz20XQJXulmd2Brv9k2gfsc/f1ue17CQKPfgaT4/XATndvdvcUcB9wEbr+hXC8az5hf5vDFHCeBJab2RIzSxAManqgwHUqamZmBGMPtrr7Pw/Z9QBwU279JuDHk123MHD3T7n7AndfTPDf+6/c/QZ0/SeNuzcCe81sZa7oKuB59DOYLHuAC8ysPPf76CqCsYC6/pPveNf8AeAdZlZiZkuA5cAT+fjAUN3J2MzWEYxJiAK3u/vnClylomZmlwD/BTzH0TEgnyYYh3MPsIjgF9Bb3X34gDTJIzO7HPiYu7/JzGag6z9pzGw1wSDvBLADeDfB/1zqZzAJzOx/Am8nmNX5NPBeoBJd/wljZncDlwMNwCHgM8CPOM41N7P/B3gPwc/ob9z9wbzUI0wBR0RERMIhTF1UIiIiEhIKOCIiIlJ0FHBERESk6CjgiIiISNFRwBEREZGio4AjIiIiRUcBR0RERIqOAo6IiIgUHQUcERERKToKOCIiIlJ0FHBERESk6CjgiIiISNFRwBGRSWNmbman5tb/3cz+39EcexKf86dm9vDJ1lNEpj89TVxExsTM/hNY7+5/N6z8euDrwAJ3Tx/nXAeWu/v2UXzOqI41s8XATiB+vM8VkfBRC46IjNW3gBvNzIaV3wjcqZAhIlOBAo6IjNWPgHrg0oECM6sD3gQ8YGa/N7M2MztoZv9mZomR3sTMvmVm/zhk++O5cw6Y2XuGHXudmT1tZh1mttfMPjtk929zyzYz6zKzC83sXWb26JDzLzKzJ82sPbe8aMi+R8zsH8zsMTPrNLOHzaxhHNdHRKYABRwRGRN37wXuAf5sSPHbgG1AF/ARoAG4ELgK+KsTvaeZXQt8DHgDsBx4/bBDunOfVwtcB7zfzN6c23dZblnr7pXu/vth710P/BT438AM4J+Bn5rZjCGHvRN4NzALSOTqIiLTmAKOiJyMbwNvNbOy3PafAd92943u/ri7p919F8GYnNeN4v3eBnzT3Te7ezfw2aE73f0Rd3/O3bPu/ixw9yjfF4JA9JK7fzdXr7sJwtgfDDnmm+7+4pDwtnqU7y0iU5QCjoiMmbs/CjQD15vZUmAtcJeZrTCzn5hZo5l1AP9E0JpzIvOAvUO2dw/daWbnm9mvzazZzNqBvxzl+w689+5hZbuB+UO2G4es9wCVo3xvEZmiFHBE5GR9h6Dl5kbgYXc/BHyNoHVkubtXA58Ghg9GHslBYOGQ7UXD9t8FPAAsdPca4N+HvO+JpoIeAE4ZVrYI2D+KeonINKWAIyIn6zsEY2X+gqDLCqAK6AC6zOw04P2jfK97gHeZ2RlmVg58Ztj+KuCwu/eZ2XkEY2YGNANZYOlx3vtnwAoze6eZxczs7cAZwE9GWTcRmYYUcETkpOTG2PwOqCBoXYFgcO47gU7gG8D3R/leDwJfBn4FbM8th/or4O/NrBP4O4JANHBuD/A54LHc7K0Lhr13K8EMr48CrcAngDe5e8tov6uITD+60Z+IiIgUHbXgiIiISNFRwBEREZGio4AjIiIiRUcBR0RERIpOrNAVGElDQ4MvXry40NUQERGRKW7jxo0t7j5zePmUDDiLFy9mw4YNha6GiIiITHFmNvxO5YC6qERERKQIKeCIiIhI0VHAERERkaKjgCMiIiJFRwFHREREio4CjoiIiBQdBRwREREpOlPyPjgiIiIyPaQzWZKZLMl08OrPvZLpoHx2dQlza8omvV4KOCIiItNUNuskM1n6UpkgWKSy9Kcz9OWWQdjI5MqHlB2zfyCY5I7LHN1/bGDJDDn2aIDJZP1V6/jRN6zgQ1ctn6QrcpQCjoiISB64+2B46E1l6EtlBpd9qSx96Qz9g2XZo+W5cBIsh+47GlSOXR4NI8lMdlx1NoOSWISSWDRYxiMkosF2IhahJBahqjRGw5BjBsoTA69odPC8xJD9A8csbajM0xUeGwUcEREpetms05sLF73JDD3JYL0nmQ6CSDJ7dD23PRhScseOuJ3M0JfO0pvM0JfO4K/emHFciWgQLkrjUUrjEUpjUUrjQaAoT8Sor8gFjHiwLB22LIlFBo8fOCYRi1A6UJ4LIAPHJHKBJR41zCy/F3uKUMAREZEpwT3obunpz9CTytDTn6Y7OWSZTNOTzNDdn6Y3maE7maE3V9YzGFzSgwHm2BAz9paORDRCWSJKWTxKWWIgbATrNWVxShNRSmNRyhIRyuLRXDgJXmUDQSW3PhheYkfLB8JMSSxKNFKcIaOQRhVwzKwWuBU4C3DgPcALwPeBxcAu4G3ufmTYeQuB7wBzgCxwi7t/JU91FxGRAnJ3+lJZuvrTdPenj1l29R8NI0fLg7DR3R+U9ySD4NKd29+TzJA+wXiOoeJRozwRozwRzb1ilCWi1FUkmF8XpSweoywRtICUxaOD4aQ0Hh08JzhmYD0XThJRSmMRYlFNNJ7ORtuC8xXgIXd/i5klgHLg08Av3f3zZvZJ4JPA3w47Lw181N2fMrMqYKOZ/dzdn8/XFxARkbFxd7qTGTr7UnT2pXOvVBBM+oJA0tE3sJ4r78/QNeyY7mTmhANMB5TFo1SURKkoiVGeiFFZEqW2PAgiFYlYrjx6dJmIUV4SLMuGbJcnopTnQkkipgAix3fCgGNm1cBlwLsA3D0JJM3seuDy3GHfBh5hWMBx94PAwdx6p5ltBeYDCjgiIicpmc7S2Zeioy9NR2+Kjr4UHb3p3DLY7sztGwgwHYNhJggpo8kllSWx4FUaG1yfWVVCVWmcypIYFSVRKkviVOaCS0VJjKrcsiJ3/EBIUReMTLbRtOAsBZqBb5rZKmAj8GFgdi7A4O4HzWzWq72JmS0GzgHWH2f/zcDNAIsWLRpl9UVEpqdUJkt7b4q2nhTtvcncMjWkLAgrA2UdfQNlaXpTmVd974hBdVmcqtIY1aXBcmF9+THbwSsIKgPblSVH91UkYkQUSmQaG03AiQHnAh9y9/Vm9hWC7qhRM7NK4IfA37h7x0jHuPstwC0Aa9asOclx6CIikyubdTr6UhzuTnKkJ0Vbz9BlEFzaeoPttp5U7pWkO/nqIaWqJEZ1WZzqsjg1ZTGWNFRQXRqnpix4BfuCwFJdFs8tg9BSkYgW7cwYkdEaTcDZB+xz94GWl3sJAs4hM5uba72ZCzSNdLKZxQnCzZ3ufl8+Ki0iMlF6kxlau/s53J2ktTvJ4a4gqLR2JznSneTwwGswsCSP290TMagrT1BTHqeuPMGc6lJWzqkKysri1JYfDSy1ubKasjjVpTENcBUZpxMGHHdvNLO9ZrbS3V8AriIYQ/M8cBPw+dzyx8PPteB/IW4Dtrr7P+e15iIio5DJOoe7k7R09dPaFSxbuvpp7U7S0hksW7uTtHYFoabnOC0rsYhRV5FgRkWCuvIEp82poj63XleeoK4iCCnBdrBeVaJuHpFCGe0sqg8Bd+ZmUO0A3k3woM57zOzPgT3AWwHMbB5wq7uvAy4GbgSeM7NNuff6tLv/LI/fQURCxt3p6E3T3NVHU0c/zV39NHcOeeW2W3KhZaQWlnjUmFFRwozKBDMqS1jWUEF9RYL6yiDE1FeUUD8QaCoSVJfG1O0jMo2MKuC4+yZgzQi7rhrh2APAutz6o4B+I4jIqLg7nf1pmjr6aGzv51BHH4c6gxDT1NnHoY6grLmzn/70K2/clohFmFlZwsyqEhbUlXPOojpmViZoqCqhobKEGRVH1xVYRIqb7mQsIpPCPegqOtjex8H2Phrbe4NlRx+N7blXR9+IXURVJTFmVZcwu7qUNafUMau6lFlVQZCZWVWSWy9VaBGRQQo4IpIXfakM+9t6OZB77T/Sy/62Pg609XIwF2aGt7rEIsbs6lLm1JRy+txqLl85izk1QZA5+iqhPKFfVSIyNvqtISKj0pvMsL+th71Hetl3uId9R3pzrx72t/XS0pU85viIwezqUubVlnHW/BquPnMOc6pLmVdbytyaMubUlNJQWaIbwInIhFDAEREg6EJq6uxnd2sPu1u72Xu4hz2Hg0Cz53APzZ39xxyfiEWYX1vGgroyTp9bHazXlzGvpoz5dWXMri4lrqnOIlIgCjgiIZLNOo0dfexq7WZXS09u2R2EmsPdxzxxOWIwt6aMRfXlXLlyFgvry1hYX86CunIW1pXRUFmiKdAiMmUp4IgUoc6+FDuau3m5uYsdzd3sbOlmR0s3O1u6jgkxiViEU+rLOWVGBZcsb+CUGeUsqi9n8YwK5tWW6WGGIjJtKeCITFPuTktXkpeaOtne1MVLh7p4uTl4Heo42p0UjRgL68pYOrOSi5bNYElDBUsaKljcUMHc6lK1wohIUVLAEZkGjnQneeFQJy8e6uSFxmD5UlMXbT2pwWOqSmIsm1XJJafOZNmsCpbNrGTZzAoW1VeoJUZEQkcBR2QKSWWybG/qYltjB9sOdrK1sZNtBztoGjLAt7o0xorZVaw7ey7LZ1WyfFYVy2dXMquqRPeAERHJUcARKZCu/jTPH+jg+QPtbDnQwfMHO3jpUBfJTDBGJhGNsHx2JZcun8lpc6pYMaeKlbOrmF2tICMiciIKOCKToKs/zeb97Wze385zudfOlm4894ykGRUJzphXzXsuWcLpc6s4Y241Sxoq9ERpEZGTpIAjkmepTJZtBzvZtK+NZ/YGr+3NXYNhZk51KWfNr+H6VfM5a341Z82vUfeSiEieKeCIjFNTZx9P7W7j6T1HeHpPG8/ubxucil1fkWDVghque81cVi2o5az5NcysKilwjUVEip8CjsgYZLPOy81dbNh9hCd3HWbj7iPsbu0BIB41zpxXwzvPO4VzFtWyemEtC+rK1DIjIlIACjgiryKbdbY1dvL4jlbW72zliZ2HOZKbmj2jIsFrT6njhvNP4dxT6jhzXjWl8WiBaywiIqCAI3IM96CF5rHtrTy2vYX1Ow/T3hsEmgV1ZVx52mzOX1LPmsV1LGmoUOuMiMgUpYAjodfU2cd/vdjCo9tbeGx7y+A9ZxbWl3HNmbO5YOkMzl86g/m1ZQWuqYiIjJYCjoROMp1lw67D/OalZn77YgtbD3YAQZfTRac2cPGyGVx8agML68sLXFMRETlZCjgSCk2dfTyyrZlfbWvi0e0tdPWniUeN155SxyeuXclly2dyxtxqPZdJRKRIKOBIUXJ3Xmrq4uEtjTz8/CGe3dcOBPeg+YNV87jytFlctGwGFSX6JyAiUoz0212KRjbrPL23jf/c0sjDWxrZlZu+vXphLR+7egVXnDaLM+ZWa2CwiEgIKODItDYQan767EEe3HyQg+19xKPGhcsaeO+lS3nDGbOZXV1a6GqKiMgkU8CRacfd2XKggx9v2s9Png1CTSIa4bIVDXzi2pVcdfpsqkvjha6miIgUkAKOTBt7D/fw4037+dGmA2xv6iIeNV63YqZCjYiIvIICjkxpPck0P3uukR9s2Mv6nYcBOG9xPZ/7o7NYd9Zc6ioSBa6hiIhMRQo4MuW4O0/taeMHG/byk2cP0tWfZvGMcj5+zUquXz2PBXW6P42IiLw6BRyZMjr7Uvzo6f3cuX4P2xo7KU9Eue7subxt7ULWnFKn2U8iIjJqCjhScM8f6OCO9bv58dP76U5mOGt+Nf/rj8/mD1bNo1L3qRERkZOgvx5SEJms88uth7jt0Z2s33mYkliEP1w1jxsuOIXXLKhRa42IiIyLAo5Mqu7+ND/YsJdv/m4Xu1t7mF9bxqfXncbb1yyiplyzoEREJD8UcGRSHO5O8q3HdvKt3+2ioy/NuYtq+cQ1p3HNmbOJRSOFrp6IiBQZBRyZUAfbe/nGb3dy9xN76E1luObM2bzvdcs4d1FdoasmIiJFTAFHJsSBtl7+9VfbuXfjXrIO16+ex/tft4zls6sKXTUREQkBBRzJq6aOPv7PIy9z1/o9OM7b1y7kfZctY2G97l0jIiKTRwFH8uJId5J//83LfPv3u0hlnLetWcAHr1zO/NqyQldNRERCSAFHxqU/neHbv9vFv/5qO939ad68ej5/fdVyFjdUFLpqIiISYqMKOGZWC9wKnAU48B7gBeD7wGJgF/A2dz8ywrnXAl8BosCt7v75fFRcCsvd+elzB/n/HtrG3sO9XLFyJp9adzorNMZGRESmgNG24HwFeMjd32JmCaAc+DTwS3f/vJl9Evgk8LdDTzKzKPBV4A3APuBJM3vA3Z/P2zeQSffsvjY++8AWntrTxmlzqrjjz8/nkuUNha6WiIjIoBMGHDOrBi4D3gXg7kkgaWbXA5fnDvs28AjDAg5wHrDd3Xfk3ut7wPWAAs401N6T4v9/eBt3rt/DjIoSvvAnr+FPXruAaER3HRYRkallNC04S4Fm4JtmtgrYCHwYmO3uBwHc/aCZzRrh3PnA3iHb+4DzR/oQM7sZuBlg0aJFo/4CMvHcnR8+tZ//9bP/296dh9lV13kef3+zLxSEhCRkJQkJYTUhlIEIjQs2S7CxfWhssBUUe+i2HRVGcUDpsWem7W6XdpqecVREUQceVsENRFDRgINAEhISshPIQipJJSRkgaz1nT/upae6rJBKajl1732/nsenzvnlntzv/R2s+8n5/X7nLGbLa3v48NvGcd0fn8CR/bzzsCSpe2pLwOkFTAM+kZlPRcTNlIaj2qK1f9pnay/MzFuAWwDq6+tbfY263guNO7jxhwt4+qVXmDZ2ED/46HROGXlU0WVJkvSm2hJw1gJrM/Op8v59lALOhogYUb56MwLYeIBjxzTbHw2sa0/B6hr7m5LvPLGSf35kGf169+RLl57GZWeMoYfDUZKkCnDQgJOZ6yNiTURMzsylwHmU5tAsAq4C/qn888etHP4MMCkixgMvA5cDH+io4tU5VmzcwfX3zefZ1Vv545OH88X3ncqwun5FlyVJUpu1dRXVJ4A7yiuoVgIfAXoA90TER4HVwGUAETGS0nLwmZm5LyL+I/ALSsvEv5uZz3f0h1DHaGpKbn1iJV99ZBkD+vTk5suncsmUkUR41UaSVFnaFHAycx5Q38ofndfKa9cBM5vtPwQ8dLgFqmts3LaL6+6Zx+9WbOb8k4fz9161kSRVMO9kLB5bspFP3zuf1/bs40uXnsb768d41UaSVNEMODVs9779fPnhpXzniRc58dg6/tcHzmLiMO9ELEmqfAacGrVu6+v89e1zeG7tq1w14zhunHkS/Xr3LLosSZI6hAGnBj21cjN/c8dcdu9r4lsfOoMLTjm26JIkSepQBpwakpn84MlV/PefLWLskAHc8qF6Jg47ouiyJEnqcAacGrFr737+9kcLuXfOWs47cRj/4/KpPmpBklS1DDg1YMvOPfzlD2YzZ9UWPvmuiVz77hO8I7EkqaoZcKrcmlde46rbnmbtltf5+gemcfFbRhRdkiRJnc6AU8WeX/cqH77tGXbvfO3hqwAAFbRJREFU3c//uXo6Z04YUnRJkiR1CQNOlXp8eSMfu30uR/brxR0fexsnDPf+NpKk2mHAqUI/nb+O6+6ex8RhR/C9j0zn2KN85IIkqbYYcKrMj+e9zHV3z6P+uMHc+uF6V0pJkmqSAaeK3D93LZ+5dz7Txw/mux9+KwP6eHolSbXJb8Aqce/sNXz2h88xY8IQvnPVW+nfx8cuSJJqV4+iC1D73fNMKdycffwxhhtJkjDgVLwHnl3LZ3/4HOdMPIZbr6o33EiShAGnov1m6Uauv/c5zpowmG9fWe/TwCVJKjPgVKj5a7byN3fMZdLwOm4x3EiS9O8YcCrQysYdfOR7zzDkiD58/yNvdSm4JEktGHAqzMZtu7jyu08TwA+uPpNhR3oTP0mSWnKZeAXZsXsfV932DK/s3MNd15zF+GMGFl2SJEndkldwKkRTU/Lpe+axdP02vvHBM3jL6EFFlyRJUrdlwKkQX39sBb94fgOfm3kSbz9haNHlSJLUrRlwKsCvFm/ga79cxp9OHclHzxlfdDmSJHV7Bpxu7oXGHVx71zxOGXkk/3TpW4iIokuSJKnbM+B0Y9t37eWaH8ymT68efOtD3utGkqS2chVVN5WZ/Kd75vPS5te4/aNnMmpQ/6JLkiSpYngFp5u6/fereHTRBm686ERmHD+k6HIkSaooBpxuaPmG7fz9g4t5+wlDnVQsSdJhMOB0M7v37edTd81jYN9efOUyJxVLknQ4nIPTzfzzI8tY1LCNW6+sZ1idj2GQJOlweAWnG/ndik3cMmslf3HmWN598vCiy5EkqWIZcLqJra/t4dP3zGfC0IHcdPHJRZcjSVJFM+B0Ezf9aCGbd+7mXy8/nf59vN+NJEntYcDpBh5bupGfPdfAJ941iVNHHVV0OZIkVTwDTsFe37Ofv/3RQo4fOpC/evuEosuRJKkqtCngRMRLEbEgIuZFxOxy25SIeLLc/tOIOPIAx14XEc9HxMKIuDMiXBrUzL/+ejlrt7zOF993Gn17OTQlSVJHOJQrOO/MzKmZWV/evxW4ITNPAx4Arm95QESMAj4J1GfmqUBP4PJ21lw1lq7fzrdnreTPzhjNWRO8W7EkSR2lPUNUk4FZ5e1HgUsP8LpeQP+I6AUMANa14z2rRlNT8vkHFlDXrxefm3lS0eVIklRV2hpwEngkIuZExDXltoXAJeXty4Axf3BQ5svAV4HVQAPwamY+0tobRMQ1ETE7ImY3NjYeymeoSPfOWcPsVVu4ceZJDB7Yp+hyJEmqKm0NOGdn5jTgIuDjEXEucHV5ew5QB+xpeVBEHA28FxgPjAQGRsQHW3uDzLwlM+szs37o0KGH8VEqx+Ydu/nHny9h+vjBXHbG6KLLkSSp6rQp4GTmuvLPjZTm20zPzCWZeX5mngHcCbzQyqHvBl7MzMbM3AvcD7ytY0qvXF/5xVJ27t7HP7zvVJ81JUlSJzhowImIgRFR98Y2cD6wMCKGldt6ADcB32zl8NXAWRExIErf5OcBizuq+Eq0YuN27pm9hitnjGPisLqiy5EkqSq15QrOcOCJiJgPPA08mJkPA1dExDJgCaWJw7cBRMTIiHgIIDOfAu4D5gILyu93S4d/igry5YeXMrBPLz7+zolFlyJJUtU66NPEM3MlMKWV9puBm1tpXwfMbLb/BeAL7SuzOsxZ9QqPLNrAZ84/wYnFkiR1Iu9k3EUyk398aAlD6/py9Tnjiy5HkqSqZsDpIr9cvJHZq7Zw7bsnMaDPQS+cSZKkdjDgdIF9+5v48sNLmHDMQN5f/we3C5IkSR3MgNMF7p/7Mss37uD6CybTu6ddLklSZ/PbtpPt2rufrz26jCljBnHhqccWXY4kSTXBgNPJ7nhqNeu37eKGC0/0pn6SJHURA04n2ru/iVsfX8n08YOZcbxPC5ckqasYcDrRT+ato+HVXXzs7ccXXYokSTXFgNNJmpqSb816gcnD63jH5Op+eKgkSd2NAaeTPLZ0I8s27OCv3j7BuTeSJHUxA04n+dZvVzJqUH/+ZMrIokuRJKnmGHA6wZxVW3j6pVf46Dnjve+NJEkF8Nu3E3zzty8waEBvLp/uXYslSSqCAaeDrdi4nUcXbeDKGeN85pQkSQUx4HSwW2atpF/vHlw147iiS5EkqWYZcDrQhm27eODZl3l//RiGHNG36HIkSapZBpwOdM8za9i7P7n67PFFlyJJUk0z4HSQpqbk7tlrOHviEMYdM7DociRJqmkGnA7yxIpNrN3yOpe/dWzRpUiSVPMMOB3krmdWc/SA3px/yvCiS5EkqeYZcDrAph27eXTRBi6dNpq+vXoWXY4kSTXPgNMBfjhnLXv3pzf2kySpmzDgtFNmcvcza3jruKOZOKyu6HIkSRIGnHZ76sVXWLlpp5OLJUnqRgw47XTn06up69eLmaeNKLoUSZJUZsBph62v7eHnC9fzvtNH0b+Pk4slSeouDDjtcP/cl9mzr8nhKUmSuhkDzmF6Y3LxlNFHcfLII4suR5IkNWPAOUyLG7azdMN2Lqt3abgkSd2NAecwPbSggZ49gotOPbboUiRJUgsGnMOQmTy0oIGzJgxmyBF9iy5HkiS1YMA5DEvWb2flpp0uDZckqZsy4ByGhxY00CPgglMcnpIkqTsy4ByizOTBBQ2cNWEIxzg8JUlSt2TAOURLN2xnZaPDU5IkdWcGnEP04HOl4akLXT0lSVK31aaAExEvRcSCiJgXEbPLbVMi4sly+08jotW73UXEoIi4LyKWRMTiiJjRkR+gK70xPHXmeIenJEnqzg7lCs47M3NqZtaX928FbsjM04AHgOsPcNzNwMOZeSIwBVh82NUW7N+Gp97i8JQkSd1Ze4aoJgOzytuPApe2fEH5qs65wHcAMnNPZm5tx3sW6qE3hqdcPSVJUrfW1oCTwCMRMScirim3LQQuKW9fBrT2zIIJQCNwW0Q8GxG3RsTA1t4gIq6JiNkRMbuxsfEQPkLXeGN4avr4wQytc3hKkqTurK0B5+zMnAZcBHw8Is4Fri5vzwHqgD2tHNcLmAZ8IzNPB3YCN7T2Bpl5S2bWZ2b90KFDD/VzdLplG3bwQuNOLnb1lCRJ3V6bAk5mriv/3Ehpvs30zFySmedn5hnAncALrRy6FlibmU+V9++jFHgqzoMLGoiAC1w9JUlSt3fQgBMRAyOi7o1t4HxgYUQMK7f1AG4Cvtny2MxcD6yJiMnlpvOARR1Ue5d6eGED08cNZlhdv6JLkSRJB9GWKzjDgSciYj7wNPBgZj4MXBERy4AlwDrgNoCIGBkRDzU7/hPAHRHxHDAV+IeO/ABdYd3W11m2YQfnnTSs6FIkSVIb9DrYCzJzJaXl3S3bb6a0BLxl+zpgZrP9eUB9y9dVkieWbwLg3BO639wgSZL0h7yTcRv8dnkjw+r6Mnl4XdGlSJKkNjDgHMT+puR3KzbxR5OGEhFFlyNJktrAgHMQC19+la2v7eXcE44puhRJktRGBpyDmLWsdNPBcyYacCRJqhQGnIN4fPkmTh11JEN8uKYkSRXDgPMmtu/ay9zVWzh3kqunJEmqJAacN/HkC5vZ15QuD5ckqcIYcN7ErOWNDOzTk2ljjy66FEmSdAgMOG/i8eWbmHH8EPr0spskSaokfnMfwKrNO1m1+TX+yPk3kiRVHAPOAczy8QySJFUsA84BzFrWyOij+zNuyICiS5EkSYfIgNOKvfubePKFzZx7go9nkCSpEhlwWvHs6q3s2L2Pcyd592JJkiqRAacVjy9vpGePYMbxBhxJkiqRAacVv1+5mdNGHcVR/XsXXYokSToMBpwW9uxrYv7aV6k/zpv7SZJUqQw4LSxq2MaefU2cYcCRJKliGXBamLNqCwDTDDiSJFUsA04Lc1dtYdSg/gw/sl/RpUiSpMNkwGlh7uotDk9JklThDDjNrNv6Og2v7mLa2EFFlyJJktrBgNPM3NWl+TdnHDe44EokSVJ7GHCambNqC/179+TEEXVFlyJJktrBgNPM3FVbeMvoo+jd026RJKmS+U1etmvvfp5ft80JxpIkVQEDTtlza19lX1MybawBR5KkSmfAKfMGf5IkVQ8DTtnc1VuYcMxABg/sU3QpkiSpnQw4QGYyd9UWTnd4SpKkqmDAAVa/8hqbd+5xgrEkSVXCgEPz+TfewViSpGpgwKEUcOr69mLSMG/wJ0lSNTDgAHNXb2Xq2EH07BFFlyJJkjpAzQecHbv3sXT9Nu9/I0lSFan5gDN/zVaaEicYS5JURXq15UUR8RKwHdgP7MvM+oiYAnwTOAJ4CfiLzNx2gON7ArOBlzPzPR1Qd4d5tvwE8aljnWAsSVK1OJQrOO/MzKmZWV/evxW4ITNPAx4Arn+TYz8FLD7MGjvVooZtjBsygCP79S66FEmS1EHaM0Q1GZhV3n4UuLS1F0XEaOBiSoGo21nSsJ0Tjz2y6DIkSVIHamvASeCRiJgTEdeU2xYCl5S3LwPGHODYfwE+CzS92RtExDURMTsiZjc2NraxrPZ5fc9+Xty8kxNHuDxckqRq0taAc3ZmTgMuAj4eEecCV5e35wB1wJ6WB0XEe4CNmTnnYG+QmbdkZn1m1g8dOrTtn6Adlm7YTiZewZEkqcq0KeBk5rryz42U5ttMz8wlmXl+Zp4B3Am80MqhZwOXlCcp3wW8KyJu75DKO8CShtKc6JNHGHAkSaomBw04ETEwIure2AbOBxZGxLByWw/gJkorqv6dzLwxM0dn5jjgcuDXmfnBDqy/XZas387APj0ZfXT/okuRJEkdqC1XcIYDT0TEfOBp4MHMfBi4IiKWAUuAdcBtABExMiIe6qyCO9Kihm1MPraOHt7BWJKkqnLQ++Bk5kpgSivtNwM3t9K+DpjZSvtvgN8cTpGdITNZ0rCNP5kysuhSJElSB6vZOxk3vLqLbbv2caLzbyRJqjo1G3CWrC9NMD7pWJeIS5JUbWo24Cxu2A7AZAOOJElVp4YDzjbGDO5PnY9okCSp6tRswFmy3kc0SJJUrWoy4Ozau5+VjTucfyNJUpWqyYCzfMMOmhJOcgWVJElVqSYDzuLyCiqXiEuSVJ1qM+A0bKN/756MHTyg6FIkSVInqMmAs6RhOyccW0dPH9EgSVJVqrmAk5ksWb+Nk0c4wViSpGpVcwFnw7bdbHltr0vEJUmqYjUXcP5tgrFLxCVJqlo1F3CWlB/R4BUcSZKqV+0FnPXbGDWoP0cN8BENkiRVq5oLOIsbtjk8JUlSlaupgLN7335eaNzJia6gkiSpqtVUwFmxcQf7m9JHNEiSVOVqKuCs2vwaEU4wliSp2vUquoCuNPO0ESz8uwvo37tn0aVIkqROVFMBB2Bg35r7yJIk1ZyaGqKSJEm1wYAjSZKqjgFHkiRVHQOOJEmqOgYcSZJUdQw4kiSp6hhwJElS1THgSJKkqhOZWXQNfyAiGoFVnfTXHwNs6qS/W23jOSiW/V88z0Gx7P/ideQ5OC4zh7Zs7JYBpzNFxOzMrC+6jlrmOSiW/V88z0Gx7P/idcU5cIhKkiRVHQOOJEmqOrUYcG4pugB5Dgpm/xfPc1As+794nX4Oam4OjiRJqn61eAVHkiRVOQOOJEmqOjUVcCLiwohYGhErIuKGouupdhExJiIei4jFEfF8RHyq3D44Ih6NiOXln0cXXWs1i4ieEfFsRPysvG//d6GIGBQR90XEkvL/F2Z4DrpORFxX/v2zMCLujIh+9n/niojvRsTGiFjYrO2AfR4RN5a/l5dGxAUdVUfNBJyI6Al8HbgIOBm4IiJOLraqqrcP+HRmngScBXy83Oc3AL/KzEnAr8r76jyfAhY327f/u9bNwMOZeSIwhdK58Bx0gYgYBXwSqM/MU4GewOXY/53te8CFLdpa7fPyd8LlwCnlY/53+fu63Wom4ADTgRWZuTIz9wB3Ae8tuKaqlpkNmTm3vL2d0i/2UZT6/fvll30f+NNiKqx+ETEauBi4tVmz/d9FIuJI4FzgOwCZuSczt+I56Eq9gP4R0QsYAKzD/u9UmTkLeKVF84H6/L3AXZm5OzNfBFZQ+r5ut1oKOKOANc3215bb1AUiYhxwOvAUMDwzG6AUgoBhxVVW9f4F+CzQ1KzN/u86E4BG4LbyMOGtETEQz0GXyMyXga8Cq4EG4NXMfAT7vwgH6vNO+26upYATrbS5Rr4LRMQRwA+BazNzW9H11IqIeA+wMTPnFF1LDesFTAO+kZmnAztxOKTLlOd5vBcYD4wEBkbEB4utSi102ndzLQWctcCYZvujKV2qVCeKiN6Uws0dmXl/uXlDRIwo//kIYGNR9VW5s4FLIuIlSkOy74qI27H/u9JaYG1mPlXev49S4PEcdI13Ay9mZmNm7gXuB96G/V+EA/V5p30311LAeQaYFBHjI6IPpUlNPym4pqoWEUFp7sHizPxasz/6CXBVefsq4MddXVstyMwbM3N0Zo6j9N/7rzPzg9j/XSYz1wNrImJyuek8YBGeg66yGjgrIgaUfx+dR2kuoP3f9Q7U5z8BLo+IvhExHpgEPN0Rb1hTdzKOiJmU5iT0BL6bmV8suKSqFhHnAI8DC/j/c0A+R2kezj3AWEq/gC7LzJYT0tSBIuIdwGcy8z0RMQT7v8tExFRKk7z7ACuBj1D6x6XnoAtExH8F/pzSqs5ngb8EjsD+7zQRcSfwDuAYYAPwBeBHHKDPI+LzwNWUztG1mfnzDqmjlgKOJEmqDbU0RCVJkmqEAUeSJFUdA44kSao6BhxJklR1DDiSJKnqGHAkFSIiPl9+yvNzETEvIs6MiGsjYkDRtUmqfC4Tl9TlImIG8DXgHZm5OyKOoXSfmP9L6cnPmwotUFLF8wqOpCKMADZl5m6AcqD5M0rPC3osIh4DiIjzI+LJiJgbEfeWn2tGRLwUEV+KiKfL/5tYbr8sIhZGxPyImFXMR5PUHXgFR1KXKweVJ4ABwC+BuzPzt+XnZtVn5qbyVZ37gYsyc2dE/Gegb2b+t/Lrvp2ZX4yIK4H3l+/SvAC4MDNfjohBmbm1kA8oqXBewZHU5TJzB3AGcA3QCNwdER9u8bKzgJOB30XEPErPrzmu2Z/f2eznjPL274DvRcR/oPRIFkk1qlfRBUiqTZm5H/gN8JvylZerWrwkgEcz84oD/RUttzPzryPiTOBiYF5ETM3MzR1buaRK4BUcSV0uIiZHxKRmTVOBVcB2oK7c9nvg7GbzawZExAnNjvnzZj+fLL/m+Mx8KjP/C7AJGNOJH0NSN+YVHElFOAL4nxExiNIThFdQGq66Avh5RDRk5jvLw1Z3RkTf8nE3AcvK230j4ilK/1B74yrPV8rBKYBfAfO75NNI6nacZCyp4jSfjFx0LZK6J4eoJElS1fEKjiRJqjpewZEkSVXHgCNJkqqOAUeSJFUdA44kSao6BhxJklR1/h9jQLhtRjhRKAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#taken from hw7\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "fig, axs = plt.subplots(2, figsize=(8, 8))\n",
    "#fig.suptitle('Loss History')\n",
    "axs[0].plot(np.arange(len(per_epoch_loss_train)), per_epoch_loss_train)\n",
    "axs[0].set_title('Training')\n",
    "axs[1].plot(np.arange(len(per_epoch_loss_val)), per_epoch_loss_val)\n",
    "axs[1].set_title('Validation')\n",
    "plt.xlabel('Steps')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "3\n",
      "2\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
